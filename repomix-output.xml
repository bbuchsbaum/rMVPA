This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: R/**/*.R, R/**/*.r, *.Rmd, *.rmd, DESCRIPTION, tests/**/*.R, tests/**/*.r
- Files matching patterns in .gitignore are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
R/
  allgeneric.R
  balance.R
  classifiers.R
  common.R
  crossval.R
  custom.R
  dataset.R
  design.R
  distcalc.R
  feature_rsa_model.R
  feature_selection.R
  manova_model.R
  model_fit.R
  mvpa_iterate.R
  mvpa_model.R
  mvpa_result.R
  new_analysis_overview.R
  performance.R
  regional.R
  resample.R
  rMVPA.R
  roisplit.R
  rsa_model.R
  searchlight.R
  utils.R
  vector_rsa_model.R
tests/
  testthat/
    test_balance_partition.R
    test_custom_regional.R
    test_custom_searchlight.R
    test_distfun.R
    test_feature_rsa.R
    test_manova_searchlight.R
    test_mvpa_regional.R
    test_mvpa_searchlight.R
    test_rsa_regional.R
    test_rsa_searchlight.R
    test_vector_rsa_regional.R
    test_vector_rsa_searchlight.R
  simple_test.R
  testthat.R
DESCRIPTION
README.rmd
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="tests/testthat.R">
library(testthat)
library(rMVPA)

test_check("rMVPA")
</file>

<file path="R/balance.R">
#' Balance Cross-Validation Partitions
#'
#' Modifies a cross-validation partitioning scheme to ensure that each
#' target class has an equal number of samples within each training fold,
#' and optionally within each test fold, using either sub-sampling or
#' oversampling.
#'
#' @param obj A `cross_validation` object (e.g., from
#'   `blocked_cross_validation`, `kfold_cross_validation`).
#' @param design An `mvpa_design` object containing the target labels
#'   (`.sa.targets`) corresponding to the original dataset samples.
#' @param method Character string specifying the balancing method:
#'   - `"subsample"` (default): Down-samples majority classes to match the
#'     size of the smallest class (sampling without replacement).
#'   - `"oversample"`: Up-samples minority classes to match the size of the
#'     largest class (sampling with replacement).
#' @param balance_test_set Logical. If `TRUE` (default), balance the test
#'   sets in each fold as well using the specified `method`. **Note:**
#'   Oversampling the test set is generally not recommended as it can
#'   lead to misleading performance estimates. A warning will be issued if
#'   `balance_test_set=TRUE` and `method="oversample"`.
#' @param seed An optional integer seed for the random number generator for
#'   reproducible sampling. If `NULL` (default), the result varies.
#' @param ... Additional arguments (currently unused).
#'
#' @return A `custom_cross_validation` object where the sample indices in
#'   `.train_indices` (and optionally `.test_indices`) for each fold have
#'   been resampled to ensure balanced representation of target classes.
#'
#' @details
#' This function implements two balancing strategies, inspired by
#' CosmoMVPA's `cosmo_balance_partitions` (which uses sub-sampling).
#'
#' **Sub-sampling (`method="subsample"`)**:
#' 1. Identifies the target class with the minimum number of samples
#'    (`min_count`) within the set (train or test).
#' 2. For *every* target class within that set, it randomly selects exactly
#'    `min_count` samples *without replacement*.
#' 3. Discards samples from majority classes.
#'
#' **Oversampling (`method="oversample"`)**:
#' 1. Identifies the target class with the maximum number of samples
#'    (`max_count`) within the set (train or test).
#' 2. For *every* target class within that set, it randomly selects exactly
#'    `max_count` samples *with replacement*.
#' 3. Duplicates samples from minority classes.
#'
#' Balancing guarantees that after the process, each target class appears
#' equally often within each balanced training (and optionally testing) set.
#' This is useful for preventing classifiers from being biased towards
#' majority classes.
#'
#' The output is always a `custom_cross_validation` object because the
#' balancing process defines specific, explicit sets of indices for each
#' fold, which may no longer strictly adhere to the original blocking or
#' k-fold structure.
#'
#' @examples
#' # Create an imbalanced dataset design (more class 'b')
#' design_df <- data.frame(condition = factor(rep(c("a", "b", "b"), 20)),
#'                        block = rep(1:6, each = 10))
#' des <- mvpa_design(design_df, y_train = ~ condition, block_var = ~ block)
#'
#' # Create standard blocked partitions (likely unbalanced)
#' cval_unbalanced <- blocked_cross_validation(des$block_var)
#' print("Unbalanced Counts (Example Fold 1 Train):")
#' print(table(des$y_train[unlist(crossval_samples(cval_unbalanced,
#'           design_df, des$y_train)$train[[1]]$idx)]))
#'
#' # Balance using sub-sampling (default)
#' cval_sub <- balance_partitions(cval_unbalanced, des, seed = 1)
#' print(cval_sub)
#' print("Subsample Balanced Counts (Example Fold 1 Train):")
#' print(table(crossval_samples(cval_sub, design_df, des$y_train)$ytrain[[1]]))
#'
#' # Balance using over-sampling
#' cval_over <- balance_partitions(cval_unbalanced, des, method = "oversample", seed = 2)
#' print(cval_over)
#' print("Oversample Balanced Counts (Example Fold 1 Train):")
#' print(table(crossval_samples(cval_over, design_df, des$y_train)$ytrain[[1]]))
#'
#' @export
#' @family cross_validation
balance_partitions <- function(obj, design, ...) {
  UseMethod("balance_partitions")
}


#' @rdname balance_partitions
#' @export
balance_partitions.default <- function(obj, design, method = "subsample", ...) {
  warning("Balancing not explicitly implemented for class '",
          class(obj)[1], "'. Attempting default balancing using method='", method, "'.", call. = FALSE)
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

#' @rdname balance_partitions
#' @export
balance_partitions.blocked_cross_validation <- function(obj, design, method = "subsample", ...) {
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

#' @rdname balance_partitions
#' @export
balance_partitions.kfold_cross_validation <- function(obj, design, method = "subsample", ...) {
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

#' @rdname balance_partitions
#' @export
balance_partitions.twofold_blocked_cross_validation <- function(obj, design, method = "subsample", ...) {
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

#' @rdname balance_partitions
#' @export
balance_partitions.bootstrap_blocked_cross_validation <- function(obj, design, method = "subsample", ...) {
  warning("Balancing bootstrap partitions might have unintended consequences. ",
          "Bootstrap already involves resampling. Proceeding with method='", method, "'.", call. = FALSE)
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

#' @rdname balance_partitions
#' @export
balance_partitions.sequential_blocked_cross_validation <- function(obj, design, method = "subsample", ...) {
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

#' @rdname balance_partitions
#' @export
balance_partitions.custom_cross_validation <- function(obj, design, method = "subsample", ...) {
  # Custom CV might already be balanced, but we allow re-balancing
  perform_balancing_on_cv_obj(obj, design, method = method, ...)
}

# Internal helper function to perform the balancing logic
# Takes a cross_validation object and design, returns a custom_cross_validation obj
#' @keywords internal
#' @noRd
perform_balancing_on_cv_obj <- function(obj, design, method = "subsample",
                                        balance_test_set = TRUE, seed = NULL, ...) {

  # Validate method argument
  valid_methods <- c("subsample", "oversample")
  if (!method %in% valid_methods) {
    stop("`method` must be one of: ", paste(valid_methods, collapse=", "), call. = FALSE)
  }

  if (!inherits(design, "mvpa_design")) {
    stop("`design` argument must be an 'mvpa_design' object.")
  }

  # Issue warning if oversampling the test set
  if (balance_test_set && method == "oversample") {
      warning("Oversampling the test set ('balance_test_set = TRUE', ",
              "method = 'oversample') is generally not recommended and ",
              "may lead to inflated performance metrics.", call. = FALSE)
  }


  targets <- design$y_train # Use training targets for reference

  # Get original sample indices for each fold using the appropriate method
  n_obs_total <- length(targets)
  dummy_data <- data.frame(dummy_col = seq_len(n_obs_total))
  initial_folds_tibble <- crossval_samples(obj, dummy_data, targets)

  balanced_sample_set <- vector("list", nrow(initial_folds_tibble))

  # Set seed if provided
  if (!is.null(seed)) {
    if(!exists(".Random.seed", envir = .GlobalEnv, inherits = FALSE)) runif(1)
    old_seed <- .GlobalEnv$.Random.seed
    on.exit(assign(".Random.seed", old_seed, envir = .GlobalEnv), add = TRUE)
    set.seed(seed)
  }

  # Iterate through each fold provided by crossval_samples
  for (i in 1:nrow(initial_folds_tibble)) {
    train_indices_orig <- initial_folds_tibble$train[[i]]$idx
    test_indices_orig  <- initial_folds_tibble$test[[i]]$idx

    # Balance training set
    balanced_train_indices <- balance_single_fold(train_indices_orig, targets, method)

    # Balance test set if requested
    balanced_test_indices <- if (balance_test_set) {
      balance_single_fold(test_indices_orig, targets, method)
    } else {
      test_indices_orig # Keep original test indices
    }

    # Store the balanced train/test indices for this fold
    balanced_sample_set[[i]] <- list(train = balanced_train_indices,
                                     test = balanced_test_indices)
  }

  # Return a new custom_cross_validation object with the balanced indices
  custom_cross_validation(balanced_sample_set)
}


# Internal helper function to balance indices for a single fold
# Now includes 'method' argument
#' @keywords internal
#' @noRd
balance_single_fold <- function(fold_indices, all_targets, method) {
  if (length(fold_indices) == 0) {
    return(integer(0)) # Return empty if fold is empty
  }

  fold_targets <- all_targets[fold_indices]
  target_counts <- table(fold_targets)

  if (length(target_counts) <= 1) {
      # Only one class present or empty fold, cannot balance further
      warning("Fold contains only one class or is empty; cannot balance.", call. = FALSE)
      return(fold_indices) # Return original indices for this fold
  }

  # Check for classes with zero samples in this fold (should ideally not happen
  # if partitions are valid, but handle defensively)
  if (any(target_counts == 0)) {
      warning("Fold contains a class with zero samples; resulting balanced fold might be incomplete or invalid.", call.=FALSE)
      # Proceed, but the target size calculation might be affected
      target_counts <- target_counts[target_counts > 0] # Exclude zero-count classes
      if (length(target_counts) <= 1) {
          return(fold_indices) # Cannot balance if only one class remains
      }
  }


  balanced_indices <- integer(0)

  # Get unique targets present in this specific fold
  unique_fold_targets <- names(target_counts)

  if (method == "subsample") {
    # --- Sub-sampling logic ---
    target_size <- min(target_counts)

    for (target_level in unique_fold_targets) {
      indices_this_target_in_fold <- fold_indices[which(fold_targets == target_level)]
      n_available <- length(indices_this_target_in_fold)
      # sample size is target_size (min_count)
      sample_size <- min(target_size, n_available) # Defensive check

      if (n_available > 0) {
          sampled_indices <- if (n_available == 1 && sample_size == 1) {
                                 indices_this_target_in_fold
                             } else {
                                 sample(indices_this_target_in_fold, size = sample_size, replace = FALSE)
                             }
          balanced_indices <- c(balanced_indices, sampled_indices)
      }
    }

  } else if (method == "oversample") {
    # --- Oversampling logic ---
    target_size <- max(target_counts)

    for (target_level in unique_fold_targets) {
      indices_this_target_in_fold <- fold_indices[which(fold_targets == target_level)]
      n_available <- length(indices_this_target_in_fold)

      if (n_available > 0) {
          # sample size is target_size (max_count)
          # Sample *with replacement*
          sampled_indices <- sample(indices_this_target_in_fold, size = target_size, replace = TRUE)
          balanced_indices <- c(balanced_indices, sampled_indices)
      } # If n_available is 0, this class cannot be oversampled
    }

  } else {
      # Should have been caught earlier, but double-check
      stop("Internal error: invalid method '", method, "' in balance_single_fold.")
  }

  # Return the sorted balanced indices for consistency
  sort(balanced_indices)
}
</file>

<file path="R/new_analysis_overview.R">
#' @keywords internal
"_PACKAGE"

#' Extending the MVPA Framework: Creating a New Analysis Type
#'
#' This documentation describes how to implement a **new MVPA analysis type** within the package. 
#' The MVPA framework here is designed to be extensible via S3 generics. By providing the necessary 
#' methods and classes, you can integrate custom analyses (beyond standard classification, regression, or RSA).
#'
#' @section Overview:
#' 1. **Define a Model Specification**  
#'    - Create an S3 class inheriting from \code{model_spec}, e.g. \code{"myanalysis_model"}.
#'    - This class should hold all the **parameters** or **configuration** needed for your analysis 
#'      (e.g., hyperparameters, references, design objects).
#'    - Construct it using a function like \code{create_model_spec("myanalysis_model", ...)}.
#'
#' 2. **Implement Required Generics**  
#'    - \strong{\code{train_model.myanalysis_model(obj, train_dat, ...)}:}  
#'      Defines how to \emph{"train"} or \emph{execute} your analysis on a subset of data 
#'      (e.g., ROI voxel intensities).  
#'      - This might involve fitting a specialized model, computing a metric, or doing an 
#'        unsupervised transform.  
#'      - Return a structured object (often a named list or model fit) that can be further 
#'        handled by the framework.
#'    - \strong{\code{process_roi.myanalysis_model(mod_spec, roi, rnum, ...)} (Optional):}  
#'      If you need custom \emph{per-ROI} logic beyond the default pipeline, implement a 
#'      \code{process_roi} method for your class. This method is called automatically by
#'      \code{run_regional(...)} for each region.  
#'      - By default, the framework calls \code{train_model} on the ROI's data, but if you 
#'        want to skip or alter that flow, provide a custom \code{process_roi} method.
#'    - (Optional) \strong{\code{predict_model.myanalysis_model(...)}} or \strong{\code{evaluate_model.myanalysis_model(...)}}:  
#'      If your analysis involves a separate test step or specialized evaluation, you can 
#'      define these S3 methods.  
#'    - (Optional) \strong{\code{run_future.myanalysis_model(...)}}, \strong{\code{merge_results.myanalysis_model(...)}}, etc.:  
#'      If you need custom behavior for parallelization or merging partial results, override 
#'      these generics as well.
#'
#' 3. **Use `run_regional()` or `run_searchlight()`**  
#'    - Once your new model class is defined, you can pass an object of that class to 
#'      \code{run_regional(...)} or \code{run_searchlight(...)} just like a built-in analysis.  
#'    - The framework automatically dispatches calls to your \code{train_model.myanalysis_model} 
#'      (and optionally your \code{process_roi.myanalysis_model}), returning a result object 
#'      consistent with other MVPA analyses.
#'
#' @section Why These Pieces Are Needed:
#' - **Model Specification**: Provides the blueprint for how the pipeline should handle your analysis 
#'   (what data you need, how you store intermediate results, etc.).
#' - **Train Model Generic**: Tells the pipeline what to \emph{do} with each chunk of data 
#'   (ROI subset, searchlight voxel set, cross-validation fold). 
#'   This is the core of your analysis.
#' - **\code{process_roi}**: If your analysis requires more complex per-region logic than the default 
#'   pipeline (e.g., skipping certain steps, altering the data structure), implementing 
#'   \code{process_roi.myanalysis_model} gives you full control.
#'
#' @section Example Code Skeleton:
#' \preformatted{
#' # Minimal template for a new analysis model
#'
#' myanalysis_model <- function(dataset, design, param1=NULL, param2=5, ...) {
#'   # Create an object of class "myanalysis_model"
#'   create_model_spec(
#'     "myanalysis_model",
#'     dataset  = dataset,
#'     design   = design,
#'     param1   = param1,
#'     param2   = param2,
#'     ...
#'   )
#' }
#'
#' # S3 method to "train" your analysis
#' train_model.myanalysis_model <- function(obj, train_dat, y, indices, ...) {
#'   # train_dat = subset of voxel intensities or features
#'   # y = optional labels/response if relevant
#'   # indices = location info
#'
#'   # 1) Do your computations (model fitting, metrics, transforms, etc.)
#'   fit_object <- your_method(train_dat, some_params=obj$param1)
#'
#'   # 2) Return a structure. The pipeline may store or evaluate it further.
#'   list(
#'     fit = fit_object,
#'     # any other stuff you want to keep
#'   )
#' }
#'
#' # Optional custom ROI processor
#' process_roi.myanalysis_model <- function(mod_spec, roi, rnum, ...) {
#'   # If you want to skip or augment the default cross-validation / train_model approach
#'   # in run_regional, define your logic here. 
#'   # Otherwise, the default calls train_model(...) on the region's data.
#' }
#'
#' # You can now call:
#' # new_model <- myanalysis_model(dataset, design, param1="xyz")
#' # results_regional <- run_regional(new_model, region_mask)
#' # results_searchlight <- run_searchlight(new_model, radius=8, method="standard")
#' }
#'
#' @name new-analysis-overview
NULL
</file>

<file path="tests/testthat/test_balance_partition.R">
library(testthat)
library(rMVPA)
library(tibble)
library(dplyr)

context("balance_partitions")

# Helper function to check if counts are balanced within a vector
is_balanced <- function(targets) {
  counts <- table(targets)
  if (length(counts) <= 1) return(TRUE) # Cannot balance 0 or 1 class
  all(counts == counts[1])
}

# --- Test Setup --- 

# Design 1: Imbalanced Binary (more 'b')
design_df_bin <- data.frame(
  condition = factor(rep(c("a", "b", "b", "b"), 25)), # 25 'a', 75 'b'
  block = rep(1:5, each = 20) # 5 blocks, 20 samples each
)
des_bin <- mvpa_design(design_df_bin, y_train = ~ condition, block_var = ~ block)
y_bin <- des_bin$y_train

# Design 2: Imbalanced Multi-class (more 'c')
design_df_multi <- data.frame(
  condition = factor(rep(c("a", "b", "c", "c", "c", "c"), 20)), # 20 'a', 20 'b', 80 'c'
  block = rep(1:6, each = 20) # 6 blocks, 20 samples each
)
des_multi <- mvpa_design(design_df_multi, y_train = ~ condition, block_var = ~ block)
y_multi <- des_multi$y_train

# Create various CV objects
cval_blocked_bin <- blocked_cross_validation(des_bin$block_var)
cval_kfold_bin <- kfold_cross_validation(length(y_bin), nfolds = 5)
cval_twofold_bin <- twofold_blocked_cross_validation(des_bin$block_var, nreps = 4) # Less reps for speed

cval_blocked_multi <- blocked_cross_validation(des_multi$block_var)

# --- Test Subsampling --- 

test_that("balance_partitions subsample works for blocked_cv (binary)", {
  cval_bal <- balance_partitions(cval_blocked_bin, des_bin, method = "subsample", seed = 1)
  expect_s3_class(cval_bal, "custom_cross_validation")
  
  samples_bal <- crossval_samples(cval_bal, design_df_bin, y_bin)
  
  for (i in 1:nrow(samples_bal)) {
    # Check training set balance
    ytrain_bal <- samples_bal$ytrain[[i]]
    expect_true(is_balanced(ytrain_bal))
    # Check test set balance (default balance_test_set = TRUE)
    ytest_bal <- samples_bal$ytest[[i]]
    expect_true(is_balanced(ytest_bal))
    # Check total size (should be smaller)
    samples_unbal <- crossval_samples(cval_blocked_bin, design_df_bin, y_bin)
    expect_lt(length(ytrain_bal), length(samples_unbal$ytrain[[i]]))
    # Cannot guarantee test set is smaller if a block was already balanced or had only one class
  }
})

test_that("balance_partitions subsample works for kfold_cv (binary)", {
  cval_bal <- balance_partitions(cval_kfold_bin, des_bin, method = "subsample", seed = 2)
  expect_s3_class(cval_bal, "custom_cross_validation")
  samples_bal <- crossval_samples(cval_bal, design_df_bin, y_bin)
  for (i in 1:nrow(samples_bal)) {
    expect_true(is_balanced(samples_bal$ytrain[[i]]))
    expect_true(is_balanced(samples_bal$ytest[[i]]))
  }
})

test_that("balance_partitions subsample works for twofold_cv (binary)", {
  cval_bal <- balance_partitions(cval_twofold_bin, des_bin, method = "subsample", seed = 3)
  expect_s3_class(cval_bal, "custom_cross_validation")
  samples_bal <- crossval_samples(cval_bal, design_df_bin, y_bin)
  expect_equal(nrow(samples_bal), cval_twofold_bin$nreps)
  for (i in 1:nrow(samples_bal)) {
    expect_true(is_balanced(samples_bal$ytrain[[i]]))
    expect_true(is_balanced(samples_bal$ytest[[i]]))
  }
})

test_that("balance_partitions subsample works for multi-class", {
  cval_bal <- balance_partitions(cval_blocked_multi, des_multi, method = "subsample", seed = 4)
  expect_s3_class(cval_bal, "custom_cross_validation")
  samples_bal <- crossval_samples(cval_bal, design_df_multi, y_multi)
  for (i in 1:nrow(samples_bal)) {
    expect_true(is_balanced(samples_bal$ytrain[[i]]))
    expect_true(is_balanced(samples_bal$ytest[[i]]))
  }
})

test_that("balance_partitions subsample respects balance_test_set = FALSE", {
  cval_bal <- balance_partitions(cval_blocked_bin, des_bin, method = "subsample", balance_test_set = FALSE, seed = 5)
  expect_s3_class(cval_bal, "custom_cross_validation")
  samples_bal <- crossval_samples(cval_bal, design_df_bin, y_bin)
  samples_unbal <- crossval_samples(cval_blocked_bin, design_df_bin, y_bin)
  
  test_sets_balanced = TRUE
  for (i in 1:nrow(samples_bal)) {
    expect_true(is_balanced(samples_bal$ytrain[[i]])) # Train should be balanced
    # Test should be *identical* to original unbalanced test set
    expect_equal(sort(samples_bal$test[[i]]$idx), sort(samples_unbal$test[[i]]$idx))
    if (!is_balanced(samples_bal$ytest[[i]])) { 
      test_sets_balanced = FALSE
    }
  }
  expect_false(test_sets_balanced) # Ensure at least one test set was not balanced
})

# --- Test Oversampling --- 

test_that("balance_partitions oversample works for blocked_cv (binary)", {
  expect_warning(
    cval_bal <- balance_partitions(cval_blocked_bin, des_bin, method = "oversample", seed = 6),
    "Oversampling the test set"
  )
  expect_s3_class(cval_bal, "custom_cross_validation")
  
  samples_bal <- crossval_samples(cval_bal, design_df_bin, y_bin)
  
  for (i in 1:nrow(samples_bal)) {
    ytrain_bal <- samples_bal$ytrain[[i]]
    expect_true(is_balanced(ytrain_bal))
    ytest_bal <- samples_bal$ytest[[i]]
    expect_true(is_balanced(ytest_bal))
    
    # Check total size (should be larger)
    samples_unbal <- crossval_samples(cval_blocked_bin, design_df_bin, y_bin)
    expect_gt(length(ytrain_bal), length(samples_unbal$ytrain[[i]]))
    # Cannot guarantee test set is larger if a block was already balanced 
  }
})

test_that("balance_partitions oversample works for multi-class", {
   expect_warning(
      cval_bal <- balance_partitions(cval_blocked_multi, des_multi, method = "oversample", seed = 7),
     "Oversampling the test set"
   )
  expect_s3_class(cval_bal, "custom_cross_validation")
  samples_bal <- crossval_samples(cval_bal, design_df_multi, y_multi)
  for (i in 1:nrow(samples_bal)) {
    expect_true(is_balanced(samples_bal$ytrain[[i]]))
    expect_true(is_balanced(samples_bal$ytest[[i]]))
  }
})

test_that("balance_partitions oversample respects balance_test_set = FALSE", {
  # No warning expected here
  cval_bal <- balance_partitions(cval_blocked_bin, des_bin, method = "oversample", balance_test_set = FALSE, seed = 8)
  expect_s3_class(cval_bal, "custom_cross_validation")
  samples_bal <- crossval_samples(cval_bal, design_df_bin, y_bin)
  samples_unbal <- crossval_samples(cval_blocked_bin, design_df_bin, y_bin)
  
  test_sets_balanced = TRUE
  for (i in 1:nrow(samples_bal)) {
    expect_true(is_balanced(samples_bal$ytrain[[i]])) # Train should be balanced
    # Test should be *identical* to original unbalanced test set
    expect_equal(sort(samples_bal$test[[i]]$idx), sort(samples_unbal$test[[i]]$idx))
     if (!is_balanced(samples_bal$ytest[[i]])) { 
      test_sets_balanced = FALSE
    }
  }
   expect_false(test_sets_balanced) # Ensure at least one test set was not balanced
})

# --- Test Edge Cases & Options --- 

test_that("balance_partitions handles folds with only one class", {
  # Create a scenario where one block (fold) has only one class ('b')
  design_df_single <- data.frame(
      condition = factor(c(rep("a", 5), rep("b", 25))), # 5 'a' in Blk 1, 25 'b' in Blk 2+3
      block = c(rep(1, 5), rep(2, 15), rep(3, 10)) # Block 3 has only 'b'
  )
  des_single <- mvpa_design(design_df_single, y_train = ~ condition, block_var = ~ block)
  y_single <- des_single$y_train
  cval_blocked_single <- blocked_cross_validation(des_single$block_var)
  samples_unbal <- crossval_samples(cval_blocked_single, design_df_single, y_single)
  
  # Run balancing - expect warnings but don't test for specific text
  suppressWarnings({
    cval_bal <- balance_partitions(cval_blocked_single, des_single, method = "subsample", seed = 9)
  })
  
  samples_bal <- crossval_samples(cval_bal, design_df_single, y_single)
  
  # Fold 1: Test=Blk1('a' only), Train=Blk2+3('b' only)
  expect_false(is_balanced(samples_bal$ytrain[[1]])) # Train cannot be balanced (only 'b')
  expect_false(is_balanced(samples_bal$ytest[[1]]))  # Test cannot be balanced (only 'a')
  expect_equal(sort(samples_bal$test[[1]]$idx), sort(samples_unbal$test[[1]]$idx)) # Test unchanged
  
  # Fold 2: Test=Blk2('b' only), Train=Blk1('a')+Blk3('b')
  expect_true(is_balanced(samples_bal$ytrain[[2]]))  # Train *can* be balanced (has 'a' and 'b')
  expect_false(is_balanced(samples_bal$ytest[[2]])) # Test cannot be balanced (only 'b')
  expect_equal(sort(samples_bal$test[[2]]$idx), sort(samples_unbal$test[[2]]$idx)) # Test unchanged

  # Fold 3: Test=Blk3('b' only), Train=Blk1('a')+Blk2('b')
  expect_true(is_balanced(samples_bal$ytrain[[3]])) # Train *can* be balanced (has 'a' and 'b')
  expect_false(is_balanced(samples_bal$ytest[[3]])) # Test cannot be balanced (only 'b')
  expect_equal(sort(samples_bal$test[[3]]$idx), sort(samples_unbal$test[[3]]$idx)) # Test unchanged
  
})

test_that("balance_partitions gives reproducible results with seed", {
  cval_bal1 <- balance_partitions(cval_blocked_bin, des_bin, method = "subsample", seed = 123)
  cval_bal2 <- balance_partitions(cval_blocked_bin, des_bin, method = "subsample", seed = 123)
  cval_bal3 <- balance_partitions(cval_blocked_bin, des_bin, method = "subsample", seed = 456)
  
  expect_identical(cval_bal1$sample_set, cval_bal2$sample_set)
  expect_false(identical(cval_bal1$sample_set, cval_bal3$sample_set))
  
  # Also test oversampling seed
   expect_warning(cval_bal_over1 <- balance_partitions(cval_blocked_bin, des_bin, method = "oversample", seed = 789))
   expect_warning(cval_bal_over2 <- balance_partitions(cval_blocked_bin, des_bin, method = "oversample", seed = 789))
   expect_warning(cval_bal_over3 <- balance_partitions(cval_blocked_bin, des_bin, method = "oversample", seed = 101))
   
  expect_identical(cval_bal_over1$sample_set, cval_bal_over2$sample_set)
  expect_false(identical(cval_bal_over1$sample_set, cval_bal_over3$sample_set))
})

# --- Test Error Handling --- 

test_that("balance_partitions throws error for invalid method", {
  expect_error(
    balance_partitions(cval_blocked_bin, des_bin, method = "invalid_method"),
    "`method` must be one of: subsample, oversample"
  )
})

test_that("balance_partitions throws error for non-mvpa_design object", {
  expect_error(
    balance_partitions(cval_blocked_bin, design_df_bin), # Pass dataframe instead of design
    "`design` argument must be an 'mvpa_design' object."
  )
})

# Test balancing an already custom_cross_validation object
test_that("balance_partitions can re-balance a custom_cross_validation object", {
  # Create an unbalanced custom CV
  samples_unbal <- crossval_samples(cval_blocked_bin, design_df_bin, y_bin)
  custom_unbal_set <- lapply(1:nrow(samples_unbal), function(i) {
      list(train = samples_unbal$train[[i]]$idx, test = samples_unbal$test[[i]]$idx)
  })
  cval_custom_unbal <- custom_cross_validation(custom_unbal_set)

  # Balance it
  cval_custom_bal <- balance_partitions(cval_custom_unbal, des_bin, method = "subsample", seed = 10)
  expect_s3_class(cval_custom_bal, "custom_cross_validation")
  samples_custom_bal <- crossval_samples(cval_custom_bal, design_df_bin, y_bin)

  # Check if balanced
  for (i in 1:nrow(samples_custom_bal)) {
      expect_true(is_balanced(samples_custom_bal$ytrain[[i]]))
      expect_true(is_balanced(samples_custom_bal$ytest[[i]]))
  }
})
</file>

<file path="tests/testthat/test_custom_regional.R">
library(testthat)
library(rMVPA)
library(dplyr)
library(tibble)
library(neuroim2)

context("run_custom_regional")

# --- Setup --- 

# Volumetric Dataset and Mask
dset_info_vol <- gen_sample_dataset(D = c(6,6,6), nobs = 40, nlevels = 2)
dataset_vol <- dset_info_vol$dataset
design_vol <- dset_info_vol$design

mask_arr <- array(0, dim(dataset_vol$mask))
mask_arr[1:3, 1:3, 1:3] <- 1 # ROI 1
mask_arr[4:6, 1:3, 1:3] <- 2 # ROI 2
mask_arr[1:3, 4:6, 4:6] <- 3 # ROI 3
mask_arr[4:6, 4:6, 4:6] <- 4 # ROI 4 (small, 1 voxel after filtering)
mask_arr[4:6, 4:6, 1:3] <- 5 # ROI 5 (will cause error in error_func)
mask_arr[1:3, 1:3, 4:6] <- 0 # Empty region
mask_arr[1,1,1] <- 0 # Ensure ROI 1 doesn't occupy the full corner
region_mask_vol <- NeuroVol(mask_arr, space(dataset_vol$mask))

# Define Custom Functions

# Returns a named list
stats_func <- function(roi_data, roi_info) {
  list(
    roi_id = roi_info$id,
    mean_val = mean(roi_data, na.rm = TRUE),
    sd_val = sd(roi_data, na.rm = TRUE),
    n_vox = ncol(roi_data)
  )
}

# Returns a single-row tibble
dataframe_func <- function(roi_data, roi_info) {
  tibble::tibble(
    avg = mean(roi_data, na.rm = TRUE),
    std = sd(roi_data, na.rm = TRUE),
    voxels = ncol(roi_data)
  )
}

# Intentionally causes an error for ROI 5
error_func <- function(roi_data, roi_info) {
  if (roi_info$id == 5) {
    stop("Intentional error in ROI 5!")
  }
  list(
    mean_val = mean(roi_data, na.rm = TRUE),
    n_vox = ncol(roi_data)
  )
}

# Returns an invalid (unnamed) list
unnamed_func <- function(roi_data, roi_info) {
  list(mean(roi_data, na.rm = TRUE), sd(roi_data, na.rm = TRUE))
}

# Returns a list with a non-scalar element
non_scalar_func <- function(roi_data, roi_info) {
  list(
    mean_val = mean(roi_data, na.rm = TRUE),
    all_vals = roi_data[,1] # Not a scalar
  )
}

# --- Basic Functionality Tests (Volumetric) --- 

test_that("run_custom_regional works with list-returning function", {
  results <- run_custom_regional(dataset_vol, region_mask_vol, stats_func)
  
  expect_s3_class(results, "tbl_df")
  expect_equal(nrow(results), 5) # ROIs 1, 2, 3, 4, 5
  expect_named(results, c("id", "roi_id", "mean_val", "sd_val", "n_vox", "error", "error_message"), ignore.order = TRUE)
  expect_true(all(!results$error))
  expect_equal(results$id, results$roi_id)
  expect_true(all(results$n_vox > 0))
})

test_that("run_custom_regional works with dataframe-returning function", {
  results <- run_custom_regional(dataset_vol, region_mask_vol, dataframe_func)
  
  expect_s3_class(results, "tbl_df")
  expect_equal(nrow(results), 5)
  expect_named(results, c("id", "avg", "std", "voxels", "error", "error_message"), ignore.order = TRUE)
  expect_true(all(!results$error))
  expect_true(all(results$voxels > 0))
})

# --- Error Handling within Custom Function --- 

test_that("run_custom_regional handles errors in custom_func correctly", {
  # Suppress warnings about the error itself during the run
  suppressWarnings({
      results <- run_custom_regional(dataset_vol, region_mask_vol, error_func) 
  })
  
  expect_s3_class(results, "tbl_df")
  expect_equal(nrow(results), 5)
  expect_named(results, c("id", "mean_val", "n_vox", "error", "error_message"), ignore.order = TRUE)
  
  # Check ROI 5 (errored)
  roi5_row <- results[results$id == 5, ]
  expect_true(roi5_row$error)
  expect_match(roi5_row$error_message, "Intentional error in ROI 5!")
  expect_true(is.na(roi5_row$mean_val))
  expect_true(is.na(roi5_row$n_vox))
  
  # Check other ROIs (not errored)
  other_rows <- results[results$id != 5, ]
  expect_true(all(!other_rows$error))
  expect_true(all(!is.na(other_rows$mean_val)))
  expect_true(all(!is.na(other_rows$n_vox)))
})

# --- Parallel Execution Test --- 

test_that("run_custom_regional runs in parallel without error", {
  skip_if_not_installed("future")
  skip_if_not_installed("future.apply") # Needed for auto plan setting
  
  # Run sequentially first
  results_seq <- run_custom_regional(dataset_vol, region_mask_vol, stats_func, .cores = 1)
  
  # Run in parallel 
  # No need to manually set plan if future.apply is installed
  # suppressMessages to hide the plan setting message
  suppressMessages({
      results_par <- run_custom_regional(dataset_vol, region_mask_vol, stats_func, .cores = 2) 
  })
 
  # Reset plan just in case
  future::plan(future::sequential)
  
  # Basic structural comparison (exact numeric values might differ slightly)
  expect_equal(nrow(results_par), nrow(results_seq))
  expect_equal(names(results_par), names(results_seq))
  expect_equal(results_par$id, results_seq$id)
  expect_equal(results_par$error, results_seq$error)
  expect_true(all(!results_par$error)) # Ensure no errors occurred during parallel run
})

# --- Input Validation and Edge Cases --- 

test_that("run_custom_regional input validation works", {
  plain_array <- array(0, c(2,2,2))
  # Invalid dataset
  expect_error(run_custom_regional(as.matrix(dataset_vol$train_data), region_mask_vol, stats_func),
               "`dataset` must be an 'mvpa_dataset' or 'mvpa_surface_dataset' object.")
  # Invalid mask (plain array)
  expect_error(run_custom_regional(dataset_vol, plain_array, stats_func),
               "`region_mask` must be a 'NeuroVol' or 'NeuroSurface' object.")
  # Invalid custom_func
  expect_error(run_custom_regional(dataset_vol, region_mask_vol, "not_a_function"),
               "`custom_func` must be a function.")
   # Invalid cores
  expect_error(run_custom_regional(dataset_vol, region_mask_vol, stats_func, .cores = 0),
               "`.cores` must be a positive integer.")
  expect_error(run_custom_regional(dataset_vol, region_mask_vol, stats_func, .cores = 1.5),
               "`.cores` must be a positive integer.")
})

test_that("run_custom_regional handles custom_func returning invalid structures", {
  # Unnamed list - should cause error during internal processing
  # Should result in error = TRUE in the output table
  results_unnamed <- run_custom_regional(dataset_vol, region_mask_vol, unnamed_func)
  expect_s3_class(results_unnamed, "tbl_df")
  expect_true(all(results_unnamed$error))
  expect_match(results_unnamed$error_message[1], "Error in custom_func: The list or data frame returned by custom_func must have names")
  
  # Non-scalar list - should cause error during internal processing
  # Should result in error = TRUE in the output table
  results_nonscalar <- run_custom_regional(dataset_vol, region_mask_vol, non_scalar_func)
  expect_s3_class(results_nonscalar, "tbl_df")
  expect_true(all(results_nonscalar$error))
  expect_match(results_nonscalar$error_message[1], "Error in custom_func: custom_func must return a named list of scalars")
})

# --- Verbose Option --- 

test_that("run_custom_regional runs with .verbose = TRUE", {
  # Just check that it runs without error, capturing output is complex
  # Remove subsetting of mask
  expect_silent({ 
      capture.output(run_custom_regional(dataset_vol, region_mask_vol, stats_func, .verbose = TRUE)) 
  })
})

# --- Cleanup --- 
# Ensure future plan is reset
future::plan(future::sequential)
</file>

<file path="tests/testthat/test_custom_searchlight.R">
library(testthat)
library(rMVPA)
library(neuroim2)

context("run_custom_searchlight")

# --- Setup ---

# Generate a sample volumetric dataset
dset_info_vol <- gen_sample_dataset(D = c(5, 5, 5), nobs = 20, nlevels = 2)
dataset_vol <- dset_info_vol$dataset

# Define a simple custom function for the searchlight
# Assume it receives data (samples x voxels_in_sphere) and info
# It should return a single named value or a list with one named value
mean_signal_sl <- function(sl_data, sl_info) {
  # sl_data: matrix of samples x voxels within the sphere
  # sl_info: list containing info like center voxel index, coords etc.
  #         (Exact structure depends on the final implementation)
  mean_val <- mean(sl_data, na.rm = TRUE)
  # Return a named list with one scalar value
  list(mean_signal = mean_val)
}

# --- Basic Functionality Test ---

test_that("run_custom_searchlight (standard) runs without error and returns correct structure", {
  # Run standard searchlight
  searchlight_results <- run_custom_searchlight(
    dataset = dataset_vol,
    custom_func = mean_signal_sl,
    radius = 5, # Use a slightly smaller radius for faster testing
    method = "standard",
    .cores = 1, # Keep it simple first
    .verbose = FALSE
  )

  # Check main object class
  expect_s3_class(searchlight_results, "searchlight_result")
  expect_true(is.list(searchlight_results))
  expect_named(searchlight_results, c("results", "n_voxels", "active_voxels", "metrics"))

  # Check metrics list
  expect_equal(searchlight_results$metrics, c("mean_signal"))
  expect_true(is.list(searchlight_results$results))
  expect_named(searchlight_results$results, c("mean_signal"))

  # Check the performance object for the metric
  perf_obj <- searchlight_results$results$mean_signal
  expect_s3_class(perf_obj, "searchlight_performance")
  expect_named(perf_obj, c("data", "metric_name", "n_nonzero", "summary_stats", "indices"))

  # Check the actual data map (NeuroVol)
  map_vol <- perf_obj$data
  expect_s3_class(map_vol, "NeuroVol")
  expect_equal(dim(map_vol), dim(dataset_vol$mask))
  expect_equal(space(map_vol), space(dataset_vol$mask))
  expect_true(is.numeric(values(map_vol)))
  
  # Check that some valid (non-NA) results were computed in the active mask areas
  active_indices <- which(as.logical(dataset_vol$mask))
  expect_false(all(is.na(values(map_vol)[active_indices]))) 
  
  # Check summary stats are populated
   expect_true(is.list(perf_obj$summary_stats))
   expect_named(perf_obj$summary_stats, c("mean", "sd", "min", "max"))
   expect_true(all(sapply(perf_obj$summary_stats, is.numeric))) 
   
   # Check indices (should be center voxels for standard)
    expect_true(is.numeric(perf_obj$indices))
    expect_equal(sort(perf_obj$indices), sort(active_indices)) # Standard covers all active centers
    
})


test_that("run_custom_searchlight (randomized) runs without error", {
  skip_on_cran() # May take a bit longer
  
  # Run randomized searchlight
  searchlight_results_rand <- run_custom_searchlight(
    dataset = dataset_vol,
    custom_func = mean_signal_sl,
    radius = 5,
    method = "randomized",
    niter = 10, # Fewer iterations for testing
    .cores = 1,
    .verbose = FALSE
  )

  # Basic structure checks (similar to standard)
  expect_s3_class(searchlight_results_rand, "searchlight_result")
  expect_named(searchlight_results_rand, c("results", "n_voxels", "active_voxels", "metrics"))
  expect_equal(searchlight_results_rand$metrics, c("mean_signal"))
  expect_s3_class(searchlight_results_rand$results$mean_signal, "searchlight_performance")
  map_vol_rand <- searchlight_results_rand$results$mean_signal$data
  expect_s3_class(map_vol_rand, "NeuroVol")
  expect_equal(dim(map_vol_rand), dim(dataset_vol$mask))
  
  # Check that some results exist (might not cover all voxels unlike standard)
  active_indices <- which(as.logical(dataset_vol$mask))
  expect_false(all(is.na(values(map_vol_rand)[active_indices]))) 
  
   # Indices should be NULL for randomized combined results
    expect_null(searchlight_results_rand$results$mean_signal$indices)
})


test_that("run_custom_searchlight handles errors in custom_func", {
  # Define a function that errors if fewer than 3 voxels in sphere
  error_sl_func <- function(sl_data, sl_info) {
    if (ncol(sl_data) < 3) {
      stop("Test Error: Too few voxels!")
    }
    list(mean_signal = mean(sl_data, na.rm = TRUE))
  }

  # Run with a small radius likely to trigger the error
  # Suppress warnings expected from the error handling during run
  suppressWarnings({
      searchlight_results_err <- run_custom_searchlight(
          dataset = dataset_vol,
          custom_func = error_sl_func,
          radius = 1, # Very small radius
          method = "standard",
          .cores = 1,
          .verbose = FALSE
      )
  })

  # Check structure is still valid
  expect_s3_class(searchlight_results_err, "searchlight_result")
  expect_named(searchlight_results_err$results, "mean_signal")
  map_vol_err <- searchlight_results_err$results$mean_signal$data
  expect_s3_class(map_vol_err, "NeuroVol")

  # Check that some values are NA (where the error occurred)
  # but not necessarily all, depending on sphere sizes
  active_indices <- which(as.logical(dataset_vol$mask))
  map_values_active <- values(map_vol_err)[active_indices]
  expect_true(any(is.na(map_values_active)))
  # Ensure not ALL are NA (unless the mask is tiny/radius catches none)
  if (length(active_indices) > 0) {
      expect_false(all(is.na(map_values_active))) 
  }
})

test_that("run_custom_searchlight runs in parallel (standard)", {
  skip_if_not_installed("future")
  skip_if_not_installed("future.apply")
  skip_on_cran()

  # Run sequentially
  results_seq <- run_custom_searchlight(
    dataset = dataset_vol,
    custom_func = mean_signal_sl,
    radius = 5,
    method = "standard",
    .cores = 1, .verbose = FALSE
  )

  # Run in parallel
  suppressMessages({
      results_par <- run_custom_searchlight(
        dataset = dataset_vol,
        custom_func = mean_signal_sl,
        radius = 5,
        method = "standard",
        .cores = 2, .verbose = FALSE
      )
  })
  
  # Reset plan
  future::plan(future::sequential)

  # Compare structure and basic properties
  expect_equal(names(results_par), names(results_seq))
  expect_equal(results_par$metrics, results_seq$metrics)
  expect_equal(dim(results_par$results$mean_signal$data), 
               dim(results_seq$results$mean_signal$data))
               
  # Compare numeric results (should be identical for standard method)
   expect_equal(values(results_par$results$mean_signal$data),
                values(results_seq$results$mean_signal$data)) 

})
</file>

<file path="tests/testthat/test_manova_searchlight.R">
context("manova searchlight")

test_that("standard one-way manova_searchlight runs without error", {
  dataset <- gen_sample_dataset(c(8,8,8), 100, blocks=3)
  
  rdes <- manova_design(~ block_var + Y, dataset$design$train_design)
  mspec <- manova_model(dataset$dataset, design=rdes)
  ret1 <- run_searchlight(mspec, radius=5)
 
  expect_true(!is.null(ret1))
  
})

test_that("randomized one-way manova_searchlight runs without error", {
  dataset <- gen_sample_dataset(c(8,8,8), 100, blocks=3)
  
  rdes <- manova_design(~ block_var + Y, dataset$design$train_design)
  mspec <- manova_model(dataset$dataset, design=rdes)
  ret1 <- run_searchlight(mspec, radius=5,method="randomized")
  
  expect_true(!is.null(ret1))
  
})

test_that("randomized one-way manova_searchlight runs without error", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  rdes <- manova_design(~ block_var + Y, dataset$design$train_design)
  mspec <- manova_model(dataset$dataset, design=rdes)
  ret1 <- run_searchlight(mspec, radius=8,method="randomized")
  
  expect_true(!is.null(ret1))
  
})
</file>

<file path="tests/simple_test.R">
# Simple test script
cat("Starting simple test script\n")

# Load required libraries
library(testthat)
library(rMVPA)

# Print a message
cat("Libraries loaded successfully\n")

# Try to run just one test
test_that("Simple test", {
  cat("Running simple test\n")
  expect_true(TRUE)
})

cat("Simple test completed\n")
</file>

<file path="R/custom.R">
#' Run a Custom Analysis Function Regionally
#'
#' Applies a user-defined function to the data within each specified region
#' of interest (ROI) and returns the results as a tibble.
#'
#' @param dataset An `mvpa_dataset` or `mvpa_surface_dataset` object.
#' @param region_mask A `NeuroVol` or `NeuroSurface` object where each region
#'   is identified by a unique integer greater than 0.
#' @param custom_func A function to apply to each ROI's data. It should
#'   accept two arguments:
#'     \itemize{
#'       \item `roi_data`: A matrix or tibble containing the data
#'             (samples x features) for the current ROI.
#'       \item `roi_info`: A list containing `id` (the region number) and
#'             `indices` (the feature indices for this ROI).
#'     }
#'   The function *must* return a named list or a single-row data frame
#'   (or tibble) containing scalar metric values.
#' @param ... Optional arguments passed to `mvpa_iterate` (e.g., `batch_size`).
#' @param .cores Number of cores to use for parallel processing via the
#'   `future` framework. Defaults to 1 (sequential). Set using
#'   `future::plan()` beforehand for more control.
#' @param .verbose Logical. If `TRUE`, prints progress messages during iteration.
#'   Defaults to `FALSE`.
#'
#' @return A `tibble` where each row corresponds to an ROI. It includes:
#'   \itemize{
#'     \item `id`: The ROI identifier (region number).
#'     \item Columns corresponding to the names returned by `custom_func`.
#'     \item `error`: Logical indicating if an error occurred for this ROI.
#'     \item `error_message`: The error message if an error occurred.
#'   }
#'
#' @details
#' This function provides a simplified interface for applying custom analyses
#' per ROI without needing to define a full `mvpa_model` specification or
#' implement S3 methods. It leverages the parallel processing and iteration
#' capabilities of `rMVPA`.
#'
#' The user-supplied `custom_func` performs the core calculation for each
#' ROI. The framework handles extracting data, iterating over ROIs (potentially
#' in parallel), catching errors from `custom_func`, and formatting the
#' output into a convenient flat table.
#'
#' @examples
#' # Generate sample dataset
#' dset_info <- gen_sample_dataset(D = c(8,8,8), nobs = 50, nlevels = 2)
#' dataset_obj <- dset_info$dataset
#' design_obj <- dset_info$design # Not used by custom_func here, but needed for setup
#'
#' # Create a region mask with 3 ROIs
#' mask_arr <- array(0, dim(dataset_obj$mask))
#' mask_arr[1:4, 1:4, 1:4] <- 1
#' mask_arr[5:8, 1:4, 1:4] <- 2
#' mask_arr[1:4, 5:8, 5:8] <- 3
#' region_mask_vol <- NeuroVol(mask_arr, space(dataset_obj$mask))
#'
#' # Define a custom function: calculate mean and sd for each ROI
#' my_roi_stats <- function(roi_data, roi_info) {
#'   # roi_data is samples x features matrix
#'   # roi_info$id is the region number
#'   # roi_info$indices are the feature indices
#'   mean_signal <- mean(roi_data, na.rm = TRUE)
#'   sd_signal <- sd(roi_data, na.rm = TRUE)
#'   num_features <- ncol(roi_data)
#'   list(
#'     roi_id = roi_info$id, # Can include id if desired, or rely on output table
#'     mean_signal = mean_signal,
#'     sd_signal = sd_signal,
#'     n_features = num_features
#'   )
#' }
#'
#' # Run the custom regional analysis
#' \donttest{
#' # Set up parallel processing (optional)
#' 
#' custom_results <- run_custom_regional(dataset_obj, region_mask_vol, my_roi_stats,
#'                                       .cores = 2, .verbose = TRUE)
#' print(custom_results)
#'
#' # Example with an error in one ROI
#' my_error_func <- function(roi_data, roi_info) {
#'   if (roi_info$id == 2) {
#'     stop("Something went wrong in ROI 2!")
#'   }
#'   list(mean_signal = mean(roi_data))
#' }
#'
#' error_results <- run_custom_regional(dataset_obj, region_mask_vol, my_error_func)
#' print(error_results)
#'
#' # Clean up parallel plan
#' future::plan(future::sequential)
#' }
#' @importFrom dplyr bind_rows select rename all_of
#' @importFrom tidyr unnest_wider
#' @importFrom tibble tibble is_tibble
#' @importFrom neuroim2 values indices space
#' @export
run_custom_regional <- function(dataset, region_mask, custom_func, ...,
                                .cores = 1, .verbose = FALSE) {

  # --- Input Validation ---
  if (!inherits(dataset, c("mvpa_dataset", "mvpa_surface_dataset"))) {
    stop("`dataset` must be an 'mvpa_dataset' or 'mvpa_surface_dataset' object.")
  }
  if (!inherits(region_mask, c("NeuroVol", "NeuroSurface"))) {
     stop("`region_mask` must be a 'NeuroVol' or 'NeuroSurface' object.")
  }
  if (!is.function(custom_func)) {
    stop("`custom_func` must be a function.")
  }
  if (!is.numeric(.cores) || .cores < 1 || round(.cores) != .cores) {
      stop("`.cores` must be a positive integer.")
  }


  # --- Setup Parallel Backend ---
  # Note: User is encouraged to set the plan *before* calling for more control
  if (.cores > 1 && !inherits(future::plan(), c("multicore", "multisession", "cluster"))) {
      if (requireNamespace("future.apply", quietly = TRUE)) {
          message("Setting future plan to 'multisession' with ", .cores, " workers for this function call.")
          old_plan <- future::plan(future::multisession, workers = .cores)
          on.exit(future::plan(old_plan), add = TRUE) # Restore previous plan on exit
      } else {
          warning("Parallel execution requested (cores > 1), but 'future' backend is not multisession/multicore ",
                  "and 'future.apply' is not installed to automatically set it. Running sequentially. ",
                  "Use future::plan() to set backend manually.", call. = FALSE)
      }
  }

  # --- Prepare for Iteration ---
  # Create a minimal dummy model spec - needed for mvpa_iterate internals
  dummy_spec <- list(
      dataset = dataset,
      design = NULL, # Not needed for custom func, but mvpa_iterate expects it
      # Key: we tell iterate we want performance metrics computed,
      # as we hijack the 'performance' slot for our custom metrics
      compute_performance = TRUE,
      return_predictions = FALSE, # Not needed
      return_fits = FALSE         # Not needed
  )
  class(dummy_spec) <- c("custom_internal_model_spec", "model_spec", "list") # Basic class

  # Define the internal processor function
  internal_processor <- function(model_spec, roi, rnum) {
      # Extract necessary info for the custom function
      roi_data <- tryCatch({
          neuroim2::values(roi$train_roi) # Assuming train_roi always exists
      }, error = function(e) { NULL }) # Handle cases where ROI data extraction fails

      roi_indices <- tryCatch({
          neuroim2::indices(roi$train_roi)
      }, error = function(e) { integer(0) })

      if (is.null(roi_data)) {
          # If ROI data extraction failed (e.g., ROI empty after filtering)
           return(tibble::tibble(result = list(NULL), indices = list(roi_indices),
                          performance = list(NULL), id = rnum,
                          error = TRUE, error_message = "Failed to extract ROI data",
                          warning = TRUE, warning_message = "Failed to extract ROI data"))
      }

      roi_info <- list(id = rnum, indices = roi_indices)

      tryCatch({
          # Execute the user's custom function
          perf_result_raw <- custom_func(roi_data, roi_info)

          # Validate and format the result
          if (is.data.frame(perf_result_raw) && nrow(perf_result_raw) == 1) {
              perf_list <- as.list(perf_result_raw)
          } else if (is.list(perf_result_raw) && !is.data.frame(perf_result_raw)) {
              if(!all(sapply(perf_result_raw, function(x) length(x) == 1 && is.atomic(x)))) {
                  stop("custom_func must return a named list of scalars or a single-row data.frame/tibble")
              }
              perf_list <- perf_result_raw
          } else {
              stop("custom_func must return a named list of scalars or a single-row data.frame/tibble")
          }

          # Check for unnamed list elements
          if (is.null(names(perf_list)) || any(names(perf_list) == "")) {
              stop("The list or data frame returned by custom_func must have names for all elements/columns.")
          }

          # Wrap into the tibble structure expected by mvpa_iterate
          tibble::tibble(result = list(NULL), # No model result needed
                         indices = list(roi_info$indices),
                         performance = list(perf_list), # Store metrics here
                         id = rnum, error = FALSE, error_message = "~",
                         warning = FALSE, warning_message = "~")

      }, error = function(e) {
          # Handle errors from custom_func
           tibble::tibble(result = list(NULL), indices = list(roi_indices),
                          performance = list(NULL), id = rnum,
                          error = TRUE, error_message = paste("Error in custom_func:", e$message),
                          warning = TRUE, warning_message = paste("Error in custom_func:", e$message))
      })
  }

  # --- Run Iteration ---
  futile.logger::flog.info("Starting custom regional analysis...")
  prepped <- prep_regional(dummy_spec, region_mask)
  iteration_results <- mvpa_iterate(
    dummy_spec,
    prepped$vox_iter,
    ids = prepped$region_set,
    processor = internal_processor,
    verbose = .verbose,
    ... # Pass other mvpa_iterate args
  )
  futile.logger::flog.info("Custom regional analysis iteration complete.")


  # --- Format Final Output ---
  if (nrow(iteration_results) == 0) {
    warning("No ROIs were successfully processed.")
    return(tibble::tibble(id = integer(), error = logical(), error_message = character()))
  }
  
  # Identify expected columns from the first successful result
  first_success_idx <- which(!iteration_results$error)[1]
  expected_names <- if (!is.na(first_success_idx)) {
      names(iteration_results$performance[[first_success_idx]])
  } else {
       # If all errored, there are no expected metric columns
       character(0) 
  }
  
  # Prepare for unnesting - ensure consistent structure even with errors
  results_to_process <- iteration_results %>%
    dplyr::mutate(performance = lapply(seq_len(nrow(iteration_results)), function(i) {
        p <- .data$performance[[i]]
        err <- .data$error[[i]]
        
        # Create a placeholder list with NA for all expected names
        placeholder <- stats::setNames(as.list(rep(NA, length(expected_names))), expected_names)
        
        if (err || is.null(p)) {
          # If error or NULL result, return the placeholder
          placeholder
        } else {
          # If success, fill the placeholder with actual values found
          common_names <- intersect(names(p), expected_names)
          if (length(common_names) > 0) {
              placeholder[common_names] <- p[common_names]
          }
          placeholder
        }
    }))
  

  # Unnest the performance list-column
  final_table <- tryCatch({
      # Ensure performance is treated as a list column for unnesting
      results_to_process$performance <- as.list(results_to_process$performance)
      
      # Use names_repair to handle potential duplicates (though unlikely now)
      tidyr::unnest_wider(results_to_process, "performance", names_repair = "minimal") %>% 
      # Select explicitly to control order and remove intermediate columns
      dplyr::select(dplyr::all_of(c("id", expected_names, "error", "error_message")))
                     
  }, error = function(e){
       warning(paste("Could not automatically flatten performance metrics:", e$message, 
                     "Returning results in a list column."), call. = FALSE)
       # Ensure fallback also has the correct columns, even if performance is just a list
       fallback_names <- c("id", "error", "error_message")
       if ("performance" %in% names(results_to_process)) {
           fallback_names <- c(fallback_names, "performance")
       }
        # Fallback returns the processed structure before unnesting attempt
        results_to_process %>%
         dplyr::select(dplyr::any_of(fallback_names)) # Use any_of for robustness
  })


  futile.logger::flog.info("Finished formatting custom regional results.")
  return(final_table)
}

# Add a dummy method for the internal class to satisfy mvpa_iterate checks
#' @export
#' @keywords internal
process_roi.custom_internal_model_spec <- function(mod_spec, roi, rnum, ...) {
  # This should not be called directly if the processor is provided,
  # but needs to exist.
  stop("Internal error: process_roi called for custom_internal_model_spec")
}


#' Run a Custom Analysis Function in a Searchlight
#'
#' Applies a user-defined function to the data within each searchlight sphere
#' and returns the results, typically as `NeuroVol` or `NeuroSurface` objects
#' within a `searchlight_result` structure.
#'
#' @param dataset An `mvpa_dataset` or `mvpa_surface_dataset` object.
#' @param custom_func A function to apply within each searchlight sphere. It
#'   should accept two arguments:
#'     \itemize{
#'       \item `sl_data`: A matrix or tibble containing the data
#'             (samples x features_in_sphere) for the current sphere.
#'       \item `sl_info`: A list containing information about the sphere,
#'             including `center_index` (the index of the center voxel/vertex),
#'             `indices` (the indices of all features within the sphere), and
#'             potentially `coords` (coordinates of the center).
#'     }
#'   The function *must* return a named list or a single-row data frame
#'   (or tibble) containing scalar metric values. All spheres must return the
#'   same named metrics.
#' @param radius The radius of the searchlight sphere (in mm for volumes,
#'   or vertex connections for surfaces - see `neuroim2::spherical_roi`).
#' @param method The type of searchlight: "standard" (systematically covers
#'   all center voxels) or "randomized" (samples spheres randomly, useful for
#'   large datasets). Defaults to "standard".
#' @param niter The number of iterations for a "randomized" searchlight.
#'   Ignored if `method = "standard"`. Defaults to 100.
#' @param ... Optional arguments passed to `mvpa_iterate` (e.g., `batch_size`).
#' @param .cores Number of cores to use for parallel processing via the
#'   `future` framework. Defaults to 1 (sequential). Set using
#'   `future::plan()` beforehand for more control.
#' @param .verbose Logical. If `TRUE`, prints progress messages during iteration.
#'   Defaults to `FALSE`.
#'
#' @return A `searchlight_result` object (see `rMVPA::wrap_out`). This is a list
#'   containing:
#'   \itemize{
#'     \item `results`: A named list where each element corresponds to a metric
#'           returned by `custom_func`. Each element is itself a
#'           `searchlight_performance` object containing a `NeuroVol` or
#'           `NeuroSurface` (`$data`) with the metric values mapped back to the
#'           brain space, along with summary statistics (`$summary_stats`).
#'     \item `metrics`: A character vector of the metric names.
#'     \item `n_voxels`, `active_voxels`: Information about the dataset mask.
#'   }
#'   If `method = "randomized"`, the values in the output maps represent the
#'   average metric value for each voxel across all spheres it participated in.
#'
#' @details
#' This function provides a flexible way to perform custom analyses across the
#' brain using a searchlight approach, without defining a full `mvpa_model`.
#' It handles iterating over searchlight spheres, extracting data, running the
#' custom function (potentially in parallel), handling errors, and combining
#' the results back into brain-space maps.
#'
#' The `custom_func` performs the core calculation for each sphere. The framework
#' manages the iteration, data handling, parallelization, error catching, and
#' result aggregation.
#'
#' For `method = "standard"`, the function iterates through every active voxel/vertex
#' in the dataset mask as a potential sphere center.
#' For `method = "randomized"`, it randomly selects sphere centers for `niter`
#' iterations. The final map represents an average of the results from spheres
#' covering each voxel. This requires the custom function's results to be meaningfully
#' averageable.
#'
#' **Important**: The `custom_func` must consistently return the same set of named
#' scalar metrics for every sphere it successfully processes.
#'
#' @examples
#' # Generate sample dataset
#' dset_info <- gen_sample_dataset(D = c(10, 10, 10), nobs = 30, nlevels = 2)
#' dataset_obj <- dset_info$dataset
#'
#' # Define a custom function: calculate mean and sd within the sphere
#' my_sl_stats <- function(sl_data, sl_info) {
#'   # sl_data is samples x features_in_sphere matrix
#'   # sl_info contains center_index, indices, etc.
#'   mean_signal <- mean(sl_data, na.rm = TRUE)
#'   sd_signal <- sd(sl_data, na.rm = TRUE)
#'   n_features <- ncol(sl_data)
#'   list(
#'     mean_signal = mean_signal,
#'     sd_signal = sd_signal,
#'     n_vox_in_sphere = n_features
#'   )
#' }
#'
#' # Run the custom searchlight (standard method)
#' \donttest{
#' 
#' custom_sl_results <- run_custom_searchlight(dataset_obj, my_sl_stats,
#'                                             radius = 7, method = "standard",
#'                                             .cores = 2, .verbose = TRUE)
#' print(custom_sl_results)
#'
#' # Access the NeuroVol for a specific metric
#' mean_signal_map <- custom_sl_results$results$mean_signal$data
#' # plot(mean_signal_map) # Requires neuroim2 plotting capabilities
#'
#' # Example with an error in some spheres (e.g., if too few voxels)
#' my_error_sl_func <- function(sl_data, sl_info) {
#'   if (ncol(sl_data) < 5) {
#'     stop("Too few voxels in this sphere!")
#'   }
#'   list(mean_signal = mean(sl_data))
#' }
#'
#' error_sl_results <- run_custom_searchlight(dataset_obj, my_error_sl_func,
#'                                            radius = 4, method = "standard")
#' print(error_sl_results) # Errors will be caught, corresponding voxels may be NA
#'
#' # Run randomized searchlight (faster for large datasets/radii)
#' custom_sl_rand_results <- run_custom_searchlight(dataset_obj, my_sl_stats,
#'                                                  radius = 7, method = "randomized",
#'                                                  niter = 50, # Fewer iterations for example
#'                                                  .cores = 2, .verbose = TRUE)
#' print(custom_sl_rand_results)
#'
#' # Clean up parallel plan
#' future::plan(future::sequential)
#' }
#'
#' @importFrom dplyr bind_rows select filter pull mutate
#' @importFrom tidyr unnest_wider
#' @importFrom tibble tibble is_tibble
#' @importFrom Matrix sparseMatrix 
#' @importFrom purrr map map_int map_dbl map_lgl list_assign
#' @importFrom stats setNames sd
#' @importFrom futile.logger flog.info flog.warn flog.error flog.debug
#' @importFrom methods is
#' @importFrom future plan multisession sequential %<-% %globals%
#' @importFrom future.apply future_lapply future_mapply
#' @importFrom assertthat assert_that
#'
#' @seealso \code{\link{run_custom_regional}}, \code{\link{run_searchlight_base}}, \code{\link{get_searchlight}}, \code{\link{mvpa_iterate}}
#' @export
run_custom_searchlight <- function(dataset, custom_func, radius,
                                   method = c("standard", "randomized"),
                                   niter = 100, ...,
                                   .cores = 1, .verbose = FALSE) {

  # --- Input Validation ---
  method <- match.arg(method)
  if (!inherits(dataset, c("mvpa_dataset", "mvpa_surface_dataset"))) {
    stop("`dataset` must be an 'mvpa_dataset' or 'mvpa_surface_dataset' object.")
  }
  if (!is.function(custom_func)) {
    stop("`custom_func` must be a function.")
  }
  if (!is.numeric(radius) || radius <= 0) {
      stop("`radius` must be a positive number.")
  }
  if (method == "randomized" && (!is.numeric(niter) || niter < 1 || round(niter) != niter)) {
      stop("`niter` must be a positive integer for randomized searchlight.")
  }
  if (!is.numeric(.cores) || .cores < 1 || round(.cores) != .cores) {
      stop("`.cores` must be a positive integer.")
  }

  # --- Setup Parallel Backend ---
  if (.cores > 1 && !inherits(future::plan(), c("multicore", "multisession", "cluster"))) {
      if (requireNamespace("future.apply", quietly = TRUE)) {
          message("Setting future plan to 'multisession' with ", .cores, " workers for this function call.")
          old_plan <- future::plan(future::multisession, workers = .cores)
          on.exit(future::plan(old_plan), add = TRUE) # Restore previous plan on exit
      } else {
          warning("Parallel execution requested (cores > 1), but 'future' backend is not multisession/multicore ",
                  "and 'future.apply' is not installed to automatically set it. Running sequentially. ",
                  "Use future::plan() to set backend manually.", call. = FALSE)
      }
  }

  # --- Prepare for Iteration ---
  # Create a minimal dummy model spec - needed for mvpa_iterate internals,
  # but most fields won't be used directly by our custom processor/combiner.
  # Pass dataset for combiner access.
  dummy_spec <- list(
      dataset = dataset,
      design = NULL,
      compute_performance = TRUE, # Hijack performance slot for custom metrics
      return_predictions = FALSE,
      return_fits = FALSE
  )
  class(dummy_spec) <- c("custom_internal_model_spec", "model_spec", "list")

  # Define the internal processor function for ONE searchlight sphere
  internal_processor <- function(model_spec, roi, rnum) {
      # `roi` here is the searchlight object (e.g., ROIVolume)
      # `rnum` is the index of the center voxel/vertex
      
      sl_data <- tryCatch({
          neuroim2::values(roi$train_roi) # Data within the sphere (samples x features)
      }, error = function(e) { NULL })

      sl_indices <- tryCatch({
           neuroim2::indices(roi$train_roi) # Feature indices within the sphere
      }, error = function(e) { integer(0) })

      if (is.null(sl_data) || ncol(sl_data) == 0) {
          # Handle cases where ROI data extraction fails or sphere is empty
           return(tibble::tibble(result = list(NULL), # No model result
                          indices = list(sl_indices), # Indices within sphere
                          performance = list(NULL), # custom_func output goes here
                          id = rnum, # Center voxel index
                          error = TRUE, error_message = "Failed to extract data or sphere empty",
                          warning = TRUE, warning_message = "Failed to extract data or sphere empty"))
      }

      sl_info <- list(center_index = rnum, indices = sl_indices)
      # Could add coords: sl_info$coords <- neuroim2::coords(neuroim2::spatial(roi$train_roi))

      tryCatch({
          # Execute the user's custom function
          perf_result_raw <- custom_func(sl_data, sl_info)

          # Validate and format the result
          if (tibble::is_tibble(perf_result_raw) && nrow(perf_result_raw) == 1) {
              perf_list <- as.list(perf_result_raw)
          } else if (is.list(perf_result_raw) && !is.data.frame(perf_result_raw)) {
              if(!all(sapply(perf_result_raw, function(x) length(x) == 1 && is.atomic(x)))) {
                  stop("custom_func must return a named list of scalars or a single-row data.frame/tibble")
              }
              perf_list <- perf_result_raw
          } else {
              stop("custom_func must return a named list of scalars or a single-row data.frame/tibble")
          }

          # Check for unnamed list elements
          if (is.null(names(perf_list)) || any(names(perf_list) == "")) {
              stop("The list or data frame returned by custom_func must have names for all elements/columns.")
          }
           # Check all are scalar
           if (!all(sapply(perf_list, function(x) length(x) == 1 && is.atomic(x)))) {
              stop("custom_func must return a named list of scalars or a single-row data.frame/tibble (all elements/columns must be single atomic values).")
           }


          # Wrap into the tibble structure expected by mvpa_iterate/combiner
          tibble::tibble(result = list(NULL), # No model result needed
                         indices = list(sl_info$indices), # Indices within sphere
                         performance = list(perf_list), # Store custom metrics here
                         id = rnum, # Center voxel index
                         error = FALSE, error_message = "~",
                         warning = FALSE, warning_message = "~")

      }, error = function(e) {
          # Handle errors from custom_func
          # Ensure the structure is consistent even on error
           tibble::tibble(result = list(NULL), indices = list(sl_indices),
                          performance = list(NULL), # Performance is NULL on error
                          id = rnum,
                          error = TRUE, error_message = paste("Error in custom_func:", e$message),
                          warning = TRUE, warning_message = paste("Error in custom_func:", e$message))
      })
  }

  # --- Run Iteration ---
  flog.info("Starting custom searchlight analysis (method: %s, radius: %s mm)...", method, radius)

  if (method == "standard") {
      slight <- get_searchlight(dataset, "standard", radius)
      center_indices <- which(dataset$mask > 0) # Center on all active voxels
      flog.info("Preparing %d standard searchlight spheres...", length(center_indices))

      iteration_results <- mvpa_iterate(
          dummy_spec,
          slight,
          ids = center_indices,
          processor = internal_processor,
          verbose = .verbose,
          ... # Pass other mvpa_iterate args like batch_size
      )
      
      flog.info("Combining results from standard searchlight...")
      # Pass dataset directly for combiner access
      final_result <- combine_custom_standard(dataset, iteration_results) 

  } else { # method == "randomized"
      # Randomized searchlight needs a loop and a different combiner
       flog.info("Running %d randomized searchlight iterations...", niter)
       all_iteration_results <- list()
       
       for (i in 1:niter) {
            if (.verbose) flog.info("Randomized iteration %d/%d", i, niter)
            # Get random spheres for this iteration
            slight <- get_searchlight(dataset, "randomized", radius) 
            
            # Extract center indices (handling both NeuroVol and NeuroSurface cases)
             center_indices <- if (methods::is(slight[[1]], "ROIVolume")) {
               sapply(slight, function(x) x@parent_index)
             } else if (methods::is(slight[[1]], "ROISurface")) {
               sapply(slight, function(x) x@center_index)
             } else {
                # Fallback/error - should identify centers based on slight type
                 warning("Could not determine center indices for randomized searchlight iteration.")
                 integer(0) 
             }
            
             if (length(slight) == 0 || length(center_indices) == 0) {
                 flog.warn("No searchlight spheres generated in iteration %d. Skipping.", i)
                 next
             }

             iter_res <- mvpa_iterate(
                 dummy_spec,
                 slight,
                 ids = center_indices, # IDs are the center indices for this random batch
                 processor = internal_processor,
                 verbose = FALSE, # Usually too noisy for randomized inner loop
                 ...
             )
             # Store results, including sphere indices for averaging
             # Need indices *within* the sphere, which processor calculates
              all_iteration_results[[length(all_iteration_results) + 1]] <- iter_res
       }
       
       if (length(all_iteration_results) == 0) {
           stop("No results generated from any randomized searchlight iteration.")
       }
       
       # Combine results from all iterations
       combined_iterations <- dplyr::bind_rows(all_iteration_results)
       flog.info("Combining results from randomized searchlight (%d total spheres processed)...", nrow(combined_iterations))
       final_result <- combine_custom_randomized(dataset, combined_iterations)
  }


  flog.info("Finished custom searchlight analysis.")
  return(final_result)
}


#' Combine Custom Standard Searchlight Results
#'
#' Internal function to combine results from a standard custom searchlight run.
#' Creates a `searchlight_result` object with NeuroVol/NeuroSurface for each metric.
#'
#' @param dataset The original mvpa_dataset object.
#' @param iteration_results The raw tibble output from `mvpa_iterate`.
#' @return A `searchlight_result` object.
#' @keywords internal
combine_custom_standard <- function(dataset, iteration_results) {
  good_results <- iteration_results %>% dplyr::filter(!error)
  bad_results <- iteration_results %>% dplyr::filter(error)

  if (nrow(good_results) == 0) {
    flog.error("No successful results for standard custom searchlight. Examining errors:")
    if (nrow(bad_results) > 0) {
      error_summary <- table(bad_results$error_message)
      for (i in seq_along(error_summary)) {
        flog.error("  - %s: %d occurrences", names(error_summary)[i], error_summary[i])
      }
    }
    stop("No valid results for standard custom searchlight: all spheres failed.")
  }

  # Extract performance lists and center IDs
  perf_lists <- good_results$performance
  center_ids <- good_results$id # Center voxel indices for successful spheres

  # Check consistency of metric names across results
  metric_names <- names(perf_lists[[1]])
  if (is.null(metric_names) || any(metric_names == "")) {
       stop("Internal error: First successful result has unnamed/empty metrics.")
  }
  
  # Validate that all successful results have the same metric names
  all_names_consistent <- all(sapply(perf_lists[-1], function(p) {
      identical(names(p), metric_names)
  }))
  if (!all_names_consistent) {
      stop("Custom function returned inconsistent metric names across different spheres.")
  }
  
  num_metrics <- length(metric_names)
  num_results <- nrow(good_results)

  # Create a matrix: rows = successful center voxels, cols = metrics
  perf_mat <- matrix(NA_real_, nrow = num_results, ncol = num_metrics,
                     dimnames = list(NULL, metric_names))

  # Fill the matrix
  for (i in 1:num_results) {
     # Ensure the list has the expected names before assigning
     p_list <- perf_lists[[i]]
     if (identical(names(p_list), metric_names)) {
        perf_mat[i, ] <- unlist(p_list)
     } else {
         # This shouldn't happen due to the check above, but as a safeguard:
         flog.warn("Metric name mismatch for center voxel %d. Setting to NA.", center_ids[i])
         # Attempt partial matching if names are just reordered (less robust)
         # matched_indices <- match(metric_names, names(p_list))
         # perf_mat[i, !is.na(matched_indices)] <- unlist(p_list[matched_indices[!is.na(matched_indices)]])
     }
  }

  # Use the wrap_out structure (adapted)
  out_list <- lapply(1:num_metrics, function(i) {
    metric_name <- metric_names[i]
    metric_data <- perf_mat[, i]
    # Create the performance object, passing the vector and center IDs
    create_searchlight_performance(dataset, metric_data, center_ids) # Needs vector + IDs
  })
  names(out_list) <- metric_names

  # Create the final searchlight_result object
  structure(
    list(
      results = out_list,
      n_voxels = length(dataset$mask),
      active_voxels = sum(dataset$mask > 0),
      metrics = metric_names
    ),
    class = c("searchlight_result", "list")
  )
}


#' Combine Custom Randomized Searchlight Results
#'
#' Internal function to combine results from a randomized custom searchlight run.
#' Averages results for each metric across overlapping spheres.
#'
#' @param dataset The original mvpa_dataset object.
#' @param iteration_results The raw tibble output from *all* iterations of `mvpa_iterate`.
#' @return A `searchlight_result` object.
#' @keywords internal
combine_custom_randomized <- function(dataset, iteration_results) {
   good_results <- iteration_results %>% dplyr::filter(!error)
   bad_results <- iteration_results %>% dplyr::filter(error)

   if (nrow(good_results) == 0) {
      flog.error("No successful results for randomized custom searchlight. Examining errors:")
      if (nrow(bad_results) > 0) {
          error_summary <- table(bad_results$error_message)
          for (i in seq_along(error_summary)) {
              flog.error("  - %s: %d occurrences", names(error_summary)[i], error_summary[i])
          }
      }
      stop("No valid results for randomized custom searchlight: all spheres failed.")
   }

   # Extract performance, sphere indices, and check metric consistency
   perf_lists <- good_results$performance
   sphere_indices_list <- good_results$indices # List of indices *within* each sphere
   
   metric_names <- names(perf_lists[[1]])
    if (is.null(metric_names) || any(metric_names == "")) {
       stop("Internal error: First successful result has unnamed/empty metrics.")
   }
   num_metrics <- length(metric_names)
   
   all_names_consistent <- all(sapply(perf_lists[-1], function(p) {
      identical(names(p), metric_names)
   }))
   if (!all_names_consistent) {
      stop("Custom function returned inconsistent metric names across different spheres.")
   }

   # --- Accumulation Logic (similar to combine_randomized) ---
   
   # Get all unique voxel indices covered by any successful sphere
   all_covered_indices <- unique(sort(unlist(sphere_indices_list)))
   if (length(all_covered_indices) == 0) {
       stop("No voxels were covered by any successful searchlight sphere.")
   }
   
   # Count how many spheres covered each voxel index
   index_counts <- table(unlist(sphere_indices_list))
   # Ensure index_counts uses character keys for subsetting sparseMatrix later
   names(index_counts) <- as.character(names(index_counts)) 
   
   # Create sparse matrices to accumulate results for *each* metric
   # Rows = all voxels in mask, Cols = 1 (for each metric)
   accumulators <- list()
   total_voxels_in_mask <- length(dataset$mask) # Total size needed for sparse matrix

   for (m_name in metric_names) {
       accumulators[[m_name]] <- Matrix::sparseMatrix(
           i = integer(0), j = integer(0), x = numeric(0),
           dims = c(total_voxels_in_mask, 1) 
       )
   }

   # Iterate through each successful sphere result and add its metric values
   # to the corresponding voxels in the accumulators
   for (i in 1:nrow(good_results)) {
       sphere_vox_indices <- sphere_indices_list[[i]]
       perf_values <- perf_lists[[i]] # Named list of metrics for this sphere

       if (length(sphere_vox_indices) > 0 && !is.null(perf_values)) {
           # Add each metric value to the appropriate accumulator
            for (m_name in metric_names) {
                metric_value <- perf_values[[m_name]]
                if (!is.null(metric_value) && is.numeric(metric_value) && length(metric_value) == 1) {
                   # Add the single metric value to all voxels in this sphere
                   tryCatch({
                       # Ensure indices are valid before subsetting
                       valid_indices <- sphere_vox_indices[sphere_vox_indices <= total_voxels_in_mask & sphere_vox_indices > 0]
                       if (length(valid_indices) > 0) {
                          accumulators[[m_name]][valid_indices, 1] <- accumulators[[m_name]][valid_indices, 1] + metric_value
                       }
                    }, error = function(e) {
                         flog.warn("Error adding metric '%s' for sphere %d (center %d): %s", 
                                   m_name, i, good_results$id[i], e$message)
                    })
                } else {
                     flog.warn("Invalid metric value ('%s') for sphere %d (center %d). Skipping.", 
                               m_name, i, good_results$id[i])
                }
            }
       }
   }

   # --- Normalization and Output Wrapping ---
   out_list <- list()
   active_mask_indices_char <- as.character(which(dataset$mask > 0))
   
   for (m_name in metric_names) {
       acc_matrix <- accumulators[[m_name]]
       
       # Identify indices present in both accumulator and counts
       indices_to_normalize_char <- intersect(rownames(acc_matrix), names(index_counts))
       indices_to_normalize_num <- as.numeric(indices_to_normalize_char)

       if (length(indices_to_normalize_num) > 0) {
           # Extract counts for normalization
           counts_for_norm <- index_counts[indices_to_normalize_char]
           
           # Perform normalization (division) using sweep or direct division
           # Ensure counts are numeric for division
            normalized_values <- acc_matrix[indices_to_normalize_num, 1] / as.numeric(counts_for_norm)
           
           # Update the accumulator matrix with normalized values
           acc_matrix[indices_to_normalize_num, 1] <- normalized_values
            
            # Set other values (those with counts but maybe no accumulation?) to 0 or NA? Let's keep them as they are (likely 0)
       } else {
           flog.warn("No overlapping indices found for normalization for metric '%s'.", m_name)
       }

       # Wrap the normalized result vector (column 1 of sparse matrix)
       # Need to extract the vector correctly, considering only relevant indices
       final_vector <- as.vector(acc_matrix[,1]) # Full vector matching dataset$mask length

       # Wrap output using create_searchlight_performance (pass the full vector, no IDs needed here)
       out_list[[m_name]] <- create_searchlight_performance(dataset, final_vector, ids=NULL)
   }

   # Create the final searchlight_result object
   structure(
       list(
           results = out_list,
           n_voxels = length(dataset$mask),
           active_voxels = sum(dataset$mask > 0),
           metrics = metric_names
       ),
       class = c("searchlight_result", "list")
   )
}
</file>

<file path="R/rMVPA.R">
#' rMVPA: A package for multi-voxel pattern analysis (MVPA)
#'
#' The rMVPA package provides tools for running region-of-interest and searchlight MVPA analyses.
#' 
#' 
#' @name rMVPA
NULL

##

if(getRversion() >= "2.15.1")  utils::globalVariables(c("."))
</file>

<file path="tests/testthat/test_distfun.R">
library(testthat)

# Test for create_dist Function
test_that("create_dist returns correct structure", {
  dist_obj <- create_dist("euclidean", labels = c("A", "B", "C"))
  expect_true(inherits(dist_obj, "distfun"))
  expect_true(inherits(dist_obj, "euclidean"))
  expect_equal(dist_obj$labels, c("A", "B", "C"))
})

# Tests for pairwise distance functions
test_that("pairwise_dist.correlation calculates correctly", {
  X <- matrix(rnorm(100), 10, 10)
  block <- rep(1:2, each=5)
  dist_obj <- cordist(rep("A", 10), method="pearson")
  result <- pairwise_dist(dist_obj, X)
  expect_true(inherits(result, "matrix"))
  expect_equal(length(result), 10*10)
})

test_that("pairwise_dist.euclidean handles empty and single-row matrices", {
  dist_obj <- structure(list(), class="euclidean")
  
  # Empty matrix case
  X <- matrix(numeric(0), ncol = 10)
  result <- pairwise_dist(dist_obj, X)
  expect_equal(dim(result), c(0,0))
  
  # Single row case
  X <- matrix(rnorm(10), nrow = 1)
  result <- pairwise_dist(dist_obj, X)
  expect_equal(dim(result), c(1,1))
  expect_equal(result[1,1], 0)
})

# Testing Mahalanobis distance with synthetic data
test_that("pairwise_dist.mahalanobis computes correctly", {
  dist_obj <- structure(list(), class="mahalanobis")
  X <- matrix(rnorm(20), nrow=4)
  result <- pairwise_dist(dist_obj, X)
  expect_true(is.matrix(result))
  expect_equal(dim(result), c(4,4))
  expect_equal(diag(result), rep(0, 4))
})

test_that("pairwise_dist.cordist calculates correctly", {
  dist_obj <- structure(list(method="pearson"), class="cordist")
  
  # Normal case
  X <- matrix(rnorm(20), ncol=4)  # 5x4 matrix
  result <- pairwise_dist(dist_obj, X)
  expect_true(is.matrix(result))
  expect_equal(dim(result), c(nrow(X), nrow(X)))  # Should match number of rows
  expect_equal(diag(result), rep(0, nrow(X)))     # Diagonal should match number of rows
  
  # Zero variance case - explicitly create a row with constant values
  X <- rbind(rep(1, 4),                          # Constant row
             matrix(rnorm(12), ncol=4))          # 3 more rows of random data
  expect_warning(
    result <- pairwise_dist(dist_obj, X),
    "the standard deviation is zero"
  )
  expect_true(is.matrix(result))
  expect_true(any(is.na(result)))
})
# Add new tests for pcadist
test_that("pairwise_dist.pcadist works correctly", {
  X <- matrix(rnorm(100), 10, 10)
  
  # Test with default settings
  dist_obj <- pcadist(labels=1:10)
  result1 <- pairwise_dist(dist_obj, X)
  expect_true(inherits(result1, "matrix"))
  expect_equal(dim(result1), c(10,10))
  
  # Test with custom settings
  dist_obj2 <- pcadist(labels=1:10, ncomp=3, whiten=FALSE, dist_method="manhattan")
  result2 <- pairwise_dist(dist_obj2, X)
  expect_true(inherits(result2, "matrix"))
  expect_equal(dim(result2), c(10,10))
})

# Add test for robustmahadist
test_that("pairwise_dist.robustmahadist handles outliers well", {
  # Create data with outliers
  X <- matrix(rnorm(100), 10, 10)
  X[1,] <- X[1,] * 10  # Create an outlier
  
  dist_obj <- robustmahadist()
  result <- pairwise_dist(dist_obj, X)
  
  expect_true(inherits(result, "matrix"))
  expect_equal(dim(result), c(10,10))
  # Distances should be symmetric
  expect_equal(result, t(result))
})

test_that("second_order_similarity computes correct scores", {
  # Create a test matrix X with 10 samples and 10 features
  X <- matrix(rnorm(100), 10, 10)
  
  # Create a reference similarity matrix D with the same dimensions as X
  D <- matrix(rnorm(100), 10, 10)
  
  # Define block structure where each block contains exactly two elements
  block <- rep(1:5, each=2)
  
  # Assuming 'create_dist' correctly initializes a Euclidean distance function object
  dist_fun <- create_dist("euclidean", labels = 1:10)
  
  # Compute the similarity scores
  scores <- second_order_similarity(dist_fun, X, D, block, method = "pearson")
  
  # Check the type and length of the output scores
  expect_type(scores, "double")
  expect_length(scores, 10)
  
  # Ensure no scores are NA when blocks are handled correctly
  # This checks that the scores are computed and are not NA due to block handling errors
  expect_false(any(is.na(scores)), "There should be no NA values in the scores if blocks are handled correctly.")
  
  # Test specific behavior for each block
  # Here, we're not expecting any specific outcomes, but you can add these checks based on your function's specifics
  for (i in seq_along(block)) {
    other_block_indices <- which(block != block[i])
    if (length(other_block_indices) > 0) {
      # Test that the score is not NA if there are valid indices to compare
      expect_true(!is.na(scores[i]), sprintf("Score for index %d should not be NA", i))
    }
  }
})

test_that("second_order_similarity handles edge cases", {
  X <- matrix(rnorm(20), 4, 5)
  D <- matrix(c(
    0, 1, 2, 3,
    1, 0, 4, 5,
    2, 4, 0, 6,
    3, 5, 6, 0
  ), nrow=4)
  
  # Test with all samples in same block
  block_same <- rep(1, 4)
  scores_same <- second_order_similarity(eucdist(), X, D, block_same)
  expect_true(all(is.na(scores_same)))
  
  # Test with each sample in different block
  block_diff <- 1:4
  scores_diff <- second_order_similarity(eucdist(), X, D, block_diff)
  expect_false(any(is.na(scores_diff)))
  
  # Test with paired blocks
  block_paired <- rep(1:2, each=2)
  scores_paired <- second_order_similarity(eucdist(), X, D, block_paired)
  expect_equal(sum(is.na(scores_paired)), 0)
})

#' @export
pairwise_dist.default <- function(obj, X, ...) {
  stop(sprintf("pairwise_dist not implemented for objects of class %s", 
               paste(class(obj), collapse=", ")))
}

#' @export
pairwise_dist.cordist <- function(obj, X, ...) {
  if (nrow(X) < 2) {
    return(matrix(0, nrow(X), nrow(X)))
  }
  
  # Check for zero standard deviation
  sds <- apply(X, 2, sd)
  if (any(sds == 0)) {
    warning("the standard deviation is zero")
    # Return matrix with NAs except diagonal
    d <- matrix(NA, nrow(X), nrow(X))
    diag(d) <- 0
    return(d)
  }
  
  # Calculate correlations and convert to distances
  cors <- stats::cor(t(X), method = obj$method)
  1 - cors
}
</file>

<file path="tests/testthat/test_rsa_searchlight.R">
context("rsa searchlight")

test_that("standard rsa_searchlight and blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  Dmat <- dist(matrix(rnorm(100*100), 100, 100))
  rdes <- rsa_design(~ Dmat, list(Dmat=Dmat, block=dataset$design$block_var), block_var="block")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="lm")
  ret1 <- run_searchlight(mspec, radius=4, method="standard")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="rfit")
  ret2 <- run_searchlight(mspec, radius=4, method="standard")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="pearson")
  ret3 <- run_searchlight(mspec, radius=4, method="standard")
  mspec <- rsa_model(dataset$dataset, design=rdes)
  ret4 <- run_searchlight(mspec, radius=4, method="standard")
  expect_true(!is.null(ret1) && !is.null(ret2) && !is.null(ret3) && !is.null(ret4))
  
})

test_that("standard rsa_searchlight with multiple distance matrices and blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  Dmat1 <- dist(matrix(rnorm(100*100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100*100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1=Dmat1, Dmat2=Dmat2, block=dataset$design$block_var), block_var="block")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="lm")
  ret1 <- run_searchlight(mspec, radius=4, method="standard")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="rfit")
  ret2 <- run_searchlight(mspec, radius=4, method="standard")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="pearson")
  ret3 <- run_searchlight(mspec, radius=4, method="standard")
  mspec <- rsa_model(dataset$dataset, design=rdes)
  ret4 <- run_searchlight(mspec, radius=4, method="standard")
  expect_true(!is.null(ret1) && !is.null(ret2) && !is.null(ret3) && !is.null(ret4))
  
})


test_that("randomized rsa_searchlight and blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  Dmat <- dist(matrix(rnorm(100*100), 100, 100))
  rdes <- rsa_design(~ Dmat, list(Dmat=Dmat, block=dataset$design$block_var), block_var="block")
  mspec <- rsa_model(dataset$dataset, design=rdes, regtype="lm")
  ret1 <- run_searchlight(mspec, radius=4, "randomized")
  expect_true(!is.null(ret1))
  
})

test_that("standard rsa_searchlight and no blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  Dmat <- dist(matrix(rnorm(100*100), 100, 100))
  rdes <- rsa_design(~ Dmat, list(Dmat=Dmat))
 
  for (regtype in c("lm", "rfit", "pearson", "spearman")) {
    mspec <- rsa_model(dataset$dataset, design=rdes, regtype=regtype)
    ret <- run_searchlight(mspec, radius=4, method="standard")
    expect_true(!is.null(ret))
  }

  
})

test_that("standard rsa_searchlight and blocking variable runs without error", {
  # Generate a sample MVPA dataset with a design including a blocking variable
  dataset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  # Create a distance matrix
  Dmat <- dist(matrix(rnorm(100 * 100), 100, 100))
  # Build RSA design including the block variable
  rdes <- rsa_design(~ Dmat, list(Dmat = Dmat, block = dataset$design$block_var), block_var = "block")
  
  # Test different regression types
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "lm")
  ret1 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "rfit")
  ret2 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "pearson")
  ret3 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  mspec <- rsa_model(dataset$dataset, design = rdes)
  ret4 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  expect_true(!is.null(ret1) && !is.null(ret2) && !is.null(ret3) && !is.null(ret4))
})

test_that("standard rsa_searchlight with multiple distance matrices and blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  # Create two distance matrices
  Dmat1 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100 * 100), 100, 100))
  
  # Build RSA design with two distance matrices and a blocking variable
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1 = Dmat1, Dmat2 = Dmat2, block = dataset$design$block_var), block_var = "block")
  
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "lm")
  ret1 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "rfit")
  ret2 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "pearson")
  ret3 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  mspec <- rsa_model(dataset$dataset, design = rdes)
  ret4 <- run_searchlight(mspec, radius = 4, method = "standard")
  
  expect_true(!is.null(ret1) && !is.null(ret2) && !is.null(ret3) && !is.null(ret4))
})

test_that("randomized rsa_searchlight and blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat, list(Dmat = Dmat, block = dataset$design$block_var), block_var = "block")
  
  mspec <- rsa_model(dataset$dataset, design = rdes, regtype = "lm")
  ret1 <- run_searchlight(mspec, radius = 4, method = "randomized")
  
  expect_true(!is.null(ret1))
})

test_that("standard rsa_searchlight and no blocking variable runs without error", {
  dataset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat <- dist(matrix(rnorm(100 * 100), 100, 100))
  # Build RSA design without a blocking variable
  rdes <- rsa_design(~ Dmat, list(Dmat = Dmat))
  
  for (regtype in c("lm", "rfit", "pearson", "spearman")) {
    mspec <- rsa_model(dataset$dataset, design = rdes, regtype = regtype)
    ret <- run_searchlight(mspec, radius = 4, method = "standard")
    expect_true(!is.null(ret))
  }
})
</file>

<file path="R/feature_selection.R">
## TODO integrate mlr "filters"


#' Feature Selection Methods
#'
#' @section Methods:
#' Two feature selection methods are available:
#' \describe{
#'   \item{FTest}{One-way ANOVA F-test for each feature}
#'   \item{catscore}{Correlation-adjusted t-scores using sda.ranking}
#' }
#'
#' @section Cutoff Types:
#' Two types of cutoffs are supported:
#' \describe{
#'   \item{top_k/topk}{Select top k features}
#'   \item{top_p/topp}{Select top p percent of features (0 < p <= 1)}
#' }
#'
#' @name feature_selection
NULL


#' @keywords internal
#' @importFrom stats pf
#' @noRd
matrixAnova <- function(Y, x) {
  if (!is.numeric(x)) stop("x must be numeric")
  if (nrow(x) != length(Y)) stop("x and Y must have compatible dimensions")
  if (any(is.na(x)) || any(is.na(Y))) stop("NA values not supported")
  x <- as.matrix(x)
  Y <- as.numeric(Y)
  k <- max(Y)
  ni <- tabulate(Y)
  n <- dim(x)[1]
  sx2 <- colSums(x^2)
  m <- rowsum(x, Y)
  a <- colSums(m^2/ni)
  b <- colSums(m)^2/n
  mst <- (a - b)/(k - 1)
  mse <- (sx2 - a)/(n - k)
  fa <- mst/mse
  pvalue <- pf(fa, k - 1, n - k, lower.tail = FALSE, log.p = FALSE)
  tab <- cbind(fa, pvalue)
  colnames(tab) <- c("Ftest", "pval")
  if (!is.null(colnames(x))) 
    rownames(tab) <- colnames(x)
  tab
  
}



#' Create a feature selection specification
#'
#' This function creates a feature selection specification using the provided
#' method, cutoff type, and cutoff value.
#'
#' @param method The type of feature selection method to use. Supported methods are "FTest" and "catscore".
#' @param cutoff_type The type of threshold used to select features. Supported cutoff types are "top_k" and "top_p".
#' @param cutoff_value The numeric value of the threshold cutoff.
#' @return A list with a class name equal to the \code{method} argument.
#' @details
#' The available feature selection methods are:
#'   - FTest: Computes a one-way ANOVA for every column in the feature matrix.
#'   - catscore: Computes a correlation adjusted t-test for every column in the matrix using \code{sda.ranking} from the \code{sda} package.
#' @examples
#' fsel <- feature_selector("FTest", "top_k", 1000)
#' fsel <- feature_selector("FTest", "top_p", .1)
#' class(fsel) == "FTest"
#' @export
feature_selector <- function(method, cutoff_type, cutoff_value) {
  ret <- list(
              cutoff_type=cutoff_type,
              cutoff_value=cutoff_value)
  class(ret) <- c(method, "feature_selector", "list")
  ret
}



#' Perform feature selection using the CATSCORE method
#'
#' This function selects features from the input data matrix X using the
#' CATSCORE method and the provided feature selection specification.
#'
#' @param obj The feature selection specification created by \code{feature_selector()}.
#' @param X The input data matrix.
#' @param Y The response variable.
#' @param ranking.score The feature score to use. Supported scores are "entropy", "avg", or "max". Default is "entropy".
#' @return A logical vector indicating which features to retain.
#' @details
#' The CATSCORE method computes a correlation adjusted t-test for every column in the matrix using \code{sda.ranking} from the \code{sda} package.
#' @seealso \code{\link{feature_selector}} for creating a feature selection specification.
#' @export
#' @examples
#' fsel <- feature_selector("catscore", "top_k", 1000)
#' X <- as.data.frame(matrix(rnorm(100 * 10), 100, 10))
#' Y <- rep(letters[1:5], 20)
#' selected_features <- select_features(fsel, X, Y, ranking.score = "entropy")
#' @importFrom sda sda.ranking
select_features.catscore <- function(obj, X, Y,  ranking.score=c("entropy", "avg", "max"),...) {
  assertthat::assert_that(obj$cutoff_type %in% c("topk", "top_k", "topp", "top_p"))
  ranking.score <- match.arg(ranking.score)
  message("selecting features via catscore")
  
  if (is.numeric(Y)) {
    medY <- median(Y)
    Y <- factor(ifelse(Y > medY, "high", "low"))
  }
  
  
  sda.1 <- sda.ranking(as.matrix(X), Y, ranking.score=ranking.score, fdr=FALSE, verbose=FALSE)
  
  keep.idx <- if (obj$cutoff_type == "top_k") {
    k <- min(ncol(X), obj$cutoff_value)
    sda.1[, "idx"][1:k]
  } else if (obj$cutoff_type == "top_p") {
    if (obj$cutoff_value <= 0 || obj$cutoff_value > 1) {
      stop("select_features.catscore: with top_p, cutoff_value must be > 0 and <= 1")
    }
    k <- max(obj$cutoff_value * ncol(X),1)
    sda.1[, "idx"][1:k]
   
  } else {
    stop(paste("select_features.catscore: unsupported cutoff_type: ", obj$cutoff_type))
  }
  
  
  keep <- logical(ncol(X))
  keep[keep.idx] <- TRUE
  message("retaining ", sum(keep), " features in matrix with ", ncol(X), " columns")
  keep
   
}



#' Perform feature selection using the F-test method
#'
#' This function selects features from the input data matrix X using the
#' F-test method and the provided feature selection specification.
#'
#' @param obj The feature selection specification created by \code{feature_selector()}.
#' @param X The input data matrix.
#' @param Y The response variable.
#' @param ... extra args (not used)
#' @return A logical vector indicating which features to retain.
#' @details
#' The F-test method computes a one-way ANOVA for every column in the feature matrix.
#' @seealso \code{\link{feature_selector}} for creating a feature selection specification.
#' @export
#' @examples
#' fsel <- feature_selector("FTest", "top_k", 1000)
#' X <- as.data.frame(matrix(rnorm(100 * 10), 100, 10))
#' Y <- rep(letters[1:5], 20)
#' selected_features <- select_features(fsel, X, Y)
#' @importFrom assertthat assert_that
select_features.FTest <- function(obj, X, Y,...) {
  message("selecting features via FTest")
  message("cutoff type ", obj$cutoff_type)
  message("cutoff value ", obj$cutoff_value)
  
  assertthat::assert_that(obj$cutoff_type %in% c("topk", "top_k", "topp", "top_p"))
  
  if (is.numeric(Y)) {
    medY <- median(Y)
    Y <- factor(ifelse(Y > medY, "high", "low"))
  }
  
  # Ensure X is numeric
  if (!is.numeric(X)) {
    X <- as.matrix(X)
    if (!is.numeric(X)) {
      stop("X must be convertible to a numeric matrix")
    }
  }
  
  pvals <- matrixAnova(Y, X)[,2]
  
  keep.idx <- if (obj$cutoff_type == "top_k" || obj$cutoff_type == "topk") {
    k <- min(ncol(X), obj$cutoff_value)
    order(pvals)[1:k]
  } else if (obj$cutoff_type == "top_p" || obj$cutoff_type == "topp") {
    if (obj$cutoff_value <= 0 || obj$cutoff_value > 1) {
      stop("select_features.FTest: with top_p, cutoff_value must be > 0 and <= 1")
    }
    k <- max(ceiling(obj$cutoff_value * ncol(X)), 1)
    order(pvals)[1:k]
  } else {
  
    stop(paste("select_features.FTest: unsupported cutoff_type: ", obj$cutoff_type))
  }
  
  
  keep <- logical(ncol(X))
  keep[keep.idx] <- TRUE
  
  message("retaining ", sum(keep), " features in matrix with ", ncol(X), " columns")
  
  keep
  
}



# Common validation function
validate_cutoff <- function(type, value, ncol) {
  type <- tolower(type)
  if (!type %in% c("top_k", "topk", "top_p", "topp")) {
    stop("Cutoff type must be one of: top_k, topk, top_p, topp")
  }
  
  if (grepl("p$", type)) {
    if (value <= 0 || value > 1) {
      stop("For percentage cutoff, value must be > 0 and <= 1")
    }
    max(ceiling(value * ncol), 1)
  } else {
    min(value, ncol)
  }
}
</file>

<file path="R/manova_model.R">
#' Create a MANOVA Design
#'
#' This function creates a MANOVA design object containing a formula expression and a named list of data.
#'
#' @param formula A formula expression specifying the MANOVA regression model.
#' @param data A named list containing the dissimilarity matrices and any other auxiliary variables.
#' @return A MANOVA design object with class attributes "manova_design" and "list".
#' @details
#' The function takes a formula expression and a named list of data as input, and returns a MANOVA design object.
#' The object is a list that contains the formula expression and the named list of data with class attributes "manova_design" and "list".
#' This object can be further used for MANOVA analysis or other related multivariate statistical methods.
#' @importFrom assertthat assert_that
#' @importFrom purrr is_formula
#' @importFrom ffmanova ffmanova
#' @examples
#' # Create a MANOVA design
#' formula <- y ~ x1 + x2
#' data_list <- list(y = dissimilarity_matrix_y, x1 = dissimilarity_matrix_x1, x2 = dissimilarity_matrix_x2)
#' manova_design_obj <- manova_design(formula, data_list)
#' @export
manova_design <- function(formula, data) {
  assert_that(purrr::is_formula(formula))
  
  des <- list(
    formula=formula,
    data=data
  )
  class(des) <- c("manova_design", "list")
  des
}


#' Create a MANOVA Model
#'
#' This function creates a MANOVA model object containing an `mvpa_dataset` instance and a `manova_design` instance.
#'
#' @param dataset An \code{mvpa_dataset} instance.
#' @param design A \code{manova_design} instance.
#' @return A MANOVA model object with class attributes "manova_model" and "list".
#' @details
#' The function takes an `mvpa_dataset` instance and a `manova_design` instance as input, and returns a MANOVA model object.
#' The object is a list that contains the dataset and the design with class attributes "manova_model" and "list".
#' This object can be used for further multivariate statistical analysis using the MANOVA method.
#' @importFrom assertthat assert_that
#' @importFrom purrr is_formula
#' @examples
#' # Create a MANOVA model
#' dataset <- create_mvpa_dataset(data_matrix, labels, subject_ids)
#' formula <- y ~ x1 + x2
#' data_list <- list(y = dissimilarity_matrix_y, x1 = dissimilarity_matrix_x1, x2 = dissimilarity_matrix_x2)
#' design <- manova_design(formula, data_list)
#' manova_model_obj <- manova_model(dataset, design)
#' @export
manova_model <- function(dataset,
                      design) {
  
  assert_that(inherits(dataset, "mvpa_dataset"))
  assert_that(inherits(design, "manova_design"))
  
  create_model_spec("manova_model", dataset, design)
  
  
}


#' Train a MANOVA Model
#'
#' This function trains a multivariate analysis of variance (MANOVA) model using the specified design.
#'
#' @param obj An object of class \code{manova_model}.
#' @param train_dat The training data.
#' @param y the response variable
#' @param indices The indices of the training data.
#' @param ... Additional arguments passed to the training method.
#' @return A named numeric vector of -log(p-values) for each predictor in the MANOVA model.
#' @importFrom stats as.formula
train_model.manova_model <- function(obj, train_dat, y, indices, ...) {
  dframe <- obj$design$data
  dframe$response <- as.matrix(train_dat)
  form <- stats::as.formula(paste("response", paste(as.character(obj$design$formula), collapse='')))
  
  fres=ffmanova(form, data=dframe)
  pvals=fres$pValues
  names(pvals) <- sanitize(names(pvals))   
  lpvals <- -log(pvals)
  lpvals
}


#' @export
#' @method print manova_model
print.manova_model <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  formula_style <- crayon::italic$blue
  var_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ MANOVA Model ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Model Specification"), "\n")
  cat(info_style("│  └─ Formula: "), formula_style(deparse(x$design$formula)), "\n")
  
  # Dataset information
  cat(section_style("├─ Dataset"), "\n")
  dims <- dim(x$dataset$train_data)
  dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                   " × ", number_style(dims[length(dims)]), " observations")
  cat(info_style("│  ├─ Dimensions: "), dim_str, "\n")
  cat(info_style("│  └─ Type: "), class(x$dataset$train_data)[1], "\n")
  
  # Variables section
  cat(section_style("├─ Variables"), "\n")
  predictors <- all.vars(x$design$formula[[3]])  # Get predictor names from RHS of formula
  response <- all.vars(x$design$formula[[2]])    # Get response name from LHS of formula
  cat(info_style("│  ├─ Response: "), var_style(response), "\n")
  cat(info_style("│  └─ Predictors: "), var_style(paste(predictors, collapse=", ")), "\n")
  
  # Data structure
  cat(section_style("└─ Data Structure"), "\n")
  
  # Check if there's a test set
  has_test <- !is.null(x$dataset$test_data)
  cat(info_style("   ├─ Test Set: "), 
      if(has_test) crayon::green("Present") else crayon::red("None"), "\n")
  
  # Check if there's a mask
  if (!is.null(x$dataset$mask)) {
    mask_sum <- sum(x$dataset$mask > 0)
    cat(info_style("   └─ Active Voxels/Vertices: "), 
        number_style(format(mask_sum, big.mark=",")), "\n")
  } else {
    cat(info_style("   └─ Mask: "), crayon::red("None"), "\n")
  }
  
  cat("\n")
}

#' @export
#' @method print manova_design
print.manova_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  formula_style <- crayon::italic$blue
  var_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ MANOVA Design ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Formula"), "\n")
  cat(info_style("│  └─ "), formula_style(deparse(x$formula)), "\n")
  
  # Data section
  cat(section_style("└─ Variables"), "\n")
  var_names <- names(x$data)
  cat(info_style("   ├─ Total Variables: "), crayon::green(length(var_names)), "\n")
  cat(info_style("   └─ Names: "), var_style(paste(var_names, collapse=", ")), "\n")
  
  cat("\n")
}
</file>

<file path="R/mvpa_result.R">
#' Create a \code{classification_result} instance
#'
#' Constructs a classification result object based on the observed and predicted values,
#' as well as other optional parameters.
#'
#' @param observed A vector of observed or true values.
#' @param predicted A vector of predicted values.
#' @param probs A \code{matrix} of predicted probabilities, with one column per level.
#' @param testind The row indices of the test observations (optional).
#' @param test_design An optional design for the test data.
#' @param predictor An optional predictor object.
#'
#' @return A classification result object, which can be one of: \code{regression_result},
#'   \code{binary_classification_result}, or \code{multiway_classification_result}.
#'
#' @examples
#' # A vector of observed values
#' yobs <- factor(rep(letters[1:4], 5))
#'
#' # Predicted probabilities
#' probs <- data.frame(a = runif(1:20), b = runif(1:20), c = runif(1:20), d = runif(1:20))
#' probs <- sweep(probs, 1, rowSums(probs), "/")
#'
#' # Get the max probability per row and use this to determine the predicted class
#' maxcol <- max.col(probs)
#' predicted <- levels(yobs)[maxcol]
#'
#' # Construct a classification result
#' cres <- classification_result(yobs, predicted, probs)
#'
#' # Compute default performance measures (Accuracy, AUC)
#' performance(cres)
#' @export
#' @family classification_result
classification_result <- function(observed, predicted, probs, testind=NULL, test_design=NULL,predictor=NULL) {
  
  
  if (is.numeric(observed)) {
    regression_result(observed, predicted, testind, test_design, predictor)
  } else if (length(levels(as.factor(observed))) == 2) {
    binary_classification_result(as.factor(observed), predicted, probs,  testind, test_design, predictor)
  } else if (length(levels(as.factor(observed))) > 2) {
    multiway_classification_result(as.factor(observed),predicted, probs, testind, test_design, predictor)
  } else {
    stop("observed data must be a factor with 2 or more levels")
  }
}

#' Classification results for binary outcome
#'
#' Constructs a binary classification result object based on the observed and predicted values,
#' as well as other optional parameters.
#'
#' @param observed A vector of observed or true values.
#' @param predicted A vector of predicted values.
#' @param probs A \code{matrix} of predicted probabilities, with one column per level.
#' @param testind The row indices of the test observations (optional).
#' @param test_design An optional design for the test data.
#' @param predictor An optional predictor object.
#'
#' @return A binary classification result object, with the class attribute set to "binary_classification_result".
#' @family classification_result
#' @export
binary_classification_result <- function(observed, predicted, probs, testind=NULL, test_design=NULL, predictor=NULL) {
  assertthat::assert_that(length(observed) == length(predicted))
  ret <- list(
    observed=observed,
    predicted=predicted,
    probs=as.matrix(probs),
    testind=testind,
    test_design=test_design,
    predictor=predictor
  )
  
  class(ret) <- c("binary_classification_result", "classification_result", "list")
  ret
}



#' Subset Multiway Classification Result
#'
#' This function subsets a multiway classification result based on the provided indices.
#'
#' @param x An object of class \code{multiway_classification_result} containing the multiway classification results.
#' @param indices The set of indices used to subset the results.
#'
#' @return A \code{multiway_classification_result} object containing the subset of results specified by the indices.
#'
#' @export
#' @family sub_result
sub_result.multiway_classification_result <- function(x, indices) {
  ret <- list(
    observed=x$observed[indices],
    predicted=x$predicted[indices],
    probs=as.matrix(x$probs)[indices,],
    testind=x$testind[indices],
    test_design=x$test_design[indices,],
    predictor=x$predictor)
  
  class(ret) <- c("multiway_classification_result", "classification_result", "list")
  ret
}

#' Subset Binary Classification Result
#'
#' This function subsets a binary classification result based on the provided indices.
#'
#' @param x An object of class \code{binary_classification_result} containing the binary classification results.
#' @param indices The set of indices used to subset the results.
#'
#' @return A \code{binary_classification_result} object containing the subset of results specified by the indices.
#'
#' @export
#' @family sub_result
sub_result.binary_classification_result <- function(x, indices) {
  ret <- list(
    observed=x$observed[indices],
    predicted=x$predicted[indices],
    probs=as.matrix(x$probs)[indices,],
    testind=x$testind[indices],
    test_design=x$test_design[indices,],
    predictor=x$predictor)
  
  class(ret) <- c("binary_classification_result", "classification_result", "list")
  ret
}


 
#' Create a Multiway Classification Result Object
#'
#' This function creates a multiway classification result object containing the observed and predicted values, class probabilities, test design, test indices, and predictor.
#'
#' @param observed A vector of observed values.
#' @param predicted A vector of predicted values.
#' @param probs A matrix of class probabilities.
#' @param testind A vector of indices for the test data (optional).
#' @param test_design The test design (optional).
#' @param predictor The predictor used in the multiway classification model (optional).
#' @return A list with class attributes "multiway_classification_result", "classification_result", and "list" containing the observed and predicted values, class probabilities, test design, test indices, and predictor.
#' @family classification_result
multiway_classification_result <- function(observed, predicted, probs,testind=NULL, test_design=NULL, predictor=NULL) {
  assertthat::assert_that(length(observed) == length(predicted))
  ret <- list(
    observed=observed,
    predicted=predicted,
    probs=as.matrix(probs),
    testind=testind,
    test_design=test_design,
    predictor=predictor)
  
  class(ret) <- c("multiway_classification_result", "classification_result", "list")
  ret
}

 
#' Create a Regression Result Object
#'
#' This function creates a regression result object containing the observed and predicted values, test design, test indices, and predictor.
#'
#' @param observed A vector of observed values.
#' @param predicted A vector of predicted values.
#' @param testind A vector of indices for the test data (optional).
#' @param test_design The test design (optional).
#' @param predictor The predictor used in the regression model (optional).
#' @return A list with class attributes "regression_result", "classification_result", and "list" containing the observed and predicted values, test design, test indices, and predictor.
#' @family classification_result
regression_result <- function(observed, predicted, testind=NULL, test_design=NULL, predictor=NULL) {
  ret <- list(
    observed=observed,
    predicted=predicted,
    test_design=test_design,
    testind=testind,
    predictor=predictor)
  class(ret) <- c("regression_result", "classification_result", "list")
  ret
}

#' @export
#' @method print classification_result
print.classification_result <- function(x, ...) {
  UseMethod("print")
}

#' @export
#' @method print regression_result
print.regression_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Regression Result ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(length(x$observed)), "\n")
  cat(info_style("│  ├─ Test Indices: "), 
      if(!is.null(x$testind)) number_style(length(x$testind)) else crayon::red("None"), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  mse <- mean((x$observed - x$predicted)^2)
  rmse <- sqrt(mse)
  r2 <- cor(x$observed, x$predicted)^2
  mae <- mean(abs(x$observed - x$predicted))
  
  cat(info_style("   ├─ MSE: "), number_style(sprintf("%.4f", mse)), "\n")
  cat(info_style("   ├─ RMSE: "), number_style(sprintf("%.4f", rmse)), "\n")
  cat(info_style("   ├─ MAE: "), number_style(sprintf("%.4f", mae)), "\n")
  cat(info_style("   └─ R²: "), number_style(sprintf("%.4f", r2)), "\n\n")
}

#' @export
#' @method print binary_classification_result
print.binary_classification_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  level_style <- crayon::blue
  
  # Print header
  cat("\n", header_style("█▀▀ Binary Classification Result ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(length(x$observed)), "\n")
  cat(info_style("│  ├─ Classes: "), level_style(paste(levels(x$observed), collapse=", ")), "\n")
  cat(info_style("│  └─ Test Indices: "), 
      if(!is.null(x$testind)) number_style(length(x$testind)) else crayon::red("None"), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  conf_mat <- table(Observed=x$observed, Predicted=x$predicted)
  accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
  sensitivity <- conf_mat[2,2] / sum(conf_mat[2,])
  specificity <- conf_mat[1,1] / sum(conf_mat[1,])
  
  cat(info_style("   ├─ Accuracy: "), number_style(sprintf("%.4f", accuracy)), "\n")
  cat(info_style("   ├─ Sensitivity: "), number_style(sprintf("%.4f", sensitivity)), "\n")
  cat(info_style("   └─ Specificity: "), number_style(sprintf("%.4f", specificity)), "\n\n")
}

#' @export
#' @method print multiway_classification_result
print.multiway_classification_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  level_style <- crayon::blue
  
  # Print header
  cat("\n", header_style("█▀▀ Multiway Classification Result ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(length(x$observed)), "\n")
  cat(info_style("│  ├─ Number of Classes: "), number_style(length(levels(x$observed))), "\n")
  cat(info_style("│  ├─ Classes: "), level_style(paste(levels(x$observed), collapse=", ")), "\n")
  cat(info_style("│  └─ Test Indices: "), 
      if(!is.null(x$testind)) number_style(length(x$testind)) else crayon::red("None"), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  conf_mat <- table(Observed=x$observed, Predicted=x$predicted)
  accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
  
  # Calculate per-class metrics
  class_metrics <- lapply(levels(x$observed), function(cls) {
    tp <- sum(x$observed == cls & x$predicted == cls)
    total <- sum(x$observed == cls)
    recall <- tp / total
    precision <- tp / sum(x$predicted == cls)
    f1 <- 2 * (precision * recall) / (precision + recall)
    c(recall=recall, precision=precision, f1=f1)
  })
  names(class_metrics) <- levels(x$observed)
  
  cat(info_style("   ├─ Overall Accuracy: "), number_style(sprintf("%.4f", accuracy)), "\n")
  cat(info_style("   └─ Per-Class Metrics:"), "\n")
  
  for(cls in levels(x$observed)) {
    metrics <- class_metrics[[cls]]
    cat(info_style("      ├─ "), level_style(cls), ":\n")
    cat(info_style("      │  ├─ Recall: "), number_style(sprintf("%.4f", metrics["recall"])), "\n")
    cat(info_style("      │  ├─ Precision: "), number_style(sprintf("%.4f", metrics["precision"])), "\n")
    cat(info_style("      │  └─ F1: "), number_style(sprintf("%.4f", metrics["f1"])), "\n")
  }
  cat("\n")
}
</file>

<file path="tests/testthat/test_rsa_regional.R">
test_that("mvpa_regional with 5 ROIS runs without error", {
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  Dmat <- dist(matrix(rnorm(100*100), 100, 100))
  rdes <- rsa_design(~ Dmat, list(Dmat=Dmat, block=dset$design$block_var), block_var="block")
  mspec <- rsa_model(dset$dataset, design=rdes)
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
   
  res <- run_regional(mspec, region_mask)
  expect_true(!is.null(res))
  
})

test_that("mvpa_regional with 5 ROIS and multiple distance matrices runs without error", {
  
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  Dmat1 <- dist(matrix(rnorm(100*100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100*100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1=Dmat1, Dmat2=Dmat2, block=dset$design$block_var), block_var="block")
  
  mspec <- rsa_model(dset$dataset, design=rdes)
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  res <- run_regional(mspec, region_mask)
  expect_true(!is.null(res))

  
})

library(testthat)

context("Comprehensive tests for RSA regional analysis and design/model functionality")



## --- rsa_design and rsa_model_mat ---
test_that("rsa_design creates a valid RSA design object", {
  # Create a dummy distance matrix and a dummy block variable
  Dmat <- dist(matrix(rnorm(100 * 100), 100, 100))
  data_list <- list(Dmat = Dmat, block = rep(1:5, each = 20))
  rdes <- rsa_design(~ Dmat, data_list, block_var = "block")
  
  expect_true(is.list(rdes))
  expect_true("rsa_design" %in% class(rdes))
  expect_true(!is.null(rdes$formula))
  expect_true(!is.null(rdes$data))
  expect_true(!is.null(rdes$model_mat))
})

test_that("rsa_model_mat returns vectors of correct length and sanitized names", {
  # For a 10x10 distance matrix, lower triangle has 45 elements.
  Dmat <- dist(matrix(rnorm(10 * 10), 10, 10))
  data_list <- list(Dmat = Dmat)
  rdes <- rsa_design(~ Dmat, data_list)
  mm <- rsa_model_mat(rdes)
  
  expect_equal(length(mm[[1]]), 45)
  # Names should be sanitized (e.g., no spaces or colons)
  expect_true(all(grepl("Dmat", names(mm))))
})

## --- Training and Print Methods ---
test_that("train_model.rsa_model with 'lm' regtype returns coefficients with proper names", {
  set.seed(123)
  # Create dummy training data (e.g., 100 observations with 20 features)
  train_data <- matrix(rnorm(100 * 20), 100, 20)
  # Create a block variable and a distance matrix for the design
  block_var <- rep(1:5, each = 20)
  Dmat <- dist(matrix(rnorm(100 * 100), 100, 100))
  data_list <- list(Dmat = Dmat, block = block_var)
  rdes <- rsa_design(~ Dmat, data_list, block_var = "block")
  
  # Create a dummy mvpa_dataset (simulate train_data and a mask)
  dset <- list(train_data = train_data, mask = 1:100)
  class(dset) <- "mvpa_dataset"
  
  mspec <- rsa_model(dset, rdes, regtype = "lm")
  coeffs <- train_model.rsa_model(mspec, train_data, y = NULL, indices = NULL)
  
  expect_true(is.numeric(coeffs))
  expect_true(!is.null(names(coeffs)))
  # Expect one coefficient per predictor in the model matrix.
  expect_equal(length(coeffs), length(rdes$model_mat))
})

test_that("Print methods for rsa_model and rsa_design produce non-empty output", {
  Dmat <- dist(matrix(rnorm(50 * 50), 50, 50))
  data_list <- list(Dmat = Dmat, block = rep(1:5, each = 10))
  rdes <- rsa_design(~ Dmat, data_list, block_var = "block")
  
  dset <- list(train_data = matrix(rnorm(50 * 20), 50, 20), mask = 1:50)
  class(dset) <- "mvpa_dataset"
  mspec <- rsa_model(dset, rdes, regtype = "pearson")
  
  out_design <- capture.output(print(rdes))
  out_model <- capture.output(print(mspec))
  expect_true(length(out_design) > 0)
  expect_true(length(out_model) > 0)
})

## --- Regional Analysis ---
# We assume that run_regional() returns a list with at least the following components:
# "performance_table", "vol_results", and optionally "prediction_table" and "fits".

test_that("mvpa_regional with 5 ROIs runs without error and returns structured result", {
  # Use your provided helper to generate a sample dataset
  dset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat, list(Dmat = Dmat, block = dset$design$block_var), block_var = "block")
  mspec <- rsa_model(dset$dataset, design = rdes)
  
  region_mask <- NeuroVol(sample(1:5, size = length(dset$dataset$mask), replace = TRUE), space(dset$dataset$mask))
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  expect_true(is.list(res))
  expect_true("performance_table" %in% names(res))
  expect_true("vol_results" %in% names(res))
  # Optionally, check prediction_table if return_predictions is enabled
  if (!is.null(res$prediction_table)) {
    expect_true(is.data.frame(res$prediction_table))
  }
})

test_that("mvpa_regional with multiple distance matrices runs without error and returns valid performance metrics", {
  dset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat1 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1 = Dmat1, Dmat2 = Dmat2, block = dset$design$block_var), block_var = "block")
  mspec <- rsa_model(dset$dataset, design = rdes)
  
  region_mask <- NeuroVol(sample(1:5, size = length(dset$dataset$mask), replace = TRUE), space(dset$dataset$mask))
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  # Check that performance_table contains expected columns (e.g., mse and correlation)
  if (!is.null(res$performance_table)) {
    colnames_perf <- names(res$performance_table)
    expect_true(any(grepl("Dmat1", colnames_perf, ignore.case = TRUE)))
    expect_true(any(grepl("Dmat2", colnames_perf, ignore.case = TRUE)))
  }
})

test_that("mvpa_regional with multiple distance matrices runs without error and returns valid performance metrics", {
  dset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat1 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat3 <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2 + Dmat3, list(Dmat1 = Dmat1, Dmat2 = Dmat2, Dmat3=Dmat3, block = dset$design$block_var), block_var = "block")
  mspec <- rsa_model(dset$dataset, design = rdes, distmethod="spearman", regtype="lm")
  
  region_mask <- NeuroVol(sample(1:5, size = length(dset$dataset$mask), replace = TRUE), space(dset$dataset$mask))
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  # Check that performance_table contains expected columns (e.g., mse and correlation)
  if (!is.null(res$performance_table)) {
    colnames_perf <- names(res$performance_table)
    expect_true(any(grepl("Dmat1", colnames_perf, ignore.case = TRUE)))
    expect_true(any(grepl("Dmat2", colnames_perf, ignore.case = TRUE)))
  }
})

test_that("mvpa_regional with semipartial correlations runs without error", {
  dset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat1 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1 = Dmat1, Dmat2 = Dmat2, block = dset$design$block_var), block_var = "block")
  
  # Create model with semipartial option set to TRUE
  mspec <- rsa_model(dset$dataset, design = rdes, regtype = "lm", semipartial = TRUE)
  
  region_mask <- NeuroVol(sample(1:5, size = length(dset$dataset$mask), replace = TRUE), space(dset$dataset$mask))
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  # Check that performance_table contains expected columns
  if (!is.null(res$performance_table)) {
    colnames_perf <- names(res$performance_table)
    expect_true(any(grepl("Dmat1", colnames_perf, ignore.case = TRUE)))
    expect_true(any(grepl("Dmat2", colnames_perf, ignore.case = TRUE)))
  }
  
  # Test that coefficients are returned as semi-partial correlations
  # This is a more indirect test, as we can't directly access the coefficients
  # but we can check that the model runs without error and returns results
  expect_true("performance_table" %in% names(res))
  expect_true("vol_results" %in% names(res))
})

test_that("mvpa_regional with non-negative constraints runs without error", {
  dset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat1 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1 = Dmat1, Dmat2 = Dmat2, block = dset$design$block_var), block_var = "block")
  
  # Create model with non-negative constraints on Dmat2
  mspec <- rsa_model(dset$dataset, design = rdes, regtype = "lm", nneg = list(Dmat2 = TRUE))
  
  region_mask <- NeuroVol(sample(1:5, size = length(dset$dataset$mask), replace = TRUE), space(dset$dataset$mask))
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  # Check that performance_table contains expected columns
  if (!is.null(res$performance_table)) {
    colnames_perf <- names(res$performance_table)
    expect_true(any(grepl("Dmat1", colnames_perf, ignore.case = TRUE)))
    expect_true(any(grepl("Dmat2", colnames_perf, ignore.case = TRUE)))
  }
})

test_that("mvpa_regional with both semipartial and non-negative constraints handles precedence correctly", {
  dset <- gen_sample_dataset(c(5, 5, 5), 100, blocks = 3)
  
  Dmat1 <- dist(matrix(rnorm(100 * 100), 100, 100))
  Dmat2 <- dist(matrix(rnorm(100 * 100), 100, 100))
  rdes <- rsa_design(~ Dmat1 + Dmat2, list(Dmat1 = Dmat1, Dmat2 = Dmat2, block = dset$design$block_var), block_var = "block")
  
  # Create model with both options - non-negative should take precedence
  mspec <- rsa_model(dset$dataset, design = rdes, regtype = "lm", 
                    nneg = list(Dmat2 = TRUE), semipartial = TRUE)
  
  region_mask <- NeuroVol(sample(1:5, size = length(dset$dataset$mask), replace = TRUE), space(dset$dataset$mask))
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  # Check that results are returned successfully
  expect_true("performance_table" %in% names(res))
  expect_true("vol_results" %in% names(res))
  
  # The model should use non-negative constraints and ignore semipartial
  # This is handled in train_model.rsa_model function, which prioritizes nneg over semipartial
})
</file>

<file path="tests/testthat/test_vector_rsa_regional.R">
library(testthat)

context("Vector RSA regional analysis tests")

test_that("vector_rsa regional analysis with 5 ROIs runs without error", {
  # Generate a sample dataset
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  
  block <- dset$design$block_var
  
  # Create vector_rsa_design and model
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  mspec <- vector_rsa_model(dset$dataset, rdes, distfun=cordist())
  
  # Create a region mask with 5 regions
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Verify that the result is not NULL and has expected components
  expect_true(!is.null(res))
  expect_true(is.list(res))
  expect_true("performance_table" %in% names(res))
  expect_true("vol_results" %in% names(res))
})

test_that("vector_rsa regional analysis works with mahalanobis distance", {
  # Generate a sample dataset
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  
  block <- dset$design$block_var
  
  # Create vector_rsa_design and model with mahalanobis distance
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  mspec <- vector_rsa_model(dset$dataset, rdes, distfun=mahadist())
  
  # Create a region mask with 5 regions
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check that result is not NULL and performance table contains the RSA score
  expect_true(!is.null(res))
  if (!is.null(res$performance_table)) {
    # Check if rsa_score exists as a column in performance_table
    expect_true("rsa_score" %in% colnames(res$performance_table))
    # Check that rsa_score values are in a reasonable range (-1 to 1, as it's often a correlation)
    expect_true(all(res$performance_table$rsa_score >= -1 & res$performance_table$rsa_score <= 1, na.rm=TRUE))
  }
})

test_that("vector_rsa regional analysis works with PCA-based distance", {
  # Generate a sample dataset
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  
  block <- dset$design$block_var
  
  # Create vector_rsa_design 
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  
  # Create PCA distance function
  threshfun <- function(x) { sum(x > 1) }
  distfun_pca <- pcadist(labels=NULL, ncomp=3, whiten=FALSE, threshfun=threshfun, dist_method="cosine")
  
  # Create model with PCA distance
  mspec <- vector_rsa_model(dset$dataset, rdes, distfun=distfun_pca)
  
  # Create a region mask with 5 regions
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  if (!is.null(res$vol_results)) {
    # Check that vol_results contains expected number of volumes
    expect_equal(length(res$vol_results), 1)
  }
})


test_that("vector_rsa regional analysis returns correct number of volumes for ROIs", {
  # Generate a sample dataset
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  
  block <- dset$design$block_var
  
  # Create vector_rsa_design and model
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  mspec <- vector_rsa_model(dset$dataset, rdes, distfun=cordist())
  
  # Create a region mask with exactly 4 regions (1,2,3,4)
  mask_data <- sample(c(1,2,3,4), size=length(dset$dataset$mask), replace=TRUE)
  region_mask <- NeuroVol(mask_data, space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check that there's one volume per region (4 regions)
  if (!is.null(res$vol_results)) {
    n_regions <- length(unique(mask_data))
    expect_equal(nrow(res$performance_table), n_regions)
  }
})

test_that("vector_rsa regional analysis maintains valid correlation values", {
  # Generate a sample dataset
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  
  block <- dset$design$block_var
  
  # Create vector_rsa_design and model
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  
  # Try both Pearson and Spearman
  mspec_pearson <- vector_rsa_model(dset$dataset, rdes, distfun=cordist(), rsa_simfun="pearson")
  mspec_spearman <- vector_rsa_model(dset$dataset, rdes, distfun=cordist(), rsa_simfun="spearman")
  
  # Create a region mask
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis for both models
  res_pearson <- run_regional(mspec_pearson, region_mask)
  res_spearman <- run_regional(mspec_spearman, region_mask)
  
  # Check that rsa_score values are in valid range (-1 to 1)
  if (!is.null(res_pearson$performance_table) && "rsa_score" %in% colnames(res_pearson$performance_table)) {
    expect_true(all(res_pearson$performance_table$rsa_score >= -1 & 
                     res_pearson$performance_table$rsa_score <= 1, na.rm=TRUE))
  }
  
  if (!is.null(res_spearman$performance_table) && "rsa_score" %in% colnames(res_spearman$performance_table)) {
    expect_true(all(res_spearman$performance_table$rsa_score >= -1 & 
                     res_spearman$performance_table$rsa_score <= 1, na.rm=TRUE))
  }
})
</file>

<file path="README.rmd">
---
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

<!-- badges: start -->
  [![R-CMD-check](https://github.com/bbuchsbaum/rMVPA/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/bbuchsbaum/rMVPA/actions/workflows/R-CMD-check.yaml)
 
<!-- badges: end -->
  
  

## Multivoxel Pattern Analysis in R


```{r, echo = FALSE}                                                                                                        
version <- as.vector(read.dcf('DESCRIPTION')[, 'Version'])                                                                 
version <- gsub('-', '.', version)                                                                                      
```                                                                                                                        

### This package is under development.

### Introduction

`rMVPA` is an R library for multivariate pattern analysis of neuroimaging data. The goal of this library is to make MVPA analyses easy. It can be used both programmatically from within R or using a command line interface. 'rMVPA' leverages the 'caret' library for the underlying machine learning interface. What `rMVPA` provides is the infrastructure for conducting machine learning analyses on neuroimaging data. 

Documentation and vignettes: https://bbuchsbaum.github.io/rMVPA/

### Installation

### Using devtools

To install `rMVPA` from within R, use the `devtools` function `install_github`. You will need the development version of `neuroim2` as well.

From within R:

```
#library(devtools)
install_github("bbuchsbaum/neuroim2")
install_github("bbuchsbaum/rMVPA")
```

### Using `git` from the command line

```
git clone git@github.com:bbuchsbaum/rMVPA.git
R CMD install rMVPA
```

### Optionally install command line scripts for "coding-free" MVPA analysis:

`wget https://raw.githubusercontent.com/bbuchsbaum/rMVPA/master/scripts/MVPA_Searchlight.R`

`wget https://raw.githubusercontent.com/bbuchsbaum/rMVPA/master/scripts/MVPA_Regional.R`

Then, move these files to a folder on your `PATH` and make them executable:

`chmod +x MVPA_Searchlight.R`

`chmod +x MVPA_Regional.R`
</file>

<file path="R/performance.R">
#' Calculate the Predicted Class from Probability Matrix
#'
#' This function calculates the predicted class from a matrix of predicted probabilities. The class with the highest probability is selected as the predicted class.
#'
#' @param prob A matrix of predicted probabilities with column names indicating the classes.
#' @return A vector of predicted classes corresponding to the highest probability for each row in the input matrix.
#' @export
predicted_class <- function(prob) {
  maxid <- max.col(prob, ties.method="random")
  pclass <- colnames(prob)[maxid]
}

#' Calculate Performance Metrics for Regression Result
#'
#' This function calculates performance metrics for a regression result object, including R-squared, Root Mean Squared Error (RMSE), and Spearman correlation.
#'
#' @param x A \code{regression_result} object.
#' @param split_list Split results by indexed sub-groups (not supported for regression analyses yet).
#' @param ... extra args (not used).
#' @return A named vector with the calculated performance metrics: R-squared, RMSE, and Spearman correlation.
#' @details
#' The function calculates the following performance metrics for the given regression result object:
#' - R-squared: proportion of variance in the observed data that is predictable from the fitted model.
#' - RMSE: root mean squared error, a measure of the differences between predicted and observed values.
#' - Spearman correlation: a measure of the monotonic relationship between predicted and observed values.
#' @seealso \code{\link{regression_result}}
#' @export
performance.regression_result <- function(x, split_list,...) {
  if (!is.null(split_list)) {
    ## TODO: add support
    stop("split_by not supported for regression analyses yet.")
  }
  
  #browser()
  R2 <- 1 - sum((x$observed - x$predicted)^2)/sum((x$observed-mean(x$observed))^2)
  rmse <- sqrt(mean((x$observed-x$predicted)^2))
  rcor <- cor(x$observed, x$predicted, method="spearman")
  c(R2=R2, RMSE=rmse, spearcor=rcor)
}


#' Apply Custom Performance Metric to Prediction Result
#'
#' This function applies a user-supplied performance metric to a prediction result object.
#'
#' @param x The prediction result object.
#' @param custom_fun The function used to compute performance metrics, i.e., \code{custom_fun(x)}.
#' @param split_list An optional named list of splitting groups. If provided, the performance metric will be computed for each group and returned as a named vector.
#' @return A named vector with the calculated custom performance metric(s).
#' @details
#' The function allows users to apply a custom performance metric to a prediction result object.
#' If a split list is provided, the performance metric will be computed for each group separately, and the results will be returned as a named vector.
#' @export
custom_performance <- function(x, custom_fun, split_list=NULL) {
  if (is.null(split_list)) {
    custom_fun(x)
  } else {
    total <- custom_fun(x)
    subtots <- unlist(lapply(names(split_list), function(tag) {
      ind <- split_list[[tag]]
      ret <- custom_fun(sub_result(x, ind))
      names(ret) <- paste0(names(ret), "_", tag)
      ret
    }))
    
    c(total, subtots)
  }
  
}

#' @export
merge_results.binary_classification_result <- function(x,...) {
  rlist <- list(x,...)
  probs <- Reduce("+", lapply(rlist, function(x) x$probs))/length(rlist)
  
  mc <- max.col(probs)
  predicted <- levels(x$observed)[mc]
  binary_classification_result(observed=x$observed, predicted=predicted, probs=probs, testind=x$testind, 
                               test_design=x$test_design, predictor=x$predictor)
}

#' @export
merge_results.regression_result <- function(x,...) {
  rlist <- list(x,...)
  pred <- Reduce("+", lapply(rlist, function(x) x$predicted))/length(rlist)
  regression_result(observed=x$observed, predicted=pred, testind=x$testind, 
                               test_design=x$test_design, predictor=x$predictor)
}



#' @export
prob_observed.binary_classification_result <- function(x) {
  x$probs[cbind(seq(1,nrow(x$probs)),as.integer(x$observed))]
}

#' @export
prob_observed.multiway_classification_result <- function(x) {
  x$probs[cbind(seq(1,nrow(x$probs)),as.integer(x$observed))]
}

#' @export
merge_results.multiway_classification_result <- function(x,...) {
  
  rlist <- list(x,...)
  #ds <- sapply(rlist, function(x) nrow(x$probs))
  
  probs <- Reduce("+", lapply(rlist, function(x) x$probs))/length(rlist)
  mc <- max.col(probs)
  predicted <- levels(x$observed)[mc]
  
  multiway_classification_result(observed=x$observed, predicted=predicted, probs=probs, 
                                 testind=x$testind,  test_design=x$test_design, predictor=x$predictor)
}

#' @export
performance.binary_classification_result <- function(x, split_list=NULL,...) {
  stopifnot(length(x$observed) == length(x$predicted))
  
  if (is.null(split_list)) {
    ret <- binary_perf(x$observed, x$predicted, x$probs)
  } else {
    total <- binary_perf(x$observed, x$predicted, x$probs)
    
    subtots <- unlist(lapply(names(split_list), function(tag) {
      ind <- split_list[[tag]]
      if (!is.null(x$testind)) {
        ind <- which(x$testind %in% ind)
      }
      ret <- binary_perf(x$observed[ind], x$predicted[ind], x$probs[ind,])
      names(ret) <- paste0(names(ret), "_", tag)
      ret
    }))
    
    ret <- c(total, subtots)
  }
}


#' @export
performance.multiway_classification_result <- function(x, split_list=NULL, class_metrics=FALSE,...) {
  stopifnot(length(x$observed) == length(x$predicted))

  if (is.null(split_list)) {
    multiclass_perf(x$observed, x$predicted, x$probs, class_metrics)
  } else {
    total <- multiclass_perf(x$observed, x$predicted, x$probs, class_metrics)
    subtots <- unlist(lapply(names(split_list), function(tag) {
      ind <- split_list[[tag]]
      
      if (!is.null(x$testind)) {
        ind <- which(x$testind %in% ind)
      }
      
      ret <- multiclass_perf(x$observed[ind], x$predicted[ind], x$probs[ind,], class_metrics)
      names(ret) <- paste0(names(ret), "_", tag)
      ret
    }))
    
    c(total, subtots)
    
  }
  
}

#' @keywords internal
#' @noRd
combinedAUC <- function(Pred, Obs) {
  Obs <- as.factor(Obs)
  mean(sapply(1:ncol(Pred), function(i) {
    lev <- levels(Obs)[i]
    pos <- Obs == lev
    pclass <- Pred[,i]
    pother <- rowMeans(Pred[,-i,drop=FALSE])
    Metrics::auc(as.numeric(pos), pclass - pother)-.5
  }))
}


#' @keywords internal
#' @noRd
combinedACC <- function(Pred, Obs) {
  levs <- levels(as.factor(Obs))
  maxind <- apply(Pred, 1, which.max)
  pclass <- levs[maxind]
  sum(pclass == Obs)/length(pclass)
  
}


#' @keywords internal
binary_perf <- function(observed, predicted, probs) {
  obs <- as.character(observed)
  ncorrect <- sum(obs == predicted)
  ntotal <- length(obs)
  #maxClass <- max(table(obs))
  
  #out <- binom.test(ncorrect,
  #                  ntotal,
  #                  p = maxClass/ntotal,
  #                  alternative = "greater")
  
  
  #c(ZAccuracy=-qnorm(out$p.value), Accuracy=ncorrect/ntotal, AUC=Metrics::auc(observed == levels(observed)[2], probs[,2])-.5)
  c(Accuracy=ncorrect/ntotal, AUC=Metrics::auc(observed == levels(observed)[2], probs[,2])-.5)
  
}

#' @keywords internal
multiclass_perf <- function(observed, predicted, probs, class_metrics=FALSE) {
  
  obs <- as.character(observed)
  ntotal <- length(obs)
 
  aucres <- sapply(1:ncol(probs), function(i) {
    lev <- try(levels(observed)[i])
    pos <- obs == lev
    pclass <- probs[,i]
    pother <- rowMeans(probs[,-i, drop=FALSE])
    Metrics::auc(as.numeric(pos), pclass - pother)-.5
  })
  
  names(aucres) <- paste0("AUC_", colnames(probs))
  
  
  if (class_metrics) {
    c(Accuracy=sum(obs == as.character(predicted))/length(obs), AUC=mean(aucres, na.rm=TRUE), aucres)
  } else {
    c(Accuracy=sum(obs == as.character(predicted))/length(obs), AUC=mean(aucres, na.rm=TRUE))
  }
}
</file>

<file path="R/roisplit.R">
#' @keywords internal
#' @noRd
roi_volume_matrix <- function(mat, refspace, indices, coords) {
  structure(mat,
            refspace=refspace,
            indices=indices,
            coords=coords,
            class=c("roi_volume_matrix", "matrix"))
  
}

#' @keywords internal
#' @noRd
roi_surface_matrix <- function(mat, refspace, indices, coords) {
  structure(mat,
            refspace=refspace,
            indices=indices,
            coords=coords,
            class=c("roi_surface_matrix", "matrix"))

}
</file>

<file path="tests/testthat/test_feature_rsa.R">
library(rMVPA)
library(neuroim2)

test_that("regional feature_rsa_model with direct F matrix runs without error", {
  # Generate a sample dataset: small volume, say 5x5x5, with 100 observations
  dset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a feature matrix F, e.g., 100 observations x 20 features
  Fmat <- matrix(rnorm(100*20), 100, 20)
  labels <- paste0("obs", 1:100)
  
  # Create feature_rsa_design using direct F matrix
  fdes <- feature_rsa_design(F=Fmat, labels=labels, max_comps=3)
  
  # Create a feature_rsa_model, for example using 'pls'
  mspec <- feature_rsa_model(dset$dataset, fdes, method="pls", crossval=blocked_cross_validation(dset$design$block_var))
  
  # Create a region mask with 5 ROIs
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), 
  space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  expect_true(!is.null(res$performance_table))
})

test_that("regional feature_rsa_model with S-based feature extraction runs without error", {
  # Generate a sample dataset: again 5x5x5 volume, 100 obs
  dset <- gen_sample_dataset(c(6,5,5), 100, blocks=3)
  
  # Create a similarity matrix S: must be symmetric and match number of observations
  obs_features <- matrix(rnorm(100*10), 100, 10)
  S <- tcrossprod(base::scale(obs_features))  # similarity matrix
  labels <- paste0("obs", 1:100)
  
  # Create feature_rsa_design using S
  fdes <- feature_rsa_design(S=S, labels=labels, k=10) # reduce to 5 dims
  
  # Create a feature_rsa_model using scca this time
  mspec <- feature_rsa_model(dset$dataset, fdes, method="scca", crossval=blocked_cross_validation(dset$design$block_var))
  
  # Create a region mask with 5 ROIs
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  expect_true(!is.null(res$performance_table))
  
  # Check that performance_table has expected columns
  expect_true("mean_correlation" %in% colnames(res$performance_table))
  expect_true("cor_difference" %in% colnames(res$performance_table))
  expect_true("voxel_correlation" %in% colnames(res$performance_table))
  expect_true("mse" %in% colnames(res$performance_table))
  expect_true("r_squared" %in% colnames(res$performance_table))
})

test_that("feature_rsa_model with permutation testing works correctly", {
  set.seed(123)
  
  # Create a small dataset for faster testing
  dset <- gen_sample_dataset(c(3,3,3), 50, blocks=2)
  
  # Create a feature matrix
  Fmat <- matrix(rnorm(50*10), 50, 10)
  labels <- paste0("obs", 1:50)
  
  # Create feature_rsa_design
  fdes <- feature_rsa_design(F=Fmat, labels=labels)
  
  # Create a feature_rsa_model with permutation testing
  mspec <- feature_rsa_model(
    dset$dataset, 
    fdes, 
    method="pca", 
    crossval=blocked_cross_validation(dset$design$block_var),
    nperm=10,  # Small number for testing
    permute_by="observations",
    save_distributions=TRUE
  )
  
  # Create a region mask with just 2 ROIs for faster testing
  region_mask <- NeuroVol(sample(1:2, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  
  # Check that permutation results are included in performance table
  perf_cols <- colnames(res$performance_table)
  expect_true(any(grepl("^p_", perf_cols)))  # p-values
  expect_true(any(grepl("^z_", perf_cols)))  # z-scores
  
  # Check specific permutation columns
  expect_true("p_mean_correlation" %in% perf_cols)
  expect_true("z_mean_correlation" %in% perf_cols)
  expect_true("p_cor_difference" %in% perf_cols)
  expect_true("z_cor_difference" %in% perf_cols)
})

test_that("feature_rsa_model with permute_by='features' works correctly", {
  set.seed(123)
  
  # Create a small dataset for faster testing
  dset <- gen_sample_dataset(c(3,3,3), 40, blocks=2)
  
  # Create a feature matrix
  Fmat <- matrix(rnorm(40*8), 40, 8)
  labels <- paste0("obs", 1:40)
  
  # Create feature_rsa_design
  fdes <- feature_rsa_design(F=Fmat, labels=labels)
  
  # Create a feature_rsa_model with permutation testing by features
  mspec <- feature_rsa_model(
    dset$dataset, 
    fdes, 
    method="pca", 
    crossval=blocked_cross_validation(dset$design$block_var),
    nperm=5,  # Small number for testing
    permute_by="features"  # Permute features instead of observations
  )
  
  # Create a region mask with just 2 ROIs for faster testing
  region_mask <- NeuroVol(sample(1:2, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  
  # Check that permutation results are included in performance table
  perf_cols <- colnames(res$performance_table)
  expect_true(any(grepl("^p_", perf_cols)))
  expect_true(any(grepl("^z_", perf_cols)))
})

test_that("feature_rsa_model with cache_pca=TRUE works correctly", {
  set.seed(123)
  
  # Create a small dataset
  dset <- gen_sample_dataset(c(3,3,3), 30, blocks=2)
  
  # Create a feature matrix
  Fmat <- matrix(rnorm(30*6), 30, 6)
  labels <- paste0("obs", 1:30)
  
  # Create feature_rsa_design
  fdes <- feature_rsa_design(F=Fmat, labels=labels)
  
  # Create a feature_rsa_model with PCA caching
  mspec <- feature_rsa_model(
    dset$dataset, 
    fdes, 
    method="pca", 
    crossval=blocked_cross_validation(dset$design$block_var),
    cache_pca=TRUE  # Enable PCA caching
  )
  
  # Create a region mask with just 2 ROIs
  region_mask <- NeuroVol(sample(1:2, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  expect_true(!is.null(res$performance_table))
})

test_that("feature_rsa_model with different max_comps values works correctly", {
  set.seed(123)
  
  # Create a small dataset
  dset <- gen_sample_dataset(c(3,3,3), 40, blocks=2)
  
  # Create a feature matrix with 10 dimensions
  Fmat <- matrix(rnorm(40*10), 40, 10)
  labels <- paste0("obs", 1:40)
  
  # Create feature_rsa_design with different max_comps values
  fdes1 <- feature_rsa_design(F=Fmat, labels=labels, max_comps=3)
  fdes2 <- feature_rsa_design(F=Fmat, labels=labels, max_comps=5)
  
  # Create feature_rsa_models
  mspec1 <- feature_rsa_model(dset$dataset, fdes1, method="pca", 
                             crossval=blocked_cross_validation(dset$design$block_var))
  mspec2 <- feature_rsa_model(dset$dataset, fdes2, method="pca", 
                             crossval=blocked_cross_validation(dset$design$block_var))
  
  # Check that max_comps was properly set
  expect_equal(mspec1$max_comps, 3)
  expect_equal(mspec2$max_comps, 5)
  
  # Create a region mask
  region_mask <- NeuroVol(sample(1:2, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Run regional analysis for both models
  res1 <- run_regional(mspec1, region_mask)
  res2 <- run_regional(mspec2, region_mask)
  
  # Check results
  expect_true(!is.null(res1))
  expect_true(!is.null(res2))
  expect_s3_class(res1, "regional_mvpa_result")
  expect_s3_class(res2, "regional_mvpa_result")
})

test_that("regional feature_rsa_model with pca method runs without error and returns performance", {
  dset <- gen_sample_dataset(c(4,4,4), 80, blocks=4)
  
  # Create an F matrix directly
  Fmat <- matrix(rnorm(80*15), 80, 15)
  labels <- paste0("obs", 1:80)
  
  fdes <- feature_rsa_design(F=Fmat, labels=labels) # no dimension reduction, just use as is
  mspec <- feature_rsa_model(dset$dataset, fdes, method="pca", crossval=blocked_cross_validation(dset$design$block_var))
  
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  res <- run_regional(mspec, region_mask)
  
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  # Check if performance metrics are available
  expect_true("performance_table" %in% names(res))
  # Check specific performance metrics
  expect_true("mean_correlation" %in% colnames(res$performance_table))
  expect_true("cor_difference" %in% colnames(res$performance_table))
  expect_true("voxel_correlation" %in% colnames(res$performance_table))
  expect_true("mse" %in% colnames(res$performance_table))
  expect_true("r_squared" %in% colnames(res$performance_table))
})


test_that("can compare feature_rsa with different methods", {
  set.seed(123)
  
  # Create a small dataset for faster testing
  dset <- gen_sample_dataset(c(4,4,4), 60, blocks=2)
  
  # Create a feature matrix
  Fmat <- matrix(rnorm(60*10), 60, 10)
  labels <- paste0("obs", 1:60)
  
  # Create feature_rsa_design
  fdes <- feature_rsa_design(F=Fmat, labels=labels, max_comps=10) # reduce to 5 dims
  
  # Create a region mask with just 2 ROIs for faster testing
  region_mask <- NeuroVol(sample(1:2, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  
  # Compare different methods
  methods <- c("scca", "pca", "pls")
  results_list <- list()
  
  for (method in methods) {
    print(method)
    # Create model with current method
    mspec <- feature_rsa_model(
      dataset = dset$dataset,
      design = fdes,
      method = method,
      crossval = blocked_cross_validation(dset$design$block_var)
    )
    
    # Run regional analysis
    results_list[[method]] <- run_regional(mspec, region_mask)
    
    # Check results
    expect_true(!is.null(results_list[[method]]))
    expect_s3_class(results_list[[method]], "regional_mvpa_result")
    expect_true(!is.null(results_list[[method]]$performance_table))
  }
  
  # Compare performance metrics across methods
  for (method in methods) {
    # Check that each method has the expected performance metrics
    perf_table <- results_list[[method]]$performance_table
    expect_true("mean_correlation" %in% colnames(perf_table))
    expect_true("cor_difference" %in% colnames(perf_table))
    expect_true("voxel_correlation" %in% colnames(perf_table))
    expect_true("mse" %in% colnames(perf_table))
    expect_true("r_squared" %in% colnames(perf_table))
  }
})

test_that("feature_rsa_model with permutation testing produces valid p-values", {
  set.seed(123)
  
  # Create a small dataset for faster testing
  dset <- gen_sample_dataset(c(3,3,3), 40, blocks=2)
  
  # Create a feature matrix with a strong signal
  X <- matrix(rnorm(40*8), 40, 8)
  B <- matrix(rnorm(8*3), 8, 3)
  Fmat <- X %*% B + matrix(rnorm(40*3, sd=0.1), 40, 3)
  labels <- paste0("obs", 1:40)
  
  # Create feature_rsa_design
  fdes <- feature_rsa_design(F=Fmat, labels=labels)
  
  # Create a feature_rsa_model with permutation testing
  mspec <- feature_rsa_model(
    dset$dataset, 
    fdes, 
    method="pca", 
    crossval=blocked_cross_validation(dset$design$block_var),
    nperm=20,  # Small number for testing
    permute_by="observations"
  )
  
  # Create a region mask with just 1 ROI for faster testing
  region_mask <- NeuroVol(rep(1, length(dset$dataset$mask)), space(dset$dataset$mask))
  
  # Run regional analysis
  res <- run_regional(mspec, region_mask)
  
  # Check results
  expect_true(!is.null(res))
  expect_s3_class(res, "regional_mvpa_result")
  
  # Check that p-values are between 0 and 1
  p_value_cols <- grep("^p_", colnames(res$performance_table), value=TRUE)
  for (col in p_value_cols) {
    p_values <- res$performance_table[[col]]
    expect_true(all(p_values >= 0 & p_values <= 1))
  }
  
  # Check that z-scores are reasonable
  z_score_cols <- grep("^z_", colnames(res$performance_table), value=TRUE)
  for (col in z_score_cols) {
    z_scores <- res$performance_table[[col]]
    expect_true(all(!is.na(z_scores)))
  }
})
</file>

<file path="tests/testthat/test_mvpa_regional.R">
library(neuroim2)
library(neurosurf)
library(testthat)
library(assertthat)


context("mvpa regional")

test_that("mvpa_regional with 5 ROIS runs without error", {
  
  dset <- gen_sample_dataset(c(10,10,8), nobs=100, nlevels=3, data_mode="image", 
                             response_type="categorical")
  cval <- twofold_blocked_cross_validation(dset$design$block_var)
  
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dset$dataset, dset$design, model_type="classification", crossval=cval, return_fits=TRUE)
  res <- run_regional(mspec, region_mask)
  expect_true(!is.null(res))
  
})



test_that("mvpa_regional with 5 ROIS runs without error and can access fitted model", {
  
  dset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3, data_mode="image", 
                             response_type="categorical")
  cval <- twofold_blocked_cross_validation(dset$design$block_var)
  
  region_mask <- NeuroVol(sample(1:5, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dset$dataset, dset$design, model_type="classification", crossval=cval, return_fits=TRUE)
  res <- run_regional(mspec, region_mask, return_fits=TRUE)
  fit1 <- res$fits[[1]]
  expect_true(class(fit1)[1] == "weighted_model")
  
})

test_that("can combine two prediction tables from two regional analyses", {
  
  dset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3, data_mode="image", 
                             response_type="categorical")
  cval <- twofold_blocked_cross_validation(dset$design$block_var)
  
  region_mask <- NeuroVol(sample(1:2, size=length(dset$dataset$mask), replace=TRUE), space(dset$dataset$mask))
  rmask1 <- region_mask
  rmask1[rmask1 == 1] <- 1
  rmask1[rmask1 == 2] <- 0
  
  rmask2 <- region_mask
  rmask2[rmask2 == 1] <- 0
  rmask2[rmask2 == 2] <- 2
  
  
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dset$dataset, dset$design, model_type="classification", crossval=cval)
  res1 <- run_regional(mspec, region_mask=rmask1, return_fits=TRUE)
  res2 <- run_regional(mspec, region_mask=rmask2, return_fits=TRUE)
  
  ptab <- combine_prediction_tables(list(res1$prediction_table, res2$prediction_table))
  ptab2 <- combine_prediction_tables(list(res1$prediction_table, res2$prediction_table), collapse_regions=TRUE)
  expect_true(nrow(ptab) == nrow(res1$prediction_table)*2)
  expect_true(nrow(ptab2) == nrow(res1$prediction_table))
})

test_that("surface_based mvpa_regional with 5 ROIS runs without error", {
  
  dset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3, data_mode="surface", response_type="categorical")
  cval <- blocked_cross_validation(dset$design$block_var)
  
  maskid <- sample(1:5, size=length(dset$dataset$mask), replace=TRUE)
  region_mask <- NeuroSurface(dset$dataset$train_data@geometry, indices=nodes(dset$dataset$train_data@geometry), 
                              data=maskid)
  
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dset$dataset, dset$design, model_type="classification", crossval=cval)
  res <- run_regional(mspec, region_mask, return_fits=TRUE)
  expect_true(!is.null(res))
  
})

test_that("mvpa_regional with 5 ROIS with sda_boot runs without error", {
  
  dataset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3,response_type="categorical")
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), neuroim2::space(dataset$dataset$mask))
  model <- load_model("sda_boot")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval)
  res <- run_regional(mspec, regionMask)
  expect_true(!is.null(res))
})

test_that("mvpa_regional with 5 ROIS with lda_thomaz_boot runs without error", {
  
  dataset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3,response_type="categorical")
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), neuroim2::space(dataset$dataset$mask))
  model <- load_model("sda_boot")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval)
  res <- run_regional(mspec, regionMask)
  expect_true(!is.null(res))
})

 
test_that("mvpa_regional with 5 ROIS with sda_boot and custom_performance runs without error", {

  dataset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3,response_type="categorical")
  cval <- blocked_cross_validation(dataset$design$block_var)
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), neuroim2::space(dataset$dataset$mask))
  model <- load_model("sda_boot")
  
  p <- function(x) { return(list(x=1)) }
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, performance=p)
  res <- run_regional(mspec, regionMask)
  expect_true(!is.null(res))
  expect_true(names(res$vol_results) == "x")
})

test_that("mvpa_regional with 5 ROIS runs and sda without error", {
  tuneGrid <- expand.grid(lambda=c(.01), diagonal=c(TRUE, FALSE))
  model <- load_model("sda")
  
  dataset <- gen_sample_dataset(c(10,10,5), nobs=100, nlevels=4)
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, tune_grid=tuneGrid, tune_reps=10)
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  res <- run_regional(mspec, regionMask, TRUE)
  expect_true(!is.null(res))
})

test_that("mvpa_regional with 5 ROIS and random forest without error", {
  
  model <- load_model("rf")
  dataset <- gen_sample_dataset(c(10,10,5), nobs=100, nlevels=3)
  cval <- blocked_cross_validation(dataset$design$block_var)

  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, 
                      tune_grid=data.frame(mtry=c(2,4,6)))
  res <- run_regional(mspec, regionMask)
  expect_true(!is.null(res))
})

test_that("mvpa_regional with 5 ROIS and random forest and k-fold cross-validation without error", {
  
  model <- load_model("rf")
  dataset <- gen_sample_dataset(c(10,10,6), nobs=100, nlevels=3)
  cval <- kfold_cross_validation(length(dataset$design$block_var), nfolds=4)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, 
                      tune_grid=data.frame(mtry=c(2,4,6)))
  res <- run_regional(mspec, regionMask)
  expect_true(!is.null(res))
})

test_that("mvpa_regional with 5 ROIS and corclass and k-fold cross-validation without error", {
  
  model <- load_model("corclass")
  tune_grid <- expand.grid(method=c("pearson", "kendall", "spearman"), robust=c(TRUE,FALSE))
  dataset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3)
  cval <- kfold_cross_validation(length(dataset$design$block_var), nfolds=4)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, 
                      tune_grid=tune_grid)
  res <- run_regional(mspec, regionMask)
  expect_true(!is.null(res))
})

test_that("mvpa_regional with 5 ROIS runs and external test set", {
  
  model <- load_model("rf")
  dataset <- gen_sample_dataset(c(10,10,4), nobs=100, nlevels=3, ntest_obs=200, external_test=TRUE)
  dataset$design$test_design$auxvar = rnorm(nrow(dataset$design$test_design))
  
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, 
                      model_type="classification", crossval=cval, 
                      tune_grid=data.frame(mtry=c(2,4,6)))
  
  expect_true(has_test_set(dataset$design))
  res <- run_regional(mspec, regionMask, coalesce_design_vars = TRUE)
  expect_true(!is.null(res))
})



# test_that("mvpa_regional with 5 ROIS and consensus learning runs without error", {
#   
#   dataset <- gen_dataset(c(10,10,2), 100, 3)
#   crossVal <- blocked_cross_validation(dataset$blockVar)
#   regionMask <- NeuroVol(sample(1:5, size=length(dataset$mask), replace=TRUE), space(dataset$mask))
#   
#   model <- load_model("sda")
#   res <- mvpa_regional(dataset, model, regionMask, crossVal)
#   
#   consResult1 <- consensusWeights(res$resultSet, "glmnet")
#   consResult2 <- consensusWeights(res$resultSet, "greedy")
#   consResult3 <- consensusWeights(res$resultSet, "auc_weights")
#   consResult4 <- consensusWeights(res$resultSet, "equal_weights")
#  
# })
# 
# test_that("mvpa_regional_consensus with 5 ROIS runs without error", {
#   
#   dataset <- gen_dataset(c(10,10,2), 100, 3)
#   regionMask <- NeuroVol(sample(1:5, size=length(dataset$mask), replace=TRUE), space(dataset$mask))
#   
#   model <- load_model("sda")
#   res <- mvpa_regional_consensus(dataset, model, regionMask)
#   
# })



test_that("mvpa_regional with 5 ROIs and ANOVA FeatureSelection with topk=10", {
  
  dataset <- gen_sample_dataset(c(10,10,5), nobs=100, nlevels=6)
  cval <- blocked_cross_validation(dataset$design$block_var)
  regionMask <- NeuroVol(sample(1:10, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  model <- load_model("sda")
  fsel <- feature_selector("FTest", "topk", 10)
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, feature_selector=fsel)
  res <- run_regional(mspec, regionMask, return_fits=TRUE)
  expect_true(!is.null(res))
})

test_that("mvpa_regional with 5 ROIs and catscore FeatureSelection with top_p=.1", {
  
  dataset <- gen_sample_dataset(c(10,10,5), nobs=100, nlevels=6)
  cval <- blocked_cross_validation(dataset$design$block_var)
  regionMask <- NeuroVol(sample(1:10, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  model <- load_model("sda")
  fsel <- feature_selector("catscore", "top_p", .1)
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, feature_selector=fsel)
  res <- run_regional(mspec, regionMask, return_fits=TRUE)
  expect_true(!is.null(res))
})




test_that("mvpa_regional with 5 ROIs and ANOVA FeatureSelection with topp=.4", {
  dataset <- gen_sample_dataset(c(10,10,10), nobs=100, nlevels=2)
  cval <- blocked_cross_validation(dataset$design$block_var)
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  model <- load_model("sda_notune")
  fsel <- feature_selector("FTest", "topp", .5)
  mspec <- mvpa_model(model, dataset$dataset, dataset$design,model_type="classification", crossval=cval, feature_selector=fsel)
  res <- run_regional(mspec, regionMask, return_fits=TRUE)
  expect_true(!is.null(res))
})


test_that("mvpa_regional with regression and 5 ROIs runs without error", {
  
  dataset <- gen_sample_dataset(c(10,10,6), 100, response_type="continuous")
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  regionMask <- NeuroVol(sample(1:4, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  tune_grid <- expand.grid(alpha=c(.1,.5), lambda=c(.001,.2,.5))
  
  model <- load_model("glmnet")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="regression", crossval=cval, tune_grid=tune_grid)
  res <- run_regional(mspec, regionMask)
  expect_equal(nobs(dataset$design), 100)
  expect_true(!is.null(res))
  
})

test_that("standard mvpa_regional and custom cross-validation runs without error", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  sample_set <- replicate(5, {
    list(train=sample(1:80), test=sample(1:100))
  }, simplify=FALSE)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  cval <- custom_cross_validation(sample_set)
  
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval)
  res <- run_regional(mspec,regionMask)
  expect_true(!is.null(res))
})

test_that("standard mvpa_regional and custom cross-validation and splitting var runs without error", {
  
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3, split_by=factor(rep(1:4, each=25)))
  sample_set <- replicate(5, {
    list(train=sample(1:80), test=sample(1:100))
  }, simplify=FALSE)
  
  regionMask <- NeuroVol(sample(1:5, size=length(dataset$dataset$mask), replace=TRUE), space(dataset$dataset$mask))
  cval <- custom_cross_validation(sample_set)
  
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval)
  res <- run_regional(mspec,regionMask)
  expect_true(!is.null(res))
})
</file>

<file path="R/distcalc.R">
#' @export
pairwise_dist.default <- function(X, dist_obj) {
  stop("pairwise_dist not implemented for objects of class ", class(dist_obj)[1])
}


#' Create a Distance Function Object
#'
#' Constructs a generic distance function object, storing:
#' \itemize{
#'   \item \code{name}: The name (method) of the distance (e.g., "euclidean", "cordist", "mahalanobis").
#'   \item \code{labels}: (Optional) a vector of labels associated with the data rows.
#'   \item \code{...}: Additional parameters relevant to the specific distance method 
#'         (e.g., correlation method for \code{cordist}, number of components for \code{pcadist}, etc.).
#' }
#'
#' This object is used by \code{pairwise_dist()} to compute an \strong{N x N} matrix of pairwise distances 
#' between rows of a data matrix.
#'
#' @param name A character string specifying the distance method (e.g., "euclidean", "cordist").
#' @param labels A vector of row labels (optional), primarily for informational/reference purposes.
#' @param ... Additional parameters for the distance method (e.g. `method="pearson"` for correlation, 
#'            or \code{whiten=TRUE} for PCA-based distances).
#'
#' @return An S3 object with class \code{c(name, "distfun")} that can be passed to \code{pairwise_dist()}.
#'
#' @details
#' The distance function object itself does \emph{not} exclude same-block comparisons or reorder rows by label.
#' Those tasks (if needed) are handled downstream (for example, in \code{second_order_similarity}).
#'
#' @examples
#' # Create a Euclidean distance function object
#' dist_obj_euc <- create_dist("euclidean")
#'
#' # Create a correlation distance function object with a specified correlation method
#' dist_obj_cor <- create_dist("cordist", method="spearman")
#'
#' @export
create_dist <- function(name, labels=NULL, ...) {
  structure(
    list(name = name, labels = labels, ...), 
    class = c(name, "distfun")
  )
}


#' Distance Function Constructors
#'
#' These convenience functions build specific types of distance function objects via \code{\link{create_dist}}.
#' Each returns an S3 object inheriting from \code{c("<method>", "distfun")}.
#'
#' @param labels Optional vector of row labels (not directly used in distance calculation).
#' @param method For \code{cordist}, the correlation method: "pearson" or "spearman".
#' @param ncomp For \code{pcadist}, the number of components (or a function threshold).
#' @param whiten For \code{pcadist}, whether to whiten principal components (logical).
#' @param threshfun For \code{pcadist}, an optional function that determines how many PCs to retain 
#'                  based on \code{pres$sdev^2}.
#' @param dist_method For \code{pcadist}, the base distance measure in PC space ("euclidean", "manhattan", or "cosine").
#'
#' @return An S3 object with class \code{c("<method>", "distfun")}.
#'
#' @details
#' - \code{cordist(labels, method="pearson")} → correlation-based distance.  
#' - \code{mahadist(labels)} → Mahalanobis distance.  
#' - \code{eucdist(labels)} → Euclidean distance.  
#' - \code{robustmahadist(labels)} → Mahalanobis distance using robust covariance.  
#' - \code{pcadist(labels, ...)} → distance in reduced PCA space.
#'
#' @examples
#' dist_obj_1 <- cordist(method="pearson")
#' dist_obj_2 <- mahadist()
#' dist_obj_3 <- eucdist()
#' dist_obj_4 <- robustmahadist()
#' dist_obj_5 <- pcadist(ncomp=2, dist_method="cosine")
#'
#' @seealso \code{\link{create_dist}} for the underlying constructor.
#'
#' @rdname distance-constructors
#' @export
cordist <- function(labels=NULL, method=c("pearson", "spearman")) {
  method <- match.arg(method)
  create_dist(name="cordist", labels=labels, method=method)
}

#' @rdname distance-constructors
#' @export
mahadist <- function(labels=NULL) {
  create_dist("mahalanobis", labels)
}

#' @rdname distance-constructors
#' @export
eucdist <- function(labels=NULL) {
  create_dist("euclidean", labels)
}

#' @rdname distance-constructors
#' @export
robustmahadist <- function(labels=NULL) {
  create_dist("robustmahadist", labels)
}

#' @rdname distance-constructors
#' @export
pcadist <- function(labels=NULL, ncomp=2, whiten=TRUE, threshfun=NULL,
                    dist_method=c("euclidean", "manhattan", "cosine")) {
  dist_method <- match.arg(dist_method)
  if (is.null(threshfun)) {
    # By default, always use the user-specified 'ncomp'
    tfun <- function(x) ncomp
  } else {
    stopifnot(is.function(threshfun))
    tfun <- threshfun
  }
  create_dist("pcadist", labels, whiten=whiten, threshfun=tfun, dist_method=dist_method)
}


#' Compute Pairwise Correlation Distances
#'
#' Computes a full NxN matrix of correlation-based distances: \code{1 - cor(t(X), method=obj$method)}.
#' \strong{No block-based exclusion is performed here.}
#'
#' @param obj A distance function object of class \code{c("cordist", "distfun")}.
#' @param X A numeric matrix (rows = observations, columns = variables).
#'
#' @return An \strong{N x N numeric matrix} of pairwise distances.
#'
#' @details
#' This function calculates correlation distances among the rows of \code{X}. 
#' If you have a block variable and wish to exclude same-block comparisons, 
#' handle that \emph{after} obtaining this full matrix (e.g., in \code{second_order_similarity}).
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_obj <- cordist(method="pearson")
#' dist_matrix <- pairwise_dist(dist_obj, X)
#'
#' @export
pairwise_dist.cordist <- function(obj, X) {
  1 - cor(t(X), method=obj$method)
}


#' Compute Pairwise Euclidean Distances
#'
#' Returns a full NxN matrix of Euclidean distances among the rows of \code{X}.
#' \strong{No block-based exclusion is performed.}
#'
#' @param obj A distance function object of class \code{c("euclidean", "distfun")}.
#' @param X A numeric matrix (rows = observations, columns = variables).
#'
#' @return An \strong{N x N numeric matrix} of pairwise Euclidean distances.
#'
#' @details
#' This function simply calls \code{dist(X)} internally and converts it to a matrix via \code{as.matrix()}.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_obj <- eucdist()
#' dist_matrix <- pairwise_dist(dist_obj, X)
#'
#' @export
pairwise_dist.euclidean <- function(obj, X) {
  as.matrix(dist(X, method="euclidean"))
}


#' Compute Pairwise Mahalanobis Distances
#'
#' Returns a full NxN matrix of Mahalanobis distances among rows of \code{X}, using a shrunken inverse covariance.
#' \strong{No block-based exclusion is performed.}
#'
#' @param obj A distance function object of class \code{c("mahalanobis", "distfun")}. 
#' @param X A numeric matrix (rows = observations, columns = variables).
#'
#' @return An \strong{N x N numeric matrix} of pairwise Mahalanobis distances.
#'
#' @details
#' Uses \code{corpcor::invcov.shrink} on \code{X} to estimate the inverse covariance matrix 
#' and then computes \code{mahalanobis(...)} for each row vs. each other row.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_obj <- mahadist()
#' dist_matrix <- pairwise_dist(dist_obj, X)
#'
#' @export
#' @importFrom corpcor invcov.shrink
#' @importFrom stats mahalanobis
pairwise_dist.mahalanobis <- function(obj, X) {
  inv_cov <- corpcor::invcov.shrink(X)
  n <- nrow(X)
  
  dist_matrix_sq <- matrix(0, n, n)
  for (i in seq_len(n)) {
    dist_matrix_sq[i, ] <- mahalanobis(X, center = X[i, ], cov = inv_cov, inverted = TRUE)
  }
  
  sqrt(dist_matrix_sq)
}


#' Compute Pairwise PCA-Based Distances
#'
#' Returns a full NxN matrix of distances in a PCA-reduced subspace, with an optional whitening step.
#' \strong{No block-based exclusion is performed.}
#'
#' @param obj A distance function object of class \code{c("pcadist", "distfun")}.
#' @param X A numeric matrix.
#'
#' @return An \strong{N x N numeric matrix} of pairwise distances in the reduced PCA space.
#'
#' @details
#' 1. Performs \code{prcomp(X)} (centered, scaled=TRUE).
#' 2. Determines the number of components via \code{obj$threshfun(...)}.
#' 3. Optionally whitens (divide each principal component by its standard deviation).
#' 4. Computes pairwise distances on the reduced data using \code{obj$dist_method}.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_obj <- pcadist(ncomp=3, dist_method="cosine")
#' dist_matrix <- pairwise_dist(dist_obj, X)
#'
#' @export
#' @importFrom stats prcomp dist
pairwise_dist.pcadist <- function(obj, X) {
  pres <- prcomp(X, center = TRUE, scale. = TRUE)
  ncomp <- obj$threshfun(pres$sdev^2)
  if (ncomp < 1) {
    ncomp <- 1
    warning("Number of components set to 1, as threshold function returned < 1.")
  }
  
  # Extract PC space
  pc_space <- pres$x[, seq_len(ncomp), drop=FALSE]
  
  # Optional whitening
  if (obj$whiten) {
    pc_space <- pc_space %*% diag(1 / pres$sdev[seq_len(ncomp)], ncomp, ncomp)
  }
  
  # Distances
  if (obj$dist_method %in% c("euclidean", "manhattan")) {
    as.matrix(dist(pc_space, method = obj$dist_method))
  } else if (obj$dist_method == "cosine") {
    # proxy::dist for 'cosine' distance
    as.matrix(proxy::dist(pc_space, method = "cosine"))
  }
}


#' Compute Pairwise Robust Mahalanobis Distances
#'
#' Returns a full NxN matrix of robust Mahalanobis distances, using a robust covariance estimator.
#' \strong{No block-based exclusion is performed.}
#'
#' @param obj A distance function object of class \code{c("robustmahadist", "distfun")}.
#' @param X A numeric matrix.
#'
#' @return An \strong{N x N numeric matrix} of pairwise robust Mahalanobis distances.
#'
#' @details
#' - Estimates a robust covariance with \code{robustcov::covGK(X)} (make sure the \code{robustcov} package is installed).
#' - Then calls \code{corpcor::invcov.shrink} to get an inverse covariance estimate.
#' - Finally, loops over row pairs to compute \code{(x_i - x_j) * inv_cov * (x_i - x_j)^T}.
#'
#' @examples
#' \dontrun{
#'   library(robustcov)
#'   X <- matrix(rnorm(100), 10, 10)
#'   dist_obj <- robustmahadist()
#'   dist_matrix <- pairwise_dist(dist_obj, X)
#' }
#'
#' @export
pairwise_dist.robustmahadist <- function(obj, X) {
  robust_cov <- robustcov::covGK(X)
  inv_cov <- corpcor::invcov.shrink(robust_cov)
  
  n <- nrow(X)
  dist_matrix <- matrix(0, n, n)
  
  for (i in seq_len(n-1)) {
    for (j in seq((i+1), n)) {
      diff <- X[i, ] - X[j, ]
      dist_val <- sqrt(t(diff) %*% inv_cov %*% diff)
      dist_matrix[i, j] <- dist_val
      dist_matrix[j, i] <- dist_val
    }
  }
  dist_matrix
}
</file>

<file path="R/common.R">
#' @import stringr
#' @importFrom io qread
initialize_configuration <- function(args) {
  if (!is.null(args$config)) {
    if (!file.exists(args$config)) {
      flog.error("cannot find configuration file: %s", args$config)
      stop()
    } else if (str_detect(args$config, "\\.yaml$")) {
      confyaml <- qread(args$config)
      config <- as.environment(confyaml)
    } else if (str_detect(args$config, "\\.[rR]")) {
      config <- new.env()
      source(args$config, config)
    }
  }
  
  config

}


#' @noRd
#' @keywords internal
initialize_standard_parameters <- function(config, args, analysisType) {
  set_arg("train_design", config, args, "mvpa_design.txt")
  set_arg("test_design", config, args, NULL)
  set_arg("train_data", config, args, "mvpa_design.txt")
  set_arg("test_data", config, args, NULL)
  set_arg("model", config, args, "corsim")
  set_arg("feature_selector", config, args, NULL)
  set_arg("pthreads", config, args, 1)
  set_arg("label_column", config, args, "labels")
  set_arg("skip_if_folder_exists", config, args, FALSE)
  set_arg("output", config, args, paste0(analysisType, "_", config$labelColumn))
  set_arg("block_column", config, args, NULL)
  set_arg("normalize_samples", config, args, FALSE)
  set_arg("tune_grid", config, args, NULL)
  set_arg("mask", config, args, NULL)
  set_arg("class_metrics", config, args, TRUE)
  set_arg("split_by", config, args, NULL)
  set_arg("custom_performance", config, args, NULL)
  set_arg("test_label_column", config, args, NULL)
  set_arg("data_mode", config, args, "image")
  
  config
}

#' @noRd
#' @keywords internal
#' @importFrom purrr map_dbl map
#' @importFrom neuroim2 SparseNeuroVec vols
normalize_image_samples <- function(bvec, mask) {
  vlist <- bvec %>% vols() %>% furrr::future_map(function(v) {
    scale(as.numeric(v[which(mask>0)]))[,1]
  }, .options=furrr::furrr_options(seed=TRUE))
  
  norm_datavec <- do.call(cbind, vlist)
  #norm_datavec <- do.call(cbind, eachVolume(bvec, function(x) scale(x)[,1], mask=mask))
  SparseNeuroVec(norm_datavec, space(bvec), mask=mask)  
}

#' @noRd
#' @keywords internal
#' @importFrom purrr map_dbl map
#' @importFrom neuroim2 vectors
standardize_vars <- function(bvec, mask, blockvar) {
  assertthat::assert_that(length(blockvar) == dim(bvec)[4])
  vlist <- bvec %>% vectors(subset=which(mask>0)) %>% furrr::future_map(function(v) {
    if (all(v == 0)) v else {
      unlist(map(split(v, blockvar), scale))
    }
  }, .options = furrr::furrr_options(seed=TRUE))
  
  sdatavec <- do.call(cbind, vlist)
  #norm_datavec <- do.call(cbind, eachVolume(bvec, function(x) scale(x)[,1], mask=mask))
  SparseNeuroVec(sdatavec, space(bvec), mask=mask)  
}


#' @noRd
#' @keywords internal       
normalize_surface_samples <- function(bvec, mask) {
  mat <- scale(bvec@data[neuroim2::indices(bvec), ,drop=FALSE])
  
  m2 <- matrix(0, length(nodes(bvec)), ncol(bvec@data))
  m2[indices(bvec),] <- mat
  
  neurosurf::NeuroSurfaceVector(geometry(bvec), indices=indices(bvec), m2)
}

#' @noRd
#' @keywords internal
initialize_surface_data <- function(config) {
  if (!is.null(config$train_subset)) {
    indices <- which(config$train_subset)
    flog.info("length of training subset %s", length(indices))
  }
  
  train_surfaces <- load_surface_data(config, "train_data", colind=indices) 
  
  if (!is.null(config$test_data)) {
    flog.info("loading test surface data: %s", config$test_data)
    indices <- which(config$test_subset)
    flog.info("length of test subset %s", length(indices))
    
    test_surfaces <- if (!is.null(config$test_subset)) {
      load_surface_data(config, "test_data", colind=indices)
    } else {
      load_surface_data(config, "test_data")
    }
    
  } else {
    test_surfaces <- NULL
  }
  
  if (config$normalize_samples) {
    flog.info("Normalizing: centering and scaling each surface of training data")
    ret <- lapply(train_surfaces, normalize_surface_samples)
    names(ret) <- names(train_surfaces)
    train_surfaces <- ret
    
    if (!is.null(test_surfaces)) {
      flog.info("Normalizing: centering and scaling each surface of test data")
      ret <- lapply(test_surfaces, normalize_surface_samples)
      names(ret) <- names(test_surfaces)
      test_surfaces <- ret
    }
  }
  
  if (!is.null(test_surfaces) && !(length(train_surfaces) == length(test_surfaces))) {
    flog.info("number of training surfaces: %s", length(train_surfaces))
    flog.info("number of test surfaces: %s", length(test_surfaces))
    flog.error("number of training surface entries must equal number of test surface entries")
    stop()
  }
  
  ret <- if (!is.null(config$mask)) {
    flog.info("loading mask: %s ", config$mask)
    masksurf <- load_surface_mask(config$mask, train_surfaces)
    
    lapply(seq_along(train_surfaces), function(i) {
      mvpa_surface_dataset(train_surfaces[[i]], test_surfaces[[i]], name=names(train_surfaces)[i], mask=masksurf[[i]])
    })
    
  } else {
    lapply(seq_along(train_surfaces), function(i) {
      mvpa_surface_dataset(train_surfaces[[i]], test_surfaces[[i]], name=names(train_surfaces)[i])
    })
  }
  
  names(ret) <- names(train_surfaces)
  ret

 
}

#' @noRd
#' @keywords internal
initialize_feature_selection <- function(config) {
  if (!is.null(config$feature_selector)) {
    feature_selector(config$feature_selector$method, config$feature_selector$cutoff_type, as.numeric(config$feature_selector$cutoff_value))
  } else {
    NULL
  }
}

#' @importFrom methods as
#' @keywords internal
initialize_image_data <- function(config, mask) {
  if (!is.null(config$train_subset)) {
    indices <- which(config$train_subset)
    flog.info("length of training subset %s", length(indices))
  }
  
  mask_volume <- as(mask, "LogicalNeuroVol")
  train_datavec <- load_image_data(config, "train_data", mask_volume=mask_volume, indices=indices)    

  if (!is.null(config$test_data)) {
    flog.info("loading test data: %s", config$test_data)
    indices=which(config$test_subset)
    flog.info("length of test subset %s", length(indices))
    
    if (!is.null(config$test_subset)) {
      test_datavec <- load_image_data(config, "test_data", mask_volume=mask_volume, indices=indices)
    } else {
      test_datavec <- load_image_data(config, "test_data", mask_volume=mask_volume)
    }
  } else {
    test_datavec <- NULL
  }
  
  if (config$normalize_samples) {
    flog.info("Normalizing: centering and scaling each volume of training data")
    train_datavec <- normalize_image_samples(train_datavec, mask_volume)
    
    if (!is.null(test_datavec)) {
      flog.info("Normalizing: centering and scaling each volume of test data")
      test_datavec <- normalize_image_samples(test_datavec, mask_volume)
    }
  }
  
  mvpa_dataset(train_datavec, test_datavec, mask=mask)

}

#' @noRd
#' @keywords internal
initialize_design <- function(config) {
  if (is.character(config$train_subset)) {
    config$train_subset <- eval(parse(text=config$train_subset))
  }
  
  if (is.character(config$test_subset)) {
    config$test_subset <- eval(parse(text=config$test_subset))
  }
  
  if (is.data.frame(config$train_design)) {
    config$full_train_design <- config$train_design
  } else {
    ## full design
    config$full_train_design <- if (length(config$train_design) > 1) {
      flog.info(paste("concatenating %s design files", length(config$train_design)))
      do.call(rbind, lapply(config$train_design, read.table, header=TRUE, comment.char=";"))
    } else {
      read.table(config$train_design, header=TRUE, comment.char=";")
    }
  }
  
  ## subset of training samples
  config$train_subset <- load_subset(config$full_train_design, config$train_subset)
  
  ## training design
  config$train_design <- config$full_train_design[config$train_subset,]
  
 
  flog.info(paste("training subset contains", nrow(config$train_design), "of", nrow(config$full_train_design), "rows."))
  
  if (!is.null(config$test_design) && is.null(config$test_data)) {
    flog.error("test_design %s is supplied with no test_data", config$test_data)
    stop()
  }
  
  if (is.null(config$test_design) && !is.null(config$test_data)) {
    flog.error("test_data %s is supplied with no test_design", config$test_data)
    stop()
  }
  
  
  #if (!is.null(config$test_subset) && is.null(config$test_design) && is.null(config$test_data)) {
  #  flog.info("test subset is taken from training design table")
  #  config$test_subset <- load_subset(config$full_train_design, config$test_subset)
  #  
  #  config$test_design <- config$full_train_design[config$test_subset,]
  #  config$full_test_design <- config$test_design
  #  config$testLabels <- loadLabels(config$test_design, config)   
  #}
  
  if (!is.null(config$test_design)) {
    has_test <- TRUE
    flog.info("test design %s is specified", config$test_design)
    
    if (is.data.frame(config$test_design)) {
      config$full_test_design <- config$test_design
    } else {
      config$full_test_design <- if (length(config$test_design) > 1) {
        flog.info(paste("concatenating %s test design files", length(config$test_design)))
        do.call(rbind, lapply(config$test_design, read.table, header=TRUE, comment.char=";"))
      } else {
        read.table(config$test_design, header=TRUE, comment.char=";")
      }
    }
    
    flog.info(paste("test design contains", nrow(config$full_test_design), "rows."))
    
    config$test_subset <- load_subset(config$full_test_design, config$test_subset)
    config$test_design <- config$full_test_design[config$test_subset,]
    
    flog.info(paste("test subset contains", nrow(config$test_design), "of", nrow(config$full_test_design), "rows."))
    
    #config$testLabels <- loadTestLabels(config$test_design, config)     
    #flog.info(paste("test subset contains", nrow(config$test_design), "of", nrow(config$full_test_design), "rows.")) 
    #flog.info(paste("first 10 test labels: ", head(config$testLabels, 10), capture=TRUE))
    
  } else {
    has_test <- FALSE
    flog.info("testing is via internal cross-validation")
    #config$testLabels <- config$labels
  }
  
  if (!is.null(config$split_by)) {
    flog.info("splitting performance metrics by %s: ", deparse(config$split_by))
  }
  
  
  if (has_test) {
    ## todo what if we don't have/need "block_column"
    mvpa_design(train_design=config$train_design, 
              y_train=config$label_column, 
              test_design=config$test_design, 
              y_test=config$test_label_column, 
              block_var=config$block_column, 
              split_by=config$split_by)
  } else {
    ## todo what if we don't have/need "block_column"
    mvpa_design(train_design=config$train_design, 
                y_train=config$label_column, 
                block_var=config$block_column, 
                split_by=config$split_by)
  }
   
  
  
}

#' @noRd
#' @keywords internal
initialize_tune_grid <- function(args, config) {
  if (!is.null(args$tune_grid) && !args$tune_grid == "NULL") {
    params <- try(expand.grid(eval(parse(text=args$tune_grid))))
    
    if (inherits(params, "try-error")) {
      stop("could not parse tune_grid expresson: ", args$tune_grid)
    }
    
    flog.info("tuning grid is", params, capture=TRUE)
    params
    
  } else if (!is.null(config$tune_grid) && !is.data.frame(config$tune_grid)) {
    params <- try(lapply(config$tune_grid, function(x) eval(parse(text=x))))
    if (inherits(params, "try-error")) {
      stop("could not parse tune_grid expresson: ", config$tune_grid)
    }
    
    flog.info("tuning grid is", params, capture=TRUE)
    expand.grid(params)
    
  } else if (is.data.frame(config$tune_grid)) {
    config$tune_grid
  } else {
    NULL
  }
  
  
}


#' @noRd
#' @keywords internal
set_default <- function(name, config, default) {
  if (is.null(config[[name]])) {
    config[[name]]<- default
  }
}

#' @noRd
#' @keywords internal
set_arg <- function(name, config, args, default) {
  if (is.null(config[[name]]) && is.null(args[[name]])) {
    config[[name]] <- default
  } else if (!is.null(args[[name]])) {
    config[[name]] <- args[[name]]
  } else if (is.null(config[[name]])) {
    config[[name]] <- default
  }    
}

#' @noRd
#' @keywords internal
make_output_dir <- function(dirname) {
  if (!file.exists(dirname)) {
    system(paste("mkdir", dirname))
    dirname
  } else {
    dirname <- paste(dirname, "+", sep="")
    Recall(dirname)
  }
}

#' @noRd
#' @keywords internal
initialize_crossval <- function(config, des=NULL) {
  cval <- if (is.null(config$cross_validation) && !is.null(des$block_var)) {
    flog.info("cross-validation type: cross validation using predefined blocking variable")
    blocked_cross_validation(des$block_var)
  } else if (!is.null(config$cross_validation)) {
    assertthat::assert_that(!is.null(des))
    cval <- config$cross_validation
    
    if (cval$name == "twofold" || cval$name == "two_fold" || cval$name == "two_fold_blocked_cross_validation") {
      flog.info("cross-validation type: twofold cross-validation.")
      if (is.null(cval$nreps)) {
        cval$nreps <- 10
      }
      flog.info("cross-validation reps: %s ", cval$nreps)
      twofold_blocked_cross_validation(block_var=des$block_var, nreps=cval$nreps)
    } else if (cval$name == "blocked" || cval$name == "blocked_cross_validation") {
      blocked_cross_validation(des$block_var)
    } else if (cval$name == "custom" || cval$name == "custom_cross_validation") {
      custom_cross_validation(cval$sample_set)
    } else {
      flog.error("unrecognized cross_validation type: %s", cval$name)
      stop()
    }
    
  } else {
    flog.info("cross-validation type: 5 fold cross-validation using random splits")
    kfold_cross_validation(nobs(des))
  }
  
  cval
}


#' Load a Pre-defined MVPA Model
#'
#' Retrieves a model specification from either the pre-defined set of MVPA models or from caret's model library.
#'
#' @param name Character string specifying the model to load. Can be either:
#'   \itemize{
#'     \item A pre-defined MVPA model name:
#'     \describe{
#'       \item{corclass}{Correlation-based classifier with template matching}
#'       \item{sda_notune}{Simple Shrinkage Discriminant Analysis without tuning}
#'       \item{sda_boot}{SDA with bootstrap resampling}
#'       \item{glmnet_opt}{Elastic net with EPSGO parameter optimization}
#'       \item{sparse_sda}{SDA with sparsity constraints}
#'       \item{sda_ranking}{SDA with automatic feature ranking}
#'       \item{mgsda}{Multi-Group Sparse Discriminant Analysis}
#'       \item{lda_thomaz}{Modified LDA for high-dimensional data}
#'       \item{hdrda}{High-Dimensional Regularized Discriminant Analysis}
#'     }
#'     \item Any valid model name from caret's model library (e.g., "rf" for random forest, "svmRadial" for SVM)
#'   }
#'
#' @return A list containing the model specification with the following components:
#'   \describe{
#'     \item{type}{Model type: "Classification" or "Regression"}
#'     \item{library}{Required R package(s) for the model}
#'     \item{label}{Human-readable model name}
#'     \item{parameters}{Data frame describing tunable parameters}
#'     \item{grid}{Function to generate parameter tuning grid}
#'     \item{fit}{Function to fit the model}
#'     \item{predict}{Function to generate predictions}
#'     \item{prob}{Function to generate class probabilities (classification only)}
#'   }
#'
#' @examples
#' # Load custom MVPA model
#' model <- load_model("sda_notune")
#' 
#' # Load correlation classifier with parameter tuning options
#' corr_model <- load_model("corclass")
#' print(corr_model$parameters)  # View tunable parameters
#' 
#' # Load caret's random forest model
#' rf_model <- load_model("rf")
#' print(rf_model$parameters)  # View RF parameters
#' 
#' # Load caret's SVM model
#' svm_model <- load_model("svmRadial")
#'
#' @seealso 
#' \code{\link{MVPAModels}} for the complete list of available custom MVPA models
#' 
#' \code{\link[caret]{getModelInfo}} for the complete list of available caret models
#' 
#' \code{\link{mvpa_model}} for using these models in MVPA analyses
#'
#' @export
load_model <- function(name) {
  if (exists(name, envir=MVPAModels)) {
    return(get(name, envir=MVPAModels))
  }
  
  # Try loading from caret if not found in MVPAModels
  caret_model <- try(getModelInfo(name, regex=FALSE)[[1]], silent=TRUE)
  if (!inherits(caret_model, "try-error")) {
    return(caret_model)
  }
  
  stop("Model '", name, "' not found in MVPAModels or caret library")
}
load_model <- function(name) {
  registry <- MVPAModels
  
  ret <- if (!is.null(registry[[name]])) {
    registry[[name]]   
  } else if (length(caret::getModelInfo(name)) > 0) {
    caret::getModelInfo(name)[[name]] 
  } else {
    stop(paste("unrecognized model: ", name))
  }
  
  ret$label <- name
  
  ret
  
}

#' @noRd
load_mask <- function(config) {
  if (config$data_mode == "image") {
    if (!file.exists(config$mask)) {
      flog.error("mask %s does not exist", config$mask)
      stop()
    }
    read_vol(config$mask)
  } else if (config$data_mode == "surface") {
    NULL
  }
}

#' @noRd
load_design <- function(config, name) {
  if (!file.exists(config[[name]])) {
    futile.logger::flog.error(paste("cannot find table named: ", name))
    stop()
  } else {
    read.table(config[[name]], header=TRUE, comment.char=";")
  }
}

#' @noRd
load_mvpa_model <- function(config, dataset, design, crossval, feature_selector) {
  mod <- load_model(config$model)
  mvp_mod <- mvpa_model(mod,dataset, design=design, 
                        model_type=config$model_type,
                        crossval=crossval,
                        feature_selector=feature_selector, 
                        tune_grid=config$tune_grid,
                        performance=config$performance,
                        class_metrics=config$class_metrics)
  
}

#' @keywords internal
#' @importFrom futile.logger flog.error
load_subset <- function(full_design, subset) {
  if (is.character(subset)) {
    if (substr(subset, 1,1) != "~") {
      subset <- paste0("~", subset)
    }   
    subset <- eval(parse(text=subset))
  } 
  
  keep <- if(is.null(subset)) rep(TRUE, nrow(full_design)) else {  
    subexpr <- subset[[2]]   
    keep <- eval(subexpr, full_design)
    if (sum(keep) == nrow(full_design)) {
      warning(paste("subset has same number of rows as full table"))
    }
    if (sum(keep) <= 1) {
      flog.error("train_subset %s results in design with only 1 row.", as.character(subexpr))
      stop()
    }
    keep
  }
  
  keep
  
}


#' @importFrom neuroim2 read_vec read_vol sub_vector
#' @noRd
load_image_data_series <- function(fnames, config, indices, mask_volume) {
  if (!all(file.exists(fnames))) {
    offenders <- fnames[!file.exists(fnames)]
    message(paste("data files", offenders, "not found."))
    stop()
  }
  
  
  ### TODO make more efficient. This loads in all data then subsets.
  #vecmat <- do.call(rbind, lapply(seq_along(fnames), function(i) {
  #  fname <- fnames[i]
  #  flog.info("loading data file %s", fname)
  #  ## TODO does as.matrix do the right thing? must return nonzero subset...
  #  mat <- neuroim2::as.matrix(read_vec(fname, mask=mask_volume))
  #  flog.info("data file %s has %s voxels and %s samples", fname, nrow(mat), ncol(mat))
  #  mat
  #}))
  
  
  ## TODO use indices in read_vec
  vec <- read_vec(fnames, mask=mask_volume)
  
  if (!is.null(indices)) {
    sub_vector(vec, indices)
  }
  
  #SparseNeuroVec(vecmat[indices,], space(mask_volume), mask=mask_volume)
}

#' @noRd
load_image_data <- function(config, name, mask_volume, indices=NULL) {
  fname <- config[[name]]
  if (length(fname) > 1) {
    load_image_data_series(fname, config, indices, mask_volume=mask_volume)
  } else if (!file.exists(fname)) {
    flog.error("datafile %s not found.", fname)
    stop()
  } else {
    flog.info("loading data file %s", fname)
    if (!is.null(indices)) {
      read_vec(fname, indices=indices, mask=mask_volume)
    } else {
      read_vec(fname, mask=mask_volume)
    }
    
  }
}


#' @noRd
load_surface_mask <- function(masklist, train_surf) {
  sections <- names(train_surf)
  assert_that(all(names(sections) == names(masklist)))
  
  masksurfaces <- lapply(sections, function(section) {
    msurf <- neurosurf::read_surf_data(train_surf[[section]]@geometry, masklist[[section]])
    flog.info("mask for %s has %s regions", section, length(unique(msurf@data)))
    msurf
  })
  
  names(masksurfaces) <- sections
  masksurfaces
}

#' @noRd
load_surface_data <- function(config, name, nodeind=NULL, colind=NULL) {
  tdat <- config[[name]]
  sections <- names(tdat)
  
  flog.info("surface sections: ", sections, capture=TRUE)
  
  surfaces <- lapply(sections, function(section) {
    ## TODO check me
    neurosurf::read_surf_data(tdat[[section]]$geometry, tdat[[section]]$data, nodeind=nodeind, colind=colind)
  })
  
  names(surfaces) <- sections
  surfaces
    
}
</file>

<file path="R/dataset.R">
#' Generate Sample Dataset for MVPA Analysis
#' 
#' Creates a synthetic dataset for testing and demonstration of MVPA analyses.
#'
#' @param D The data dimension(s): vector of length 2 or 3 for image data, or single number for surface data
#' @param nobs The number of observations
#' @param response_type Either 'categorical' or 'continuous'
#' @param data_mode Either 'image' or 'surface'
#' @param spacing The voxel spacing (default: c(1,1,1))
#' @param blocks The number of 'blocks' in the data (for cross-validation)
#' @param nlevels The number of category levels (only used if response_type='categorical')
#' @param external_test Whether to generate an external test set
#' @param split_by Optional factor for splitting analyses
#' @param na_cols The number of columns to randomly set to NA (default: 0)
#' @param ntest_obs The number of test observations (default: nobs)
#'
#' @return A list containing:
#'   \describe{
#'     \item{dataset}{An \code{mvpa_dataset} object containing:
#'       \itemize{
#'         \item \code{train_data}: Training data as \code{NeuroVec} or \code{ROISurface}
#'         \item \code{test_data}: Test data (if external_test=TRUE)
#'         \item \code{mask}: Binary mask indicating valid voxels/vertices
#'       }
#'     }
#'     \item{design}{An \code{mvpa_design} object containing:
#'       \itemize{
#'         \item \code{y_train}: Response variable for training
#'         \item \code{y_test}: Response variable for test set (if external_test=TRUE)
#'         \item \code{block_var}: Block variable for cross-validation
#'         \item \code{split_by}: Optional splitting factor
#'       }
#'     }
#'   }
#'
#' @examples
#' # Generate categorical image dataset
#' dataset <- gen_sample_dataset(
#'   D = c(10,10,10),
#'   nobs = 100,
#'   response_type = "categorical",
#'   data_mode = "image",
#'   blocks = 3,
#'   nlevels = 2
#' )
#'
#' # Generate continuous surface dataset
#' surf_data <- gen_sample_dataset(
#'   D = 1000,  # number of vertices
#'   nobs = 50,
#'   response_type = "continuous",
#'   data_mode = "surface"
#' )
#'
#' # Generate dataset with external test set
#' test_dataset <- gen_sample_dataset(
#'   D = c(8,8,8),
#'   nobs = 80,
#'   response_type = "categorical",
#'   nlevels = 3,
#'   external_test = TRUE
#' )
#'
#' @export
gen_sample_dataset <- function(D, nobs, response_type=c("categorical", "continuous"), 
                             data_mode=c("image", "surface"), spacing=c(1,1,1), 
                             blocks=5, nlevels=5, external_test=FALSE, 
                             ntest_obs=nobs, split_by=NULL, na_cols=0) {
 
  response_type <- match.arg(response_type)
  data_mode <- match.arg(data_mode)
  
  # Ensure na_cols is numeric and has a default
  na_cols <- as.numeric(na_cols)
  if (is.null(na_cols) || is.na(na_cols)) {
    na_cols <- 0
  }
  
  if (data_mode == "image") {
    mat <- array(rnorm(prod(D)*nobs), c(D,nobs))
    if (na_cols > 0) {
      naidx <- sample(dim(mat)[4], na_cols)
      for (naid in naidx) {
        ind <- arrayInd(naid, dim(mat)[1:3])
        mat[ind[1], ind[2], ind[3],] <- NA
      }
    } 
    bspace <- neuroim2::NeuroSpace(c(D,nobs), spacing)
    bvec <- neuroim2::NeuroVec(mat, bspace)
    
    mask <- as.logical(neuroim2::NeuroVol(array(rep(0, prod(D)), D), neuroim2::NeuroSpace(D, spacing)))
    roi <- neuroim2::spherical_roi(mask, ceiling((dim(bspace)[1:3])/2), radius=ceiling(min(dim(bspace)/2)))
    mask[coords(roi)] <- 1
    
    if (external_test) {
      mat <- array(rnorm(prod(D)*ntest_obs), c(D,ntest_obs))
      bspace <- neuroim2::NeuroSpace(c(D,ntest_obs), spacing)
      testvec <- neuroim2::NeuroVec(mat, bspace)
      dset <- mvpa_dataset(train_data=bvec, test_data=testvec, mask=mask)
    } else {
      dset <- mvpa_dataset(train_data=bvec,mask=mask)
    }
  } else {
    fname <- system.file("extdata/std.8_lh.inflated.asc", package="neurosurf")
    geom <- neurosurf::read_surf_geometry(fname)
    nvert <- nrow(neurosurf::vertices(geom))
    mat <- matrix(rnorm(nvert*nobs), nvert, nobs)
    bvec <- neurosurf::NeuroSurfaceVector(geom, 1:nvert, mat)
    
    if (external_test) {
      test_data <- neurosurf::NeuroSurfaceVector(geom, 1:nvert, matrix(rnorm(nvert*ntest_obs), nvert, ntest_obs))
      dset <- mvpa_surface_dataset(train_data=bvec, test_data=test_data)
    } else {
      dset <- mvpa_surface_dataset(train_data=bvec)
    }
  }
  
  Y <- if (response_type == "categorical") {
    sample(factor(rep(letters[1:nlevels], length.out=nobs)))
  } else {
    rnorm(nobs)
  }
  
  Ytest <- if (response_type == "categorical") {
    sample(factor(rep(letters[1:nlevels], length.out=ntest_obs)))
  } else {
    rnorm(ntest_obs)
  }
  
  block_var <- as.integer(as.character(cut(1:nobs, blocks, labels=1:blocks)))
  
  if (external_test) {
    message("external test")
    mvdes <- mvpa_design(data.frame(Y=Y, block_var=block_var), test_design=data.frame(Ytest = Ytest), 
                       block_var= "block_var", y_train= ~ Y, y_test = ~ Ytest, split_by=split_by)
  } else {
    mvdes <- mvpa_design(data.frame(Y=Y, block_var=block_var), block_var="block_var", y_train= ~ Y, split_by=split_by)
  }
  
  # Make sure the dataset also has the _has_test_set flag set consistently
  if (is.list(dset) && "dataset" %in% names(dset)) {
    dset$dataset$has_test_set <- external_test
  } else {
    dset$has_test_set <- external_test
  }
  
  list(dataset=dset, design=mvdes)
}


#' Create an MVPA Dataset Object
#'
#' Creates a dataset object for MVPA analysis that encapsulates a training dataset, 
#' an optional test dataset, and a voxel mask.
#'
#' @param train_data The training data set: a \code{NeuroVec} instance
#' @param test_data Optional test data set: a \code{NeuroVec} instance (default: NULL)
#' @param mask The set of voxels to include: a \code{NeuroVol} instance
#'
#' @return An \code{mvpa_dataset} object (S3 class) containing:
#'   \describe{
#'     \item{train_data}{The training data as a \code{NeuroVec} instance}
#'     \item{test_data}{The test data as a \code{NeuroVec} instance (if provided, otherwise NULL)}
#'     \item{mask}{The binary mask defining valid voxels as a \code{NeuroVol} instance}
#'     \item{has_test_set}{Logical flag indicating whether this dataset has a test set}
#'   }
#'
#' @examples
#' # Create dataset from NeuroVec objects
#' train_vec <- NeuroVec(array(rnorm(1000*100), c(10,10,10,100)))
#' mask_vol <- NeuroVol(array(1, c(10,10,10)))
#' dataset <- mvpa_dataset(train_vec, mask=mask_vol)
#'
#' # Create dataset with test data
#' test_vec <- NeuroVec(array(rnorm(1000*20), c(10,10,10,20)))
#' dataset_with_test <- mvpa_dataset(train_vec, test_vec, mask=mask_vol)
#'
#' @seealso 
#' \code{\link{mvpa_surface_dataset}} for creating surface-based MVPA datasets
#' 
#' \code{\link{mvpa_design}} for creating the corresponding design object
#'
#' @importFrom assertthat assert_that
#' @export
mvpa_dataset <- function(train_data, test_data=NULL, mask) {
  assert_that(inherits(train_data, "NeuroVec"))
  if (!is.null(test_data)) {
    assert_that(inherits(test_data, "NeuroVec"))
  }
  assert_that(inherits(mask, "NeuroVol"))
  
  # Check for single-voxel datasets (1,1,1,time)
  mask_dims <- dim(mask)[1:3]
  total_voxels <- prod(mask_dims)
  if (total_voxels <= 1) {
    stop("Invalid dataset: Only 1 voxel detected (dimensions ",
         paste(mask_dims, collapse="×"),
         "). Feature RSA analysis requires multiple voxels.")
  }
  
  # Check for active voxels in mask
  active_voxels <- sum(mask > 0)
  if (active_voxels <= 1) {
    stop("Invalid dataset: Only ", active_voxels, " active voxel(s) in mask. Feature RSA analysis requires multiple active voxels.")
  }
  
  # Store a flag indicating whether this dataset has a test set
  has_test <- !is.null(test_data)
  
  ret <- structure(
    list(
      train_data=train_data,
      test_data=test_data,
      mask=mask,
      has_test_set=has_test  # Add flag for test set presence
    ),
    class=c("mvpa_image_dataset", "mvpa_dataset", "list")
  )
  ret
}


#' Create a Surface-Based MVPA Dataset Object
#'
#' Creates a dataset object for surface-based MVPA analysis that encapsulates a training dataset,
#' an optional test dataset, and a vertex mask.
#'
#' @param train_data The training data set: must inherit from \code{NeuroSurfaceVector}
#' @param test_data Optional test data set: must inherit from \code{NeuroSurfaceVector} (default: NULL)
#' @param mask Optional binary mask for vertices. If NULL, creates mask from training data indices
#' @param name Optional label to identify the dataset (e.g., "lh" or "rh" to indicate hemisphere)
#'
#' @return An \code{mvpa_surface_dataset} object (S3 class) containing:
#'   \describe{
#'     \item{train_data}{The training data as a \code{NeuroSurfaceVector} instance}
#'     \item{test_data}{The test data as a \code{NeuroSurfaceVector} instance (if provided)}
#'     \item{mask}{A numeric vector indicating valid vertices (1) and excluded vertices (0)}
#'     \item{name}{Character string identifier for the dataset}
#'     \item{has_test_set}{Logical flag indicating whether this dataset has a test set}
#'   }
#'
#' @details
#' If no mask is provided, one will be created automatically using the indices from the training data.
#' The mask will be a numeric vector with length equal to the number of nodes in the surface geometry.
#'
#' @examples
#' \dontrun{
#' # Create surface dataset with automatic mask
#' train_surf <- NeuroSurfaceVector(geometry, data)
#' dataset <- mvpa_surface_dataset(train_surf, name="lh")
#'
#' # Create dataset with test data and custom mask
#' test_surf <- NeuroSurfaceVector(geometry, test_data)
#' mask <- numeric(length(nodes(geometry)))
#' mask[roi_indices] <- 1
#' dataset <- mvpa_surface_dataset(train_surf, test_surf, mask, name="rh")
#' }
#'
#' @seealso 
#' \code{\link{mvpa_dataset}} for creating volume-based MVPA datasets
#' 
#' \code{\link{mvpa_design}} for creating the corresponding design object
#'
#' @importFrom assertthat assert_that
#' @export
mvpa_surface_dataset <- function(train_data, test_data=NULL, mask=NULL, name="") {
  
  assert_that(inherits(train_data, "NeuroSurfaceVector"))
  
  if (!is.null(test_data)) {
    assert_that(inherits(test_data, "NeuroSurfaceVector"))
  }
  
  if (is.null(mask)) {
    mask <- numeric(length(nodes(train_data@geometry)))
    mask[indices(train_data)] <- 1
  }
  
  # Store a flag indicating whether this dataset has a test set
  has_test <- !is.null(test_data)
  
  structure(
    list(
      train_data=train_data,
      test_data=test_data,
      mask=mask,
      name=name,
      has_test_set=has_test  # Add flag for test set presence
    ),
    class=c("mvpa_surface_dataset", "mvpa_dataset", "list")
  )
}

#' @export
#' @method print mvpa_dataset
print.mvpa_dataset <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  dim_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ MVPA Dataset ▀▀█"), "\n\n")
  
  # Training data section
  cat(section_style("├─ Training Data"), "\n")
  dims <- dim(x$train_data)
  dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                   " × ", dim_style(dims[length(dims)]), " observations")
  cat(info_style("│  ├─ Dimensions: "), number_style(dim_str), "\n")
  cat(info_style("│  └─ Type: "), class(x$train_data)[1], "\n")
  
  # Test data section
  cat(section_style("├─ Test Data"), "\n")
  if (is.null(x$test_data)) {
    cat(info_style("│  └─ "), crayon::red("None"), "\n")
  } else {
    dims <- dim(x$test_data)
    dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                     " × ", dim_style(dims[length(dims)]), " observations")
    cat(info_style("│  ├─ Dimensions: "), number_style(dim_str), "\n")
    cat(info_style("│  └─ Type: "), class(x$test_data)[1], "\n")
  }
  
  # Mask information
  cat(section_style("└─ Mask Information"), "\n")
  mids <- table(x$mask[x$mask != 0])
  if (length(mids) > 0) {
    midstr <- paste(names(mids), ":", number_style(mids), collapse = ", ")
    cat(info_style("   ├─ Areas: "), midstr, "\n")
  }
  cat(info_style("   └─ Active voxels/vertices: "), 
      number_style(format(sum(x$mask > 0), big.mark=",")), "\n\n")
}

#' @export
#' @method print mvpa_surface_dataset
print.mvpa_surface_dataset <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  dim_style <- crayon::italic$blue
  name_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ Surface MVPA Dataset ▀▀█"), "\n\n")
  
  # Dataset name
  if (nzchar(x$name)) {
    cat(section_style("├─ Name: "), name_style(x$name), "\n")
  }
  
  # Training data section
  cat(section_style("├─ Training Data"), "\n")
  dims <- dim(x$train_data)
  vertices <- length(nodes(geometry(x$train_data)))
  cat(info_style("│  ├─ Vertices: "), number_style(format(vertices, big.mark=",")), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(dims[length(dims)]), "\n")
  cat(info_style("│  └─ Type: "), class(x$train_data)[1], "\n")
  
  # Test data section
  cat(section_style("├─ Test Data"), "\n")
  if (is.null(x$test_data)) {
    cat(info_style("│  └─ "), crayon::red("None"), "\n")
  } else {
    dims <- dim(x$test_data)
    cat(info_style("│  ├─ Observations: "), number_style(dims[length(dims)]), "\n")
    cat(info_style("│  └─ Type: "), class(x$test_data)[1], "\n")
  }
  
  # Mask information
  cat(section_style("└─ Mask Information"), "\n")
  mids <- table(x$mask[x$mask != 0])
  if (length(mids) > 0) {
    midstr <- paste(names(mids), ":", number_style(mids), collapse = ", ")
    cat(info_style("   ├─ Areas: "), midstr, "\n")
  }
  cat(info_style("   └─ Active vertices: "), 
      number_style(format(sum(x$mask > 0), big.mark=",")), "\n\n")
}


#' @export
#' @method get_searchlight mvpa_image_dataset
get_searchlight.mvpa_image_dataset <- function(obj, type=c("standard", "randomized"), radius=8,...) {
  type <- match.arg(type)
  if (type == "standard") {
    neuroim2::searchlight(obj$mask, radius=radius,...)
  } else {
    neuroim2::random_searchlight(obj$mask, radius=radius,...)
  }
}

#' @export
#' @method get_searchlight mvpa_surface_dataset
get_searchlight.mvpa_surface_dataset <- function(obj, type=c("standard", "randomized"), radius=8,...) {
  type <- match.arg(type)
  #browser()
  # Create the iterator once
  slight <- if (type == "standard") {
    neurosurf::SurfaceSearchlight(geometry(obj$train_data), radius, nodeset=which(obj$mask>0), as_deflist=TRUE)
  } else {
    neurosurf::RandomSurfaceSearchlight(geometry(obj$train_data), radius, nodeset=which(obj$mask>0), as_deflist=TRUE)
  }
  
  slight
}



#' @keywords internal
#' @noRd
#' @importFrom neuroim2 NeuroVol
wrap_output.mvpa_dataset <- function(obj, vals, indices=NULL) {
  if (!is.null(indices)) {
    NeuroVol(vals, space(obj$mask), indices=indices)
  } else {
    NeuroVol(vals, space(obj$mask))
  }
}


#' @keywords internal
#' @noRd
#' @importFrom neurosurf nodes geometry NeuroSurface
wrap_output.mvpa_surface_dataset <- function(obj, vals, indices) {
  #browser()
  
  dvals <- numeric(length(nodes(geometry(obj$train_data))))
  
  #if (length(indices) != length(vals)) {
  #  browser()
  #}
  
  dvals[indices] <- vals[indices]
  ## bit of a hack
  ## we fill in with non-zero rather than allow indices to be missing
  #NeuroSurface(geometry=geometry(obj$train_data), indices=indices, data=dvals)
  NeuroSurface(geometry=geometry(obj$train_data), indices=seq_len(length(dvals)), data=dvals)
}

#' @export
has_test_set.mvpa_dataset <- function(obj) {
  # Use the stored flag rather than checking for existence of test_data
  isTRUE(obj$has_test_set)
}
</file>

<file path="R/design.R">
#' @export
nobs.mvpa_design <- function(x) {
  length(x$y_train)
}

#' @export
nresponses.mvpa_design <- function(x) {
  if (is.factor(x$y_train)) {
    length(levels(x$y_train))
  } else if (is.vector(x$y_train)) {
    length(x$y_train)
  } else if (is.matrix(x$y_train)) {
    ncol(x$y_train)
  } else {
    stop()
  }
}

#' @export
has_test_set.mvpa_design <- function(obj) {
  !is.null(obj$y_test) 
}


#' @export
y_train.mvpa_design <- function(obj) obj$y_train


#' @export
y_test.mvpa_design <- function(obj) if (is.null(obj$y_test)) obj$y_train else obj$y_test


#' @export
test_design.mvpa_design <- function(obj) {
  if (is.null(obj$y_test)) obj$train_design else obj$test_design
}



#' @keywords internal
#' @noRd
parse_variable <- function(var, design) {
  ret <- if (purrr::is_formula(var)) {
    vnames <- all.vars(var[[2]])
    ret <- if (length(vnames) > 1) {
      do.call("interaction", c(lapply(vnames, function(vname) as.factor(design[[vname]])), sep=":"))
    } else {
      design[[vnames]]
    }
    
    assertthat::assert_that(!is.null(ret), msg=paste("formula variable", 
                                                     paste(as.character(var), collapse=" "), " not found."))
    ret
    
  } else if (is.character(var) && length(var) == 1) {
    if (is.null(design[[var]])) {
      stop(paste("`design` does not contain variable named: ", var))
    }
    design[[var]]
  } else if (is.factor(var) || is.integer(var)) {
    if (is.data.frame(design)) {
      assertthat::assert_that(nrow(design) == length(var)) 
    }
    var
  } else {
    stop("'var' must be a formula, factor, or character vector")
  }
  
  if (is.factor(ret)) {
    droplevels(ret)
  } else {
    ret
  }
  
}


#' Create an MVPA Design Object
#'
#' Creates a design object for MVPA analysis that encapsulates training and testing designs,
#' response variables, and optional blocking and splitting factors.
#'
#' @param train_design A data frame containing the training design matrix
#' @param test_design Optional data frame containing the test design matrix (default: NULL)
#' @param y_train Formula or vector specifying the training response variable
#' @param y_test Optional formula or vector specifying the test response variable (default: NULL)
#' @param block_var Optional formula or vector specifying the blocking variable for cross-validation
#' @param split_by Optional formula or vector for splitting analyses
#' @param ... Additional arguments (currently unused)
#'
#' @return An \code{mvpa_design} object (S3 class) containing:
#'   \describe{
#'     \item{train_design}{Data frame of training design}
#'     \item{test_design}{Data frame of test design (if provided)}
#'     \item{y_train}{Training response variable}
#'     \item{y_test}{Test response variable (if provided)}
#'     \item{block_var}{Blocking variable for cross-validation (if provided)}
#'     \item{split_by}{Splitting factor (if provided)}
#'   }
#'
#' @details
#' The \code{y_train} and \code{y_test} can be specified either as formulas (e.g., ~ condition) 
#' or as vectors. If formulas are used, they are evaluated within the respective design matrices.
#' 
#' The \code{block_var} and \code{split_by} can also be specified as formulas or vectors. 
#' If formulas, they are evaluated within the training design matrix.
#'
#' @examples
#' # Basic design with only training data
#' train_df <- data.frame(condition = rep(c("A", "B"), each = 50),
#'                        block = rep(1:5, each = 20),
#'                        group = rep(c("Group1", "Group2"), 50))
#' design <- mvpa_design(train_df, y_train = ~ condition)
#'
#' # Design with test data and blocking variable
#' test_df <- data.frame(condition = rep(c("A", "B"), each = 25))
#' design_with_test <- mvpa_design(
#'   train_df, 
#'   test_df, 
#'   y_train = ~ condition, 
#'   y_test = ~ condition,
#'   block_var = ~ block
#' )
#'
#' # Design with split_by factor
#' design_split <- mvpa_design(
#'   train_df, 
#'   y_train = ~ condition,
#'   split_by = ~ group
#' )
#'
#' @seealso 
#' \code{\link{mvpa_dataset}} for creating the corresponding dataset object
#'
#' @importFrom stats as.formula
#' @export
mvpa_design <- function(train_design, test_design=NULL, y_train, y_test=NULL, block_var=NULL, split_by=NULL, ...) {
 
  y_train <- if (!purrr::is_formula(y_train) && length(y_train) > 1) {
    y_train
  } else {
    parse_variable(y_train, train_design)
  }
  
  if (is.factor(y_train) || is.character(y_train)) {
    y_train <- as.factor(y_train)
   
    if (any(table(y_train) == 0)) {
      futile.logger::flog.warn("y_train: ", table(y_train), capture=TRUE)
      futile.logger::flog.warn("y_train factor has at least one level with zero training instances: dropping unused levels.")
      y_train <- droplevels(y_train)
    }
    
    if (length(table(y_train)) <= 1) {
      #futile.logger::flog.error("y_train: ", table(y_train), capture=TRUE)
      stop(paste("error: y_train factor must have at least 2 levels with one or more training instances"))
    }
    
    ytab <- table(levels(y_train))
    
    if (any(ytab == 0)) {
      futile.logger::flog.info("y_train: ", table(y_train), capture=TRUE)
      stop(paste("error: y_train factor must have at least 1 training instance for every factor level"))
    }
  }
  
  if (!is.null(y_test)) {
    
    ## must have a test_design
    assert_that(!is.null(test_design))
    y_test <- if (!purrr::is_formula(y_test) && length(y_test) > 1) {
      y_test
    } else {
      parse_variable(y_test, test_design)
    }
  }
  
  check_split <- function(split_var) {
    minSplits <- min(table(split_var))
    if (minSplits < 3) {
      stop(paste("error: splitting condition results in fewer than 3 observations in at least one set"))
    }
  }
  
  if (!is.null(split_by)) {
    des <- if (!is.null(test_design) && nrow(test_design) > 0) test_design else train_design
    split_var <- parse_variable(split_by, des)
    split_groups <- split(1:nrow(des), split_var)
  } else {
    split_groups=NULL
  }
  
  if (!is.null(block_var)) {
    block_var <- parse_variable(block_var, train_design)
    assertthat::assert_that(!is.null(block_var))
  }
 
  test_design <- if (!is.null(test_design)) {
    tibble::as_tibble(test_design, .name_repair = .name_repair) %>% mutate(.rownum=1:n()) 
  }
  
  train_design <- tibble::as_tibble(train_design,.name_repair = .name_repair) %>% mutate(.rownum=1:n())
  
  des <- list(
    train_design=tibble::as_tibble(train_design, .name_repair = .name_repair),
    y_train=y_train,
    test_design=if (!is.null(test_design)) tibble::as_tibble(test_design, .name_repair = .name_repair) else NULL,
    y_test=y_test,
    split_by=split_by,
    split_groups=split_groups,
    block_var=block_var
  )
  
  class(des) <- c("mvpa_design", "list")
  des
}

#' @export
#' @method print rsa_design
print.rsa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  formula_style <- crayon::italic$green
  var_style <- crayon::blue
  
  # Print header
  cat("\n", header_style("█▀▀ RSA Design ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Formula"), "\n")
  cat(info_style("│  └─ "), formula_style(deparse(x$formula)), "\n")
  
  # Variables section
  cat(section_style("├─ Variables"), "\n")
  cat(info_style("│  └─ "), var_style(paste(names(x$data), collapse=", ")), "\n")
  
  # Block information
  cat(section_style("└─ Structure"), "\n")
  if (!is.null(x$block_var)) {
    blocks <- table(x$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), crayon::green(length(blocks)), "\n")
    cat(info_style("   └─ Block Sizes: "), 
        crayon::green(paste0(names(blocks), ": ", blocks, collapse=", ")), "\n")
  } else {
    cat(info_style("   └─ Blocking: "), crayon::red("None"), "\n")
  }
  cat("\n")
}

#' @export
#' @method print mvpa_design
print.mvpa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  level_style <- crayon::blue
  stat_style <- crayon::italic$white
  
  # Print header
  cat("\n", header_style("█▀▀ MVPA Design ▀▀█"), "\n\n")
  
  # Training section
  cat(section_style("├─ Training Data"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$y_train), big.mark=",")), "\n")
  
  if (is.factor(x$y_train)) {
    cat(info_style("│  ├─ Response Type: "), "Factor\n")
    cat(info_style("│  ├─ Levels: "), level_style(paste(levels(x$y_train), collapse=", ")), "\n")
    level_counts <- table(x$y_train)
    cat(info_style("│  └─ Class Distribution: "), 
        number_style(paste0(names(level_counts), ": ", level_counts, collapse=", ")), "\n")
  } else {
    cat(info_style("│  ├─ Response Type: "), "Numeric\n")
    cat(info_style("│  └─ Range: "), 
        number_style(sprintf("[%.2f, %.2f]", min(x$y_train), max(x$y_train))), "\n")
  }
  
  # Test data section
  cat(section_style("├─ Test Data"), "\n")
  if (!is.null(x$y_test)) {
    cat(info_style("│  ├─ Observations: "), number_style(format(length(x$y_test), big.mark=",")), "\n")
    if (is.factor(x$y_test)) {
      test_counts <- table(x$y_test)
      cat(info_style("│  └─ Class Distribution: "), 
          number_style(paste0(names(test_counts), ": ", test_counts, collapse=", ")), "\n")
    } else {
      cat(info_style("│  └─ Range: "), 
          number_style(sprintf("[%.2f, %.2f]", min(x$y_test), max(x$y_test))), "\n")
    }
  } else {
    cat(info_style("│  └─ "), crayon::red("None"), "\n")
  }
  
  # Structure section
  cat(section_style("└─ Structure"), "\n")
  
  # Block variable info
  if (!is.null(x$block_var)) {
    blocks <- table(x$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), number_style(length(blocks)), "\n")
    cat(info_style("   ├─ Mean Block Size: "), 
        number_style(format(mean(blocks), digits=2)),
        stat_style(" (SD: "),
        number_style(format(sd(blocks), digits=2)),
        stat_style(")"), "\n")
  } else {
    cat(info_style("   ├─ Blocking: "), crayon::red("None"), "\n")
  }
  
  # Split information
  if (!is.null(x$split_by)) {
    split_info <- table(x$split_by)
    cat(info_style("   └─ Split Groups: "), 
        number_style(paste0(names(split_info), ": ", split_info, collapse=", ")), "\n")
  } else {
    cat(info_style("   └─ Split Groups: "), crayon::red("None"), "\n")
  }
  cat("\n")
}
</file>

<file path="R/resample.R">
#' @keywords internal
gen_id <- function(n) {
  width <- nchar(n)
  sprintf(paste0("%0", width, "d"), seq_len(n))
}

#' @keywords internal
.get_samples <- function(obj, voxlist) {
  ret <- lapply(voxlist, function(vox) {
    sam <- data_sample(obj, vox)
  })
  
  n <- length(ret)
  df <- tibble::tibble(sample = ret)
  df[[".id"]] <- gen_id(n)
  df
}

#' @export
get_samples.mvpa_dataset <- function(obj, vox_list) {
  .get_samples(obj, vox_list)
}

#' @export
get_samples.mvpa_surface_dataset <- function(obj, vox_list) {
  .get_samples(obj, vox_list)
}


#' @export
data_sample.mvpa_dataset <- function(obj, vox) {
  structure(
    list(
      #data = obj,
      data=NULL,
      vox=vox
    ),
    class = "data_sample"
  )
}


#' @export
print.data_sample <- function(x, ...) {
  if (is.matrix(x$vox)) {
    cat("data sample with : ", nrow(x$vox), "features")
  } else {
    cat("data sample with : ", length(x$vox), "features")
  }
}

#' @keywords internal
filter_roi.default <- function(roi, ...) {
  stop("Unsupported ROI type")
}

#' @keywords internal
#' @importFrom neuroim2 ROIVec space coords values
filter_roi.ROIVec <- function(roi, ...) {
  # Extract the train data values
  trdat <- values(roi$train_roi)
  
  # Find columns with missing values (NA)
  nas <- apply(trdat, 2, function(v) any(is.na(v)))
  
  # Find columns with non-zero standard deviation
  sdnonzero <- apply(trdat, 2, sd, na.rm=TRUE) > 0
  
  # Determine columns to keep
  keep <- !nas & sdnonzero
  
  # If no valid columns are found, throw an error
  if (sum(keep) == 0) {
    stop("filter_roi: roi has no valid columns")
  }
  
  # If there's no test ROI data, return filtered train ROI data only
  if (is.null(roi$test_roi)) {
    troi <- ROIVec(space(roi$train_roi), 
                   coords(roi$train_roi)[keep,,drop=FALSE], 
                   data=trdat[,keep,drop=FALSE])
    list(train_roi=troi, test_roi=NULL)
  } else {
    # Filter train ROI data
    troi <- ROIVec(space(roi$train_roi), 
                   coords(roi$train_roi)[keep,,drop=FALSE], 
                   data=trdat[,keep,drop=FALSE])
    
    # Filter test ROI data
    tedat <- values(roi$test_roi)
    teroi <- ROIVec(space(roi$test_roi), 
                    coords(roi$test_roi)[keep,,drop=FALSE], 
                    data=tedat[,keep,drop=FALSE])
    
    list(train_roi=troi, test_roi=teroi)
  }
}

#' @keywords internal
#' @importFrom neurosurf ROISurfaceVector geometry nodes
filter_roi.ROISurfaceVector <- function(roi, ...) {
  # Extract the train data values
  trdat <- roi$train_roi@data
  
  # Find columns with missing values (NA)
  nas <- apply(trdat, 2, function(v) any(is.na(v)))
  
  # Find columns with non-zero standard deviation
  sdnonzero <- apply(trdat, 2, sd, na.rm=TRUE) > 0
  
  # Determine columns to keep
  keep <- !nas & sdnonzero
  
  # If no valid columns are found, throw an error
  if (sum(keep) == 0) {
    stop("filter_roi: roi has no valid columns")
  }
  
  # If there's no test ROI data, return filtered train ROI data only
  if (is.null(roi$test_roi)) {
    troi <- ROISurfaceVector(geometry=roi$train_roi@geometry,
                            indices=roi$train_roi@indices[keep],
                            data=trdat[,keep,drop=FALSE])
    list(train_roi=troi, test_roi=NULL)
  } else {
    # Filter train ROI data
    troi <- ROISurfaceVector(geometry=roi$train_roi@geometry,
                            indices=roi$train_roi@indices[keep],
                            data=trdat[,keep,drop=FALSE])
    
    # Filter test ROI data
    tedat <- roi$test_roi@data
    teroi <- ROISurfaceVector(geometry=roi$test_roi@geometry,
                             indices=roi$test_roi@indices[keep],
                             data=tedat[,keep,drop=FALSE])
    
    list(train_roi=troi, test_roi=teroi)
  }
}


#' @keywords internal
#' @noRd
#' @importFrom neuroim2 series_roi
as_roi.data_sample <- function(x, data, ...) {
  
  train_roi <- try(series_roi(data$train_data, x$vox), silent=TRUE)
  
  test_roi <- if (has_test_set(data)) {
    # Also use try with silent=TRUE for test data
    try(series_roi(data$test_data, x$vox), silent=TRUE)
  }
  
  #cds <- if (is.vector(x$vox)) {
  #  cds <- indexToGrid(space(x$data$mask), x$vox)
  #} else {
  #  x$vox
  #}

  if (is.null(test_roi)) {
    list(train_roi=train_roi,
         test_roi=NULL)
  } else {
    list(train_roi=train_roi,
         test_roi=test_roi)
  }
  
  
}

#' @keywords internal
#' @noRd
#' @importFrom neuroim2 space series series_roi
as.data.frame.data_sample <- function(x, data, ...) {
  train_mat <- neuroim2::series(data$train_data, x$vox)
  
  test_mat <- if (has_test_set(data)) {
    neuroim2::series(data$test_data, x$vox)
  }
  
  cds <- if (is.vector(x$vox)) {
    cds <- neuroim2::index_to_grid(space(data$mask), x$vox)
  } else {
    x$vox
  }
  
  if (!is.null(test_mat)) {
    .type <- rep(c("train", "test"), c(nrow(train_mat), nrow(test_mat)))
    ret <- as.data.frame(rbind(train_mat, test_mat))
    ret$.type <- .type
    attr(ret, "vox") <- cds
    ret
  } else {
    .type <- rep(c("train"), nrow(train_mat))
    ret <- as.data.frame(train_mat)
    ret$.type <- .type
    attr(ret, "vox") <- cds
    ret
  }
  
}
</file>

<file path="R/utils.R">
#' Compute Group Means of a Matrix
#'
#' This function calculates the average vector for each level of a grouping variable in a given matrix.
#'
#' @param X A matrix for which group means should be calculated.
#' @param margin An integer specifying the margin to average over. Use 1 for averaging over rows, and 2 for averaging over columns.
#' @param group A grouping variable, either a factor or an integer vector, that defines the groups to calculate the means for.
#' @return A matrix with the same number of rows or columns (depending on the margin) as the input matrix X, and the number of columns or rows corresponding to the number of unique groups in the grouping variable.
#' @examples
#' # Create a random matrix
#' data <- matrix(rnorm(100 * 100), 100, 100)
#'
#' # Define a grouping variable
#' groups <- factor(rep(1:5, each = 20))
#'
#' # Calculate group means for each row
#' row_means <- group_means(data, margin = 1, group = groups)
#'
#' # Calculate group means for each column
#' col_means <- group_means(data, margin = 2, group = groups)
#' @export
group_means <- function(X, margin, group) {
  if (margin == 1) {
    xsum <- rowsum(X, group)
    sweep(xsum, 1, table(group), "/") 
  } else if (margin == 2) {
    xsum <- rowsum(t(X), group)
    t(sweep(xsum, 1, table(group), "/"))
  } else {
    stop("'margin' must be 1 or 2")
  }
}

#' @noRd
spearman_cor <- function(x, y=NULL, use="everything") {
  cor(x,y,use, method="spearman")
}

#' @noRd
kendall_cor <- function(x, y=NULL, use="everything") {
  cor(x,y,use, method="kendall")
}

#' @noRd
zeroVarianceColumns <- function(M) {
  which(apply(M, 2, sd, na.rm=TRUE) == 0)
}

#' @keywords internal
#' @noRd
zeroVarianceColumns2 <- function(M) {
  apply(M, 2, sd, na.rm=TRUE) == 0
}

#' @keywords internal
#' @noRd
na_cols <- function(M) {
  apply(M, 2, function(x) any(is.na(x)))
}

#' @keywords internal
#' @noRd
nonzeroVarianceColumns <- function(M) {
  which(apply(M, 2, sd, na.rm=TRUE) > 0)
}

#' @keywords internal
#' @noRd
nonzeroVarianceColumns2 <- function(M) {
  ret <- apply(M, 2, sd, na.rm=TRUE) > 0
  ret[is.na(ret)] <- FALSE
  ret
}

#' @noRd
removeZeroVarianceColumns <- function(M) {
  hasVariance <- which(apply(M, 2, sd, na.rm=TRUE) != 0)
  if (length(hasVariance) > 0) {
    M[, hasVariance, drop=FALSE]
  } else {
    M
  }
}

## dfferent version
## https://alistaire.rbind.io/blog/coalescing-joins/
#' Coalesce Join Two Data Frames
#'
#' This function performs a specified type of join on two data frames and then coalesces the joined columns based on their common column names.
#'
#' @param x A data frame to be joined.
#' @param y A second data frame to be joined.
#' @param by A character vector of variables to join by. If NULL (the default), the function will use the common column names in 'x' and 'y'.
#' @param suffix A character vector of length 2, specifying the suffixes to be used for making unique the common column names in 'x' and 'y'. The default is c(".x", ".y").
#' @param join A join function to be used for joining the data frames. The default is dplyr::full_join.
#' @param ... Additional arguments passed on to the join function.
#' @return A data frame resulting from the specified join operation and coalesced columns.
#' @keywords internal
coalesce_join2 <- function(x, y, 
                          by = NULL, suffix = c(".x", ".y"), 
                          join = dplyr::full_join, ...) {
  joined <- join(x, y, by = by, suffix = suffix, ...)
  # names of desired output
  cols <- union(names(x), names(y))
  
  to_coalesce <- names(joined)[!names(joined) %in% cols]
  suffix_used <- suffix[ifelse(endsWith(to_coalesce, suffix[1]), 1, 2)]
  # remove suffixes and deduplicate
  to_coalesce <- unique(substr(
    to_coalesce, 
    1, 
    nchar(to_coalesce) - nchar(suffix_used)
  ))
  
  coalesced <- purrr::map_dfc(to_coalesce, ~dplyr::coalesce(
    joined[[paste0(.x, suffix[1])]], 
    joined[[paste0(.x, suffix[2])]]
  ))
  names(coalesced) <- to_coalesce
  
  dplyr::bind_cols(joined, coalesced)[cols]
}


#' @keywords internal
coalesce_join <- function(x, y, 
                          by = NULL, suffix = c(".x", ".y"), 
                          join = dplyr::full_join, ...) {
  joined <- join(x, y, by = by, suffix = suffix, ...)
  # names of desired output
  cols <- union(names(x), names(y))
  
  to_coalesce <- names(joined)[!names(joined) %in% cols]
  
  if (length(to_coalesce) == 0) {
    ## nothing to coalesce...
    return(joined)
  }
  
  suffix_used <- suffix[ifelse(endsWith(to_coalesce, suffix[1]), 1, 2)]
  # remove suffixes and deduplicate
  to_coalesce <- unique(substr(
    to_coalesce, 
    1, 
    nchar(to_coalesce) - nchar(suffix_used)
  ))
  
  coalesced <- purrr::map_dfc(to_coalesce, ~dplyr::coalesce(
    joined[[paste0(.x, suffix[1])]], 
    joined[[paste0(.x, suffix[2])]]
  ))
  names(coalesced) <- to_coalesce
  
  dplyr::bind_cols(joined, coalesced)[cols]
}

#' Report System and Package Information for rMVPA
#'
#' Gathers and displays information about the R session, operating system,
#' rMVPA version, and key dependencies. This information is helpful for
#' debugging, reporting issues, and ensuring reproducibility.
#'
#' @return Invisibly returns a list containing the gathered system and package
#'         information. It is primarily called for its side effect: printing
#'         the formatted information to the console.
#' @export
#' @examples
#' \dontrun{
#' # Display system information in the console
#' mvpa_sysinfo()
#'
#' # Capture the information in a variable
#' sys_info <- mvpa_sysinfo()
#' print(sys_info$r_version)
#' print(sys_info$dependencies$caret)
#' }
mvpa_sysinfo <- function() {
  info <- list()

  # --- R and System Information ---
  r_version <- R.version
  info$r_version <- r_version$string
  info$platform <- r_version$platform
  sys_info <- Sys.info()
  info$os <- paste(sys_info["sysname"], sys_info["release"])
  info$nodename <- sys_info["nodename"]
  info$user <- sys_info["user"]

  # --- rMVPA Version ---
  tryCatch({
    # Use '::' to avoid issues if utils is masked
    info$rmvpa_version <- as.character(utils::packageVersion("rMVPA"))
  }, error = function(e) {
    info$rmvpa_version <- "Error: rMVPA package not found or version unavailable."
  })

  # --- Key Dependency Versions ---
  # Core dependencies and commonly used ones
  deps <- c("neuroim2", "neurosurf", "caret", "future", "furrr",
            "dplyr", "tibble", "purrr", "stats", "MASS") # Added stats/MASS

  dep_versions <- list()
  for (dep in deps) {
    tryCatch({
      # Check if namespace exists before getting version
      if (requireNamespace(dep, quietly = TRUE)) {
        dep_versions[[dep]] <- as.character(utils::packageVersion(dep))
      } else {
        dep_versions[[dep]] <- "Not installed"
      }
    }, error = function(e) {
      # Fallback if packageVersion fails for an installed package
      dep_versions[[dep]] <- "Error retrieving version"
    })
  }
  info$dependencies <- dep_versions

  # --- Parallel Backend (Future plan) ---
  tryCatch({
     # Get the current plan structure without changing it
     current_plan <- future::plan("list")

     # Format the plan description
     # Based on future:::as.character.FutureStrategy and internal structure
     plan_to_string <- function(p) {
        cl <- class(p)[1]
        details <- ""
        if (inherits(p, "multicore") || inherits(p, "cluster")) {
            workers <- tryCatch(length(p$workers), error=function(e) NA)
            if (!is.na(workers)) {
              details <- paste0(" (workers=", workers, ")")
            }
        } else if (inherits(p, "sequential")) {
            # No extra details needed
        }
        paste0(cl, details)
     }

     if (is.list(current_plan)) {
         # Handle plan stack (e.g., plan(list(cluster, sequential)))
         plan_desc <- paste(sapply(current_plan, plan_to_string), collapse = " -> ")
     } else {
         # Handle single plan function
         plan_desc <- plan_to_string(current_plan)
     }

     info$parallel_backend <- plan_desc
   }, error = function(e) {
     info$parallel_backend <- paste("Error retrieving plan:", e$message)
   })


  # --- Locale ---
  tryCatch({
    # Get all locale categories if possible
    info$locale <- Sys.getlocale("LC_ALL")
  }, error = function(e) {
    # Fallback for specific category if LC_ALL fails
    tryCatch({
       info$locale <- Sys.getlocale("LC_CTYPE")
    }, error = function(e2) {
        info$locale <- "Error retrieving locale"
    })
  })

  # Assign class for custom printing
  class(info) <- "mvpa_sysinfo"

  # Print the formatted info and return the list invisibly
  print(info)
  invisible(info)
}


#' Print mvpa_sysinfo Object
#'
#' Formats and prints the system information gathered by \code{mvpa_sysinfo}.
#' This method provides a user-friendly display of the system configuration.
#'
#' @param x An object of class `mvpa_sysinfo`.
#' @param ... Ignored.
#' @export
#' @keywords internal
print.mvpa_sysinfo <- function(x, ...) {
  # Helper for formatting lines
  cat_line <- function(label, value) {
    # Use a standard check instead of custom %||%
    display_value <- ifelse(is.null(value) || length(value) == 0 || value == "", "N/A", value)
    cat(sprintf("%-25s: %s\n", label, display_value))
  }

  # Use crayon for optional coloring if available
  has_crayon <- requireNamespace("crayon", quietly = TRUE)
  header_style <- if (has_crayon) crayon::bold$cyan else function(s) s
  label_style <- if (has_crayon) crayon::bold else function(s) s

  cat(header_style("------------------------- rMVPA System Information -------------------------\n"))

  # Apply the helper function
  cat_line("R Version", x$r_version)
  cat_line("Platform", x$platform)
  cat_line("Operating System", x$os)
  cat_line("Node Name", x$nodename)
  cat_line("User", x$user)
  cat_line("Locale", x$locale)
  cat_line("rMVPA Version", x$rmvpa_version)
  cat_line("Parallel Backend (Future)", x$parallel_backend)

  cat(label_style("\nKey Dependencies:\n"))
  if (!is.null(x$dependencies) && length(x$dependencies) > 0) {
    max_name_len <- max(nchar(names(x$dependencies))) %||% 10 # Provide default width
    for (i in seq_along(x$dependencies)) {
      dep_version <- x$dependencies[[i]]
      dep_display <- ifelse(is.null(dep_version) || length(dep_version) == 0 || dep_version == "", "N/A", dep_version)
      cat(sprintf("  - %-*s: %s\n", max_name_len, names(x$dependencies)[i], dep_display))
    }
  } else {
    cat("  (No dependency versions found or rMVPA not fully loaded)\n")
  }

  cat(header_style("--------------------------------------------------------------------------\n"))
  invisible(x)
}


# Helper for print method (borrowed from purrr, avoids dependency)
# No longer strictly needed by print.mvpa_sysinfo after the changes, but keep for now 
# in case it's used elsewhere or intended for future use.
`%||%` <- function(x, y) {
  if (is.null(x) || length(x) == 0) y else x
}
</file>

<file path="R/rsa_model.R">
#' @noRd
#' @keywords internal
sanitize <- function(name) {
  name <- gsub(":", ".", name)
  name <- gsub(" ", "", name)
  name <- gsub("[\\(\\)]", ".", name, perl=TRUE)
  name <- gsub(",", "_", name)
  name <- gsub("\\.$", "", name)
  name
}


#' Construct a design for an RSA (Representational Similarity Analysis) model
#'
#' This function constructs a design for an RSA model using the provided formula, data, and optional parameters.
#'
#' @param formula A formula expression specifying the dissimilarity-based regression function.
#' @param data A named list containing the dissimilarity matrices and any other auxiliary variables.
#' @param block_var An optional \code{formula}, \code{character} name or \code{integer} vector designating the block structure.
#' @param split_by An optional \code{formula} indicating grouping structure for evaluating test performance.
#' @param keep_intra_run A \code{logical} indicating whether to include within-run comparisons (default: FALSE).
#' @return A list with class attributes "rsa_design" and "list", containing:
#'   \describe{
#'     \item{formula}{The input formula}
#'     \item{data}{The input data}
#'     \item{split_by}{The split_by formula}
#'     \item{split_groups}{Grouping structure for split_by}
#'     \item{block_var}{Block structure}
#'     \item{include}{Logical vector for including/excluding comparisons}
#'     \item{model_mat}{Model matrix generated by rsa_model_mat}
#'   }
#' @details
#' The function creates an RSA design based on the input parameters. It checks the validity of the input data and
#' handles splitting conditions for evaluation of test performance. It also processes optional block structures and
#' within-run comparisons.
#' @importFrom assertthat assert_that
#' @export
#' @examples
#' dismat <- dist(matrix(rnorm(100*100), 100, 100))
#' rdes <- rsa_design(~ dismat, list(dismat=dismat))
rsa_design <- function(formula, data, block_var=NULL, split_by=NULL, keep_intra_run=FALSE) {
  assert_that(purrr::is_formula(formula))
  
  # Check that all variables are either matrices, "dist", or vectors
  nr <- sapply(data, function(x) {
    if (is.matrix(x)) {
      nrow(x)
    } else if (inherits(x, "dist")) {
      attr(x, "Size")
    } else if (is.vector(x)) {
      length(x)
    } else {
      stop(paste("illegal variable type", class(x)))
    }
  })
  
  assert_that(all(nr == nr[1]), msg="all elements in 'data' must have the same number of rows")
  
  check_split <- function(split_var) {
    minSplits <- min(table(split_var))
    if (minSplits < 3) {
      stop(paste("error: splitting condition results in fewer than 3 observations in at least one set"))
    }
  }
  
  # Create split groups if split_by is provided
  split_groups <- if (!is.null(split_by)) {
    split_var <- parse_variable(split_by, data)
    split(seq_along(split_var), split_var)
  }
  
  # Process block_var if provided
  block_var <- if (!is.null(block_var)) {
    parse_variable(block_var, data)
  }
  
  # Include/exclude within-run comparisons based on keep_intra_run
  include <- if (!is.null(block_var) && !keep_intra_run) {
    as.vector(dist(block_var)) != 0
  }
  
  # Create the RSA design as a list
  des <- list(
    formula=formula,
    data=data,
    split_by=split_by,
    split_groups=split_groups,
    block_var=block_var,
    include=include
  )
  
  # Add model matrix to the design list
  mmat <- rsa_model_mat(des)
  des$model_mat <- mmat
  
  # Set the class attributes
  class(des) <- c("rsa_design", "list")
  
  # Return the RSA design
  des
}


#' Construct a model matrix for an RSA (Representational Similarity Analysis) design
#'
#' This function constructs a model matrix for the given RSA design by processing distance matrices and other variables.
#'
#' @param rsa_des An RSA design object created by \code{rsa_design()}, containing formula, data, and optional parameters.
#' @return A named list of vectors, where:
#'   \itemize{
#'     \item Names correspond to sanitized variable names from the formula
#'     \item Each vector is the processed version of the corresponding input data
#'     \item For distance matrices, only the lower triangle is included
#'     \item If rsa_des$include is specified, vectors are subset accordingly
#'   }
#' @details
#' The function takes an RSA design object as input and processes the distance matrices and other variables to
#' construct a model matrix. It handles different types of input matrices, including symmetric and asymmetric
#' distance matrices, and can include or exclude within-run comparisons based on the RSA design.
#' @examples
#' dismat <- dist(matrix(rnorm(100*100), 100, 100))
#' rdes <- rsa_design(~ dismat, list(dismat=dismat))
#' rsa_model_mat(rdes)
#' @keywords internal
#' @noRd
rsa_model_mat <- function(rsa_des) {
  rvars <- labels(terms(rsa_des$formula))
  denv <- list2env(rsa_des$data)
  vset <- lapply(rvars, function(x) eval(parse(text=x), envir=denv))
  
  # Process input variables to create vectors from distance matrices
  vmatlist <- lapply(vset, function(v) {
    if (inherits(v, "dist")) {
      as.vector(v)  # class "dist"
    } else if (isSymmetric(v)) {
      v[lower.tri(v)]
    } else {
      as.vector(dist(v))
    }
  })
  
  # Include or exclude within-run comparisons based on rsa_des$include
  if (!is.null(rsa_des$include)) {
    vmatlist <- lapply(vmatlist, function(v) v[rsa_des$include])
  }
  
  # Assign sanitized names
  names(vmatlist) <- sanitize(rvars)
  
  # Return the model matrix as a named list of vectors
  vmatlist
}


#' @keywords internal
#' @importFrom Rfit rfit
#' @noRd
run_rfit <- function(dvec, obj) {
  form <- paste("dvec", "~", paste(names(obj$design$model_mat), collapse = " + "))
  obj$design$model_mat$dvec <- dvec
  res <- Rfit::rfit(form, data=obj$design$model_mat)
  coef(res)[-1]
}


#' @keywords internal
#' @importFrom stats coef cor dist rnorm terms lm sd
#' @noRd
check_collinearity <- function(model_mat) {
  # Get the design matrix and names
  vnames <- names(model_mat)
  design_matrix <- as.matrix(model_mat)
  
  # Basic check - need at least 2 columns for collinearity
  if (ncol(design_matrix) > 1) {
    # Calculate correlation matrix and check for high correlations
    tryCatch({
      cor_matrix <- cor(design_matrix, use = "pairwise.complete.obs")
      diag(cor_matrix) <- 0
      if (any(abs(cor_matrix) > 0.99, na.rm = TRUE)) {
        high_cor_pairs <- which(abs(cor_matrix) > 0.99, arr.ind = TRUE)
        if (nrow(high_cor_pairs) > 0) {
          problem_vars <- apply(high_cor_pairs, 1, function(idx) {
            paste(vnames[c(idx[1], idx[2])], collapse = " and ")
          })
          stop(sprintf("Collinearity detected among predictors: %s. Consider removing one of the correlated variables.",
                       paste(problem_vars, collapse = "; ")))
        }
      }
      
      # Also check linear dependencies using QR decomposition
      qr_result <- qr(design_matrix)
      if (qr_result$rank < ncol(design_matrix)) {
        stop(sprintf("Design matrix is rank deficient (rank %d < %d columns). Some predictors are linear combinations of others.", 
                     qr_result$rank, ncol(design_matrix)))
      }
    }, error = function(e) {
      if (grepl("NA/NaN/Inf", e$message)) {
        stop("Cannot compute correlations due to NA/NaN/Inf values in predictor variables.")
      } else {
        stop(paste("Error checking collinearity:", e$message))
      }
    })
  }
}


#' @keywords internal
#' @importFrom stats coef cor dist rnorm terms lm sd
#' @noRd
run_lm <- function(dvec, obj) {
  # This is the standard LM approach that returns T-values for each predictor.
  form <- paste("dvec", "~", paste(names(obj$design$model_mat), collapse = " + "))
  vnames <- names(obj$design$model_mat)
  obj$design$model_mat$dvec <- dvec
  
  fit <- lm(form, data=obj$design$model_mat)
  # Return T-values (3rd column of coef summary)
  tvals <- coef(summary(fit))[-1, 3]
  names(tvals) <- vnames
  tvals
}


#' @keywords internal
#' @noRd
run_cor <- function(dvec, obj) {
  # For 'pearson' or 'spearman' regtype, we just do correlation with each predictor
  res <- sapply(obj$design$model_mat, function(x) cor(dvec, x, method=obj$distmethod))
  names(res) <- names(obj$design$model_mat)
  res
}


################################################################################
# NEW: Constrained LM with glmnet
################################################################################
#' @keywords internal
#' @importFrom glmnet glmnet
#' @noRd
run_lm_constrained <- function(dvec, obj) {
  # Check if glmnet is available and install if needed
  if (!requireNamespace("glmnet", quietly = TRUE)) {
    stop("Package 'glmnet' is required for non-negative constraints. Please install it with: install.packages('glmnet')")
  }
  
  # Convert list of vectors (predictors) into a matrix
  var_names <- names(obj$design$model_mat)
  X <- do.call(cbind, obj$design$model_mat)  # columns = predictors
  colnames(X) <- var_names
  
  # Identify which predictors should be constrained to be >= 0
  if (is.null(obj$nneg)) {
    stop("run_lm_constrained called, but obj$nneg is NULL.")
  }
  
  nneg_names <- names(obj$nneg)
  lower_lim_vec <- sapply(var_names, function(vn) {
    if (vn %in% nneg_names) 0 else -Inf
  })
  
  # Create penalty_factor vector - add a small penalty to non-negative terms
  # and zero penalty to unconstrained terms to avoid the "all penalty factors <= 0" error
  penalty_factor <- sapply(var_names, function(vn) {
    if (vn %in% nneg_names) 1e-5 else 0
  })
  
  # Use a small alpha (mixing parameter) to ensure stability
  alpha_val <- 1e-5
  
  # Fit glmnet with near-zero lambda and minimal penalty on non-negative variables
  fit <- tryCatch({
    glmnet::glmnet(
      x = X,
      y = dvec,
      alpha = alpha_val,                       
      lambda = 1e-5,                  
      penalty.factor = penalty_factor,
      lower.limits = lower_lim_vec,
      standardize = FALSE,
      intercept = TRUE
    )
  }, error = function(e) {
    # If error still occurs, try a different approach with all penalty factors = 1
    warning("First glmnet attempt failed, trying with uniform penalties: ", e$message)
    glmnet::glmnet(
      x = X,
      y = dvec,
      alpha = alpha_val,
      lambda = 1e-5,
      lower.limits = lower_lim_vec,
      standardize = FALSE,
      intercept = TRUE
    )
  })
  
  # Extract coefficients at smallest lambda
  lambda_min <- min(fit$lambda)
  
  # Properly extract coefficients - coef is S3 method
  coef_matrix <- predict(fit, type = "coefficients", s = lambda_min)
  # First row is intercept, we want rows 2:end
  betas <- as.numeric(coef_matrix)[-1]
  names(betas) <- var_names
  
  betas
}


################################################################################
# NEW: Semi-Partial LM
################################################################################
#' @keywords internal
#' @noRd
run_lm_semipartial <- function(dvec, obj) {
  # Compute semi-partial correlations from a single LM fit
  # sr_i = sign(t_i) * sqrt( (t_i^2 * MSE) / TSS )
  
  form <- paste("dvec", "~", paste(names(obj$design$model_mat), collapse = " + "))
  vnames <- names(obj$design$model_mat)
  obj$design$model_mat$dvec <- dvec
  
  fit  <- lm(form, data=obj$design$model_mat)
  smry <- summary(fit)
  
  # t-values (excluding intercept)
  tvals <- coef(smry)[-1, 3]
  
  # Residual MSE
  MSE <- smry$sigma^2
  
  # TSS = total sum of squares of the outcome
  y_mod <- fit$model$dvec
  TSS   <- sum((y_mod - mean(y_mod))^2)
  
  # Compute sr_i
  sr2 <- (tvals^2 * MSE) / TSS
  sr  <- sign(tvals) * sqrt(sr2)
  
  names(sr) <- vnames
  sr
}


################################################################################
# TRAINING METHOD
################################################################################
#' Train an RSA Model
#'
#' This function trains an RSA (representational similarity analysis) model using the specified method and distance calculation.
#'
#' @param obj An object of class \code{rsa_model}.
#' @param train_dat The training data.
#' @param y The response variable.
#' @param indices The indices of the training data.
#' @param ... Additional arguments passed to the training method.
#' @return 
#' Depending on \code{obj$regtype}:
#' \itemize{
#'   \item \code{"lm"} + no constraints + \code{obj$semipartial=TRUE}: semi-partial correlations
#'   \item \code{"lm"} + no constraints + \code{obj$semipartial=FALSE}: T-values of each predictor
#'   \item \code{"lm"} + \code{nneg} constraints: raw coefficients from constrained \code{glmnet}
#'   \item \code{"rfit"}: robust regression coefficients
#'   \item \code{"pearson"} or \code{"spearman"}: correlation coefficients
#' }
#' @export
train_model.rsa_model <- function(obj, train_dat, y, indices, ...) {
  # 1) correlation-based distance
  dtrain <- 1 - cor(t(train_dat), method=obj$distmethod)
  dvec   <- dtrain[lower.tri(dtrain)]
  
  # 2) Exclude certain comparisons if needed
  if (!is.null(obj$design$include)) {
    dvec <- dvec[obj$design$include]
  }
  
  # 3) Switch on regtype + constraints + semipartial
  out <- switch(
    obj$regtype,
    
    # robust regression
    rfit = run_rfit(dvec, obj),
    
    # linear model
    lm = {
      has_nneg <- (!is.null(obj$nneg) && length(obj$nneg) > 0)
      
      if (has_nneg) {
        # Use constrained approach
        run_lm_constrained(dvec, obj)
      } else if (isTRUE(obj$semipartial)) {
        # Semi-partial correlations
        run_lm_semipartial(dvec, obj)
      } else {
        # Standard approach = t-values
        run_lm(dvec, obj)
      }
    },
    
    # correlation-based
    pearson  = run_cor(dvec, obj),
    spearman = run_cor(dvec, obj)
  )
  
  out
}


################################################################################
# PRINT METHODS
################################################################################
#' @export
#' @method print rsa_model
print.rsa_model <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Color scheme
  header_style  <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style    <- crayon::white
  number_style  <- crayon::green
  method_style  <- crayon::magenta
  formula_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ RSA Model ▀▀█"), "\n\n")
  
  # Model configuration
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Distance Method: "), method_style(x$distmethod), "\n")
  cat(info_style("│  └─ Regression Type: "), method_style(x$regtype), "\n")
  
  # If nonneg constraints are present
  if (!is.null(x$nneg) && length(x$nneg) > 0) {
    cat(info_style("│  └─ Non-negativity on: "),
        method_style(paste(names(x$nneg), collapse=", ")), "\n")
  }
  
  # If semipartial
  if (isTRUE(x$semipartial) && (is.null(x$nneg) || length(x$nneg) == 0)) {
    cat(info_style("│  └─ Semi-partial: "), method_style("TRUE"), "\n")
  }
  
  # Dataset info
  cat(section_style("├─ Dataset"), "\n")
  dims <- dim(x$dataset$train_data)
  dim_str <- paste0(
    paste(dims[-length(dims)], collapse=" × "), 
    " × ", number_style(dims[length(dims)]), " observations"
  )
  cat(info_style("│  ├─ Dimensions: "), dim_str, "\n")
  cat(info_style("│  └─ Type: "), class(x$dataset$train_data)[1], "\n")
  
  # Design info
  cat(section_style("├─ Design"), "\n")
  cat(info_style("│  ├─ Formula: "), formula_style(deparse(x$design$formula)), "\n")
  
  var_names <- names(x$design$model_mat)
  cat(info_style("│  └─ Predictors: "), method_style(paste(var_names, collapse=", ")), "\n")
  
  # Structure info
  cat(section_style("└─ Structure"), "\n")
  
  # Block info
  if (!is.null(x$design$block_var)) {
    blocks <- table(x$design$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), number_style(length(blocks)), "\n")
    cat(info_style("   ├─ Mean Block Size: "), 
        number_style(format(mean(blocks), digits=2)),
        crayon::italic$white(" (SD: "),
        number_style(format(sd(blocks), digits=2)),
        crayon::italic$white(")"), "\n")
  } else {
    cat(info_style("   ├─ Blocking: "), crayon::red("None"), "\n")
  }
  
  # Split info
  if (!is.null(x$design$split_by)) {
    split_info <- length(x$design$split_groups)
    cat(info_style("   └─ Split Groups: "), number_style(split_info), "\n")
  } else {
    cat(info_style("   └─ Split Groups: "), crayon::red("None"), "\n")
  }
  
  cat("\n")
}


#' @export
#' @method print rsa_design
print.rsa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Color scheme
  header_style  <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style    <- crayon::white
  number_style  <- crayon::green
  formula_style <- crayon::italic$blue
  var_style     <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ RSA Design ▀▀█"), "\n\n")
  
  # Formula
  cat(section_style("├─ Formula"), "\n")
  cat(info_style("│  └─ "), formula_style(deparse(x$formula)), "\n")
  
  # Variables
  cat(section_style("├─ Variables"), "\n")
  var_types <- sapply(x$data, function(v) {
    if (inherits(v, "dist")) "distance matrix"
    else if (is.matrix(v)) "matrix"
    else if (is.vector(v)) "vector"
    else "other"
  })
  
  cat(info_style("│  ├─ Total Variables: "), number_style(length(x$data)), "\n")
  for (i in seq_along(x$data)) {
    prefix <- if (i == length(x$data)) "└" else "├"
    cat(info_style(sprintf("│  %s─ ", prefix)), 
        var_style(names(x$data)[i]), ": ", 
        number_style(var_types[i]), "\n")
  }
  
  # Structure
  cat(section_style("└─ Structure"), "\n")
  
  # Block info
  if (!is.null(x$block_var)) {
    blocks <- table(x$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), number_style(length(blocks)), "\n")
    cat(info_style("   ├─ Block Sizes: "), 
        number_style(paste0(names(blocks), ": ", blocks, collapse=", ")), "\n")
  } else {
    cat(info_style("   ├─ Blocking: "), crayon::red("None"), "\n")
  }
  
  # Include/exclude info
  if (!is.null(x$include)) {
    n_comparisons <- length(x$include)
    n_included <- sum(x$include)
    cat(info_style("   └─ Comparisons: "), 
        number_style(n_included), 
        crayon::italic$white(" of "), 
        number_style(n_comparisons), 
        crayon::italic$white(sprintf(" (%.1f%%)", 100*n_included/n_comparisons)), "\n")
  } else {
    cat(info_style("   └─ Comparisons: "), "All included\n")
  }
  
  cat("\n")
}


#' Construct an RSA (Representational Similarity Analysis) model
#'
#' This function creates an RSA model object by taking an MVPA (Multi-Variate Pattern Analysis) dataset and an RSA design.
#'
#' @param dataset An instance of an \code{mvpa_dataset}.
#' @param design An instance of an \code{rsa_design} created by \code{rsa_design()}.
#' @param distmethod A character string specifying the method used to compute distances between observations. 
#'        One of: \code{"pearson"} or \code{"spearman"} (defaults to "spearman").
#' @param regtype A character string specifying the analysis method. 
#'        One of: \code{"pearson"}, \code{"spearman"}, \code{"lm"}, or \code{"rfit"} (defaults to "pearson").
#' @param check_collinearity Logical indicating whether to check for collinearity in the design matrix. 
#'        Only applies when \code{regtype="lm"}. Default is TRUE.
#' @param nneg A named list of variables (predictors) for which non-negative regression coefficients should be enforced 
#'        (only if \code{regtype="lm"}). Defaults to \code{NULL} (no constraints).
#' @param semipartial Logical indicating whether to compute semi-partial correlations in the \code{"lm"} case 
#'        (only if \code{nneg} is not used). Defaults to \code{FALSE}.
#'
#' @return An object of class \code{"rsa_model"} (and \code{"list"}), containing:
#' \itemize{
#'   \item \code{dataset}    : the input dataset
#'   \item \code{design}     : the RSA design
#'   \item \code{distmethod} : the distance method used
#'   \item \code{regtype}    : the regression type
#'   \item \code{nneg}       : a named list of constrained variables, if any
#'   \item \code{semipartial}: whether to compute semi-partial correlations
#' }
#' @examples
#' # Create a random MVPA dataset
#' data <- matrix(rnorm(100 * 100), 100, 100)
#' labels <- factor(rep(1:2, each = 50))
#' mvpa_data <- mvpa_dataset(data, labels)
#'
#' # Create an RSA design with two distance matrices
#' dismat1 <- dist(data)
#' dismat2 <- dist(matrix(rnorm(100*100), 100, 100))
#' rdes <- rsa_design(~ dismat1 + dismat2, list(dismat1=dismat1, dismat2=dismat2))
#'
#' # Create an RSA model with standard 'lm' (returns t-values):
#' rsa_mod <- rsa_model(mvpa_data, rdes, regtype="lm")
#'
#' # Create an RSA model enforcing non-negativity for dismat2 only:
#' # Requires the 'glmnet' package to be installed
#' # rsa_mod_nneg <- rsa_model(mvpa_data, rdes, regtype="lm",
#' #                          nneg = list(dismat2 = TRUE))
#'
#' # Create an RSA model using 'lm' but returning semi-partial correlations:
#' rsa_mod_sp <- rsa_model(mvpa_data, rdes, regtype="lm",
#'                         semipartial = TRUE)
#'
#' # Train the model
#' fit_params <- train_model(rsa_mod_sp, mvpa_data$train_data)
#' # 'fit_params' = named vector of semi-partial correlations for each predictor
#'
#' @export
rsa_model <- function(dataset, 
                      design, 
                      distmethod = "spearman", 
                      regtype = "pearson", 
                      check_collinearity = TRUE,
                      nneg = NULL,
                      semipartial = FALSE) {
  
  assert_that(inherits(dataset, "mvpa_dataset"))
  assert_that(inherits(design, "rsa_design"))
  
  distmethod <- match.arg(distmethod, c("pearson", "spearman"))
  regtype    <- match.arg(regtype, c("pearson", "spearman", "lm", "rfit"))
  
  # Check for glmnet if nneg constraints are provided
  if (!is.null(nneg) && length(nneg) > 0 && regtype == "lm") {
    if (!requireNamespace("glmnet", quietly = TRUE)) {
      stop("Package 'glmnet' is required for non-negative constraints. Please install it with: install.packages('glmnet')")
    }
  }
  
  # If using LM, optionally check for collinearity
  if (regtype == "lm" && check_collinearity) {
    message("Checking design matrix for collinearity...")
    check_collinearity(design$model_mat)
    message("Collinearity check passed.")
  }
  
  # Create the RSA model object
  obj <- create_model_spec(
    "rsa_model", 
    dataset, 
    design, 
    distmethod = distmethod, 
    regtype    = regtype,
    nneg       = nneg,
    semipartial = semipartial
  )
  obj
}
</file>

<file path="tests/testthat/test_vector_rsa_searchlight.R">
test_that("vector_rsa runs without error and produces valid outputs", {
  # Generate a sample dataset with 100 rows, 3 blocks, and a (5,5,5) volume structure
  # Assuming a helper function gen_sample_dataset() that creates suitable data
  dataset <- gen_sample_dataset(c(5,5,5), 15, blocks=3)
  
  # Create a reference distance matrix from random noise, dimensions should match dataset
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- rep(paste0("Label", 1:15), length.out=15)
  row.names(D) <- labels
  colnames(D) <- labels
 
  block <- dataset$design$block_var
  
  # Create vector_rsa_design and model
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(labels), replace=TRUE), block)
  mspec <- vector_rsa_model(dataset$dataset, rdes, distfun=cordist())
  
  out <- run_searchlight(mspec, radius=4, method="standard")
  expect_true(inherits(out[[1]][[1]]$data, "DenseNeuroVol"))
  # Set up parallel processing capabilities
  
})

test_that("vector_rsa runs with mahalanobis distance without error and produces valid outputs", {
  # Generate a sample dataset with 100 rows, 3 blocks, and a (5,5,5) volume structure
  # Assuming a helper function gen_sample_dataset() that creates suitable data
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix from random noise, dimensions should match dataset
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- rep(paste0("Label", 1:15), length.out=15)
  row.names(D) <- labels
  colnames(D) <- labels
  
  block <- dataset$design$block_var
  
  # Create vector_rsa_design and model
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  mspec <- vector_rsa_model(dataset$dataset, rdes, distfun=mahadist())
  
  out <- run_searchlight(mspec, radius=4, method="standard")
  expect_true(inherits(out$results[[1]]$data, "DenseNeuroVol"))
  # Set up parallel processing capabilities
  
})


test_that("vector_rsa runs with pca distance without error and produces valid outputs", {
  # Generate a sample dataset with 100 rows, 3 blocks, and a (5,5,5) volume structure
  # Assuming a helper function gen_sample_dataset() that creates suitable data
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  
  # Create a reference distance matrix from random noise, dimensions should match dataset
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- rep(paste0("Label", 1:15), length.out=15)
  row.names(D) <- labels
  colnames(D) <- labels
  
  block <- dataset$design$block_var
  
  # Create vector_rsa_design and model
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  threshfun <- function(x) {print(sum(x > 1)); sum(x>1)}
  distfun <- pcadist(labels=NULL, ncomp=3, whiten=FALSE, threshfun=threshfun, dist_method="cosine")
  mspec <- vector_rsa_model(dataset$dataset, rdes, distfun=distfun)
  
  out <- run_searchlight(mspec, radius=4, method="standard")
  expect_true(inherits(out$results[[1]]$data, "DenseNeuroVol"))
  # Set up parallel processing capabilities
  
})


test_that("vector_rsa_design errors when labels are missing from row.names of D", {
  # Create a distance matrix with proper dimensions
  D <- as.matrix(dist(matrix(rnorm(100), 10, 10)))
  labels <- paste0("Label", 1:10)
  # Purposely set one rowname to a wrong value
  rownames(D)[1] <- "NotALabel"
  expect_error(
    vector_rsa_design(D = D, labels = labels, block_var = rep(1, 10)),
    "All labels must be present"
  )
})

test_that("vector_rsa_design errors when length of labels and block_var do not match", {
  D <- as.matrix(dist(matrix(rnorm(100), 10, 10)))
  labels <- paste0("Label", 1:10)
  rownames(D) <- labels
  colnames(D) <- labels
  # Provide a block variable of wrong length
  expect_error(
    vector_rsa_design(D = D, labels = labels, block_var = rep(1, 9)),
    "Length of labels and block_var must match"
  )
})

test_that("vector_rsa_design constructs a valid design object", {
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  block <- rep(1:3, length.out = 15)
  
  design <- vector_rsa_design(D = D, labels = labels, block_var = block)
  expect_true(is.list(design))
  expect_true("vector_rsa_design" %in% class(design))
  expect_true(!is.null(design$model_mat))
  # Check that the expanded matrix has dimensions matching labels
  expect_equal(dim(design$model_mat$Dexpanded), c(15,15))
  # Check that cross-block data is computed
  expect_true(is.list(design$model_mat$cross_block_data))
})

## --- Tests for vector_rsa_model ---

test_that("vector_rsa_model errors when design is not a vector_rsa_design", {
  fake_design <- list(a = 1)
  class(fake_design) <- "list"
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks = 3)$dataset
  expect_error(
    vector_rsa_model(dataset, fake_design),
    "Input must be a 'vector_rsa_design'"
  )
})

test_that("vector_rsa_model constructs a valid model spec", {
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  block <- gen_sample_dataset(c(5,5,5), 100, blocks = 3)$design$block_var
  # Use sampled labels from the block (to match length)
  rdes <- vector_rsa_design(D = D, labels = sample(labels, length(block), replace = TRUE), block_var = block)
  mspec <- vector_rsa_model(gen_sample_dataset(c(5,5,5), 100, blocks = 3)$dataset, rdes, distfun = cordist())
  
  expect_true(inherits(mspec, "vector_rsa_model"))
  # Check that the model spec includes permutation parameters
  expect_equal(mspec$nperm, 50)
  expect_equal(mspec$save_distributions, FALSE)
  
  # Check that other required components are present
  expect_true(!is.null(mspec$distfun))
  expect_true(!is.null(mspec$rsa_simfun))
  expect_true(!is.null(mspec$dataset))
  expect_true(!is.null(mspec$design))
})

## --- Tests for train_model.vector_rsa_model ---
## For these tests we override the second_order_similarity function to simulate output.

test_that("train_model.vector_rsa_model returns expected scores", {
  # Override second_order_similarity with a dummy function
  dummy_second_order <- function(distfun, X, Dref, block_var, method) {
    # For testing, return the mean of X (ignoring NA) as a named scalar.
    # Note: When used across many voxels, the names might not be preserved.
    setNames(mean(X, na.rm = TRUE), "score")
  }
  old_fun <- get("second_order_similarity", envir = .GlobalEnv)
  assign("second_order_similarity", dummy_second_order, envir = .GlobalEnv)
  on.exit(assign("second_order_similarity", old_fun, envir = .GlobalEnv))
  
  # Generate a sample dataset, design, and model spec
  dataset_obj <- gen_sample_dataset(c(5,5,5), 100, blocks = 3)
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  block <- dataset_obj$design$block_var
  
  # Use sampled labels to match length(block)
  rdes <- vector_rsa_design(D = D, labels = sample(labels, length(block), replace = TRUE), block_var = block)
  mspec <- vector_rsa_model(dataset_obj$dataset, rdes, distfun = cordist())
  
  # Compute trial scores via the training function
  scores <- train_model.vector_rsa_model(mspec, as.matrix(dataset_obj$dataset$train_data), y = NULL, indices = NULL)
  
  # Check that the output is numeric and has a positive length
  testthat::expect_true(is.numeric(scores))
  testthat::expect_true(length(scores) > 0)
})

## --- Tests for printing methods ---

test_that("print.vector_rsa_design produces output", {
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  block <- rep(1:3, length.out = 15)
  rdes <- vector_rsa_design(D = D, labels = sample(labels, length(block), replace = TRUE), block_var = block)
  out <- capture.output(print(rdes))
  expect_true(length(out) > 0)
})

test_that("print.vector_rsa_model produces output", {
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  block <- gen_sample_dataset(c(5,5,5), 100, blocks = 3)$design$block_var
  rdes <- vector_rsa_design(D = D, labels = sample(labels, length(block), replace = TRUE), block_var = block)
  mspec <- vector_rsa_model(gen_sample_dataset(c(5,5,5), 100, blocks = 3)$dataset, rdes, distfun = cordist())
  out <- capture.output(print(mspec))
  expect_true(length(out) > 0)
})

## --- Tests for run_searchlight.vector_rsa ---
test_that("vector_rsa searchlight runs without error with different distance functions", {
  dataset_obj <- gen_sample_dataset(c(5,5,5), 100, blocks = 3)
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  block <- dataset_obj$design$block_var
  
  # Test with default cordist
  rdes <- vector_rsa_design(D = D, labels = sample(labels, length(block), replace = TRUE), block_var = block)
  mspec <- vector_rsa_model(dataset_obj$dataset, rdes, distfun = cordist())
  out1 <- run_searchlight(mspec, radius = 4, method = "standard")
  expect_true(inherits(out1$results[[1]]$data, "DenseNeuroVol"))
  
  # Test with mahalanobis distance
  mspec <- vector_rsa_model(dataset_obj$dataset, rdes, distfun = mahadist())
  out2 <- run_searchlight(mspec, radius = 4, method = "standard")
  expect_true(inherits(out2$results[[1]]$data, "DenseNeuroVol"))
  
  # Test with a PCA-based distance
  threshfun <- function(x) { sum(x > 1) }
  distfun_pca <- pcadist(labels = NULL, ncomp = 3, whiten = FALSE, threshfun = threshfun, dist_method = "cosine")
  mspec <- vector_rsa_model(dataset_obj$dataset, rdes, distfun = distfun_pca)
  out3 <- run_searchlight(mspec, radius = 4, method = "standard")
  expect_true(inherits(out3$results[[1]]$data, "DenseNeuroVol"))
})

## --- Tests for permutation testing ---
test_that("vector_rsa runs with permutation testing and produces valid statistical outputs", {
  # Generate sample dataset
  dataset <- gen_sample_dataset(c(5,5,5), 15, blocks=3)
  
  # Create a reference distance matrix
  D <- as.matrix(dist(matrix(rnorm(15*15), 15, 15)))
  labels <- paste0("Label", 1:15)
  rownames(D) <- labels
  colnames(D) <- labels
  
  block <- dataset$design$block_var
  
  # Create vector_rsa_design
  rdes <- vector_rsa_design(D=D, labels=sample(labels, length(block), replace=TRUE), block)
  
  # Create vector_rsa_model with permutation testing enabled
  mspec <- vector_rsa_model(
    dataset$dataset, 
    rdes, 
    distfun=cordist(),
    nperm=50,  # Run 50 permutations
    save_distributions=FALSE  # Don't save full distributions for efficiency
  )
  
  # Run a small searchlight to test permutation
  out <- run_searchlight(mspec, radius=3, method="standard")
  
  # Test that output contains permutation results
  # First get the performance data
  perf_data <- out$results[[1]]$data
  first_nonzero <- which(perf_data@.Data > 0)[1]
  
  # Extract the performance value at the first nonzero location
  if (!is.na(first_nonzero)) {
    # Check if metadata contains permutation results
    meta <- attr(perf_data, "meta")
    
    # Look for p-values and z-scores in column names
    col_names <- colnames(meta$performance_names)
    
    # Verify permutation results exist
    expect_true(any(grepl("^p_", col_names)), "Permutation p-values should exist in results")
    expect_true(any(grepl("^z_", col_names)), "Permutation z-scores should exist in results")
    
    # Also check that there's at least one valid p-value between 0 and 1
    p_cols <- meta$performance_names[, grepl("^p_", col_names), drop=FALSE]
    if (ncol(p_cols) > 0) {
      p_vals <- p_cols[first_nonzero, ]
      expect_true(all(p_vals >= 0 & p_vals <= 1), "P-values should be between 0 and 1")
    }
  }
})

# Setup: Generate sample data and design
set.seed(123)
dset_info <- gen_sample_dataset(c(10,10,10), 60, blocks=3)
vdes <- vector_rsa_design(as.matrix(dist(rnorm(20*10))), 
                          labels=rep(paste0("s", 1:20), 3),
                          block_var=dset_info$design$block_var)

test_that("vector_rsa_model constructs a valid model spec", {
  # No permutation args here
  mspec <- vector_rsa_model(dset_info$dataset, vdes)
  
  expect_s3_class(mspec, "vector_rsa_model")
  expect_true(inherits(mspec, "model_spec"))
  expect_true(!is.null(mspec$dataset))
  expect_true(!is.null(mspec$design))
  expect_true(!is.null(mspec$distfun))
  expect_true(!is.null(mspec$rsa_simfun))
  # REMOVED: These are not stored in the model spec
  # expect_null(mspec$nperm)
  # expect_false(mspec$save_distributions)
})






# Potential additional tests:
# - Different rsa_simfun ('spearman')
# - Errors with invalid radius or mask
</file>

<file path="R/model_fit.R">
#' @keywords internal
#' @noRd
requireNamespaceQuietStop <- function(package) {
  if (!requireNamespace(package, quietly = TRUE))
    stop(paste('package',package,'is required'), call. = FALSE)
}

#' @keywords internal
#' @noRd
mclass_summary <- function (data, lev = NULL, model = NULL) {
  if (!all(levels(data[, "pred"]) == levels(data[, "obs"]))) 
    stop("levels of observed and predicted data do not match")
  has_class_probs <- all(lev %in% colnames(data))
  
  if (has_class_probs) {
    requireNamespaceQuietStop("ModelMetrics")
    prob_stats <- lapply(levels(data[, "pred"]), function(x) {
      obs <- ifelse(data[, "obs"] == x, 1, 0)
      prob <- data[, x]
      AUCs <- try(ModelMetrics::auc(obs, data[, x]), silent = TRUE)
      return(AUCs)
    })
    roc <- mean(unlist(prob_stats))
  } else {
    stop("Cannot compute AUC. Class probabilities unavailable for model: ", model)
  }
  
  c(AUC=roc)
}



#' @keywords internal
#' @noRd
load_caret_libs <- function(x) {
  for (lib in x$model$library) {
    library(lib, character.only = TRUE)
  }
}


#' @keywords internal
#' @noRd
get_control <- function(y, nreps) {
  if (is.factor(y) && length(levels(y)) == 2) {
    ctrl <- caret::trainControl("boot", number=nreps, verboseIter=TRUE, classProbs=TRUE, returnData=FALSE, returnResamp="none",allowParallel=FALSE, trim=TRUE, summaryFunction=caret::twoClassSummary)
    metric <- "ROC"
  } else if (is.factor(y) && length(levels(y)) > 2) {
    ctrl <- caret::trainControl("boot", number=nreps, verboseIter=TRUE, classProbs=TRUE, returnData=FALSE, returnResamp="none",allowParallel=FALSE, trim=TRUE, summaryFunction=mclass_summary)
    metric <- "AUC"
  } else {
    ctrl <- caret::trainControl("boot", number=nreps, verboseIter=TRUE, returnData=FALSE, returnResamp="none",allowParallel=FALSE, trim=TRUE)
    metric = "RMSE"
  }
  
  list(ctrl=ctrl, metric=metric)
}


#'
#' This function finds the best hyperparameters for a given model specification
#' using a specified tuning grid and cross-validation.
#'
#' @param mspec A model specification derived from the \code{mvpa_model} class.
#' @param x The training data matrix.
#' @param y The response vector.
#' @param wts Optional class weights (if the underlying model supports it).
#' @param param A \code{data.frame} representing the tuning grid, where
#'        parameter names are indicated by column names.
#' @param nreps The number of bootstrap replications (default is 10).
#' @return A data frame containing the best hyperparameter values.
#' @keywords internal
#' @noRd
tune_model <- function(mspec, x, y, wts, param, nreps=10) {
  ctrl <- get_control(y, nreps)
  cfit <-caret::train(as.data.frame(x), y, method=mspec$model, weights=wts, metric=ctrl$metric, trControl=ctrl$ctrl, tuneGrid=param)
  cfit$bestTune
}

#' Fit an MVPA model
#'
#' This function fits a multivariate pattern analysis (MVPA) model to the given data.
#'
#' @param obj An object derived from the \code{mvpa_model} class.
#' @param x The training data matrix.
#' @param y The response vector.
#' @param wts Optional class weights (if the underlying model supports it).
#' @param param The hyperparameters of the model.
#' @param classProbs Logical; if TRUE, class probabilities should be computed (default is FALSE).
#' @param ... Additional arguments to be passed to the underlying model fitting function.
#' @return A fitted model object with additional attributes "obsLevels" and "problemType".
#' @noRd
fit_model.mvpa_model <- function(obj, x, y, wts, param, classProbs, ...) {
  fit <- obj$model$fit(x,y,wts=wts,param=param,lev=levels(y), classProbs=classProbs, ...)
  
  # Add levels both as attribute and list element for consistency
  fit$obsLevels <- levels(y)
  attr(fit, "obsLevels") <- levels(y)
  
  if (is.factor(y)) {
    attr(fit, "problemType") <- "Classification"
  } else {
    attr(fit, "problemType") <- "Regression"
  }
  
  fit
}

#' Predict class labels and probabilities for new data using a fitted model
#'
#' @param object A fitted model object of class \code{class_model_fit}.
#' @param newdata New data to predict on, either as a \code{matrix} or a \code{NeuroVec} or \code{NeuroSurfaceVector} object.
#' @param sub_indices The subset of row indices to compute predictions on (optional).
#' @param ... Additional arguments to be passed to the underlying prediction function.
#' @return A list containing class predictions and probabilities with class attributes "classification_prediction", "prediction", and "list".
#' @noRd
#' @keywords internal
predict.class_model_fit <- function(object, newdata, sub_indices=NULL,...) {
  tryCatch({
    mat <- if (inherits(newdata, "NeuroVec") || inherits(newdata, "NeuroSurfaceVector")) {
      series(newdata, object$fit$vox_ind)
    } else {
      newdata
    }
    
    if (!is.null(sub_indices)) {
      assert_that(is.vector(sub_indices))
      mat <- mat[sub_indices,,drop=FALSE]
    }
    
    if (!is.null(object$feature_mask)) {
      mat <- mat[, object$feature_mask,drop=FALSE]
    }

    futile.logger::flog.debug("Predicting with data dimensions: %s", paste(dim(mat), collapse=" x "))
    
    probs <- object$model$prob(object$fit, mat)
    if (is.null(probs) || length(probs) == 0) {
      stop("Model probability calculation returned NULL or empty result")
    }
    
    colnames(probs) <- levels(object$y)
    cpred <- max.col(probs)
    cpred <- levels(object$y)[cpred]
    ret <- list(class=cpred, probs=probs)
    class(ret) <- c("classification_prediction", "prediction", "list")
    ret
    
  }, error = function(e) {
    futile.logger::flog.error("Class model prediction failed: %s", e$message)
    futile.logger::flog.debug("Input data dimensions: %s", paste(dim(newdata), collapse=" x "))
    stop(sprintf("Prediction failed: %s", e$message))
  })
}



#' Predict continuous values for a new dataset using a regression model
#'
#' This function predicts continuous values for new data using a fitted regression model.
#'
#' @param object A fitted model object of class \code{regression_model_fit}.
#' @param newdata New data to predict on, either as a matrix or a \code{NeuroVec} or \code{NeuroSurfaceVector} object.
#' @param sub_indices A vector of indices used to subset rows of `newdata` (optional).
#' @param ... Additional arguments to be passed to the underlying prediction function.
#' @return A list containing predicted continuous values with class attributes "regression_prediction", "prediction", and "list".
#' @noRd
#' @keywords internal
predict.regression_model_fit <- function(object, newdata, sub_indices=NULL,...) {
  #browser()
  tryCatch({
    mat <- if (inherits(newdata, "NeuroVec") || inherits(newdata, "NeuroSurfaceVector")) {
      series(newdata, object$fit$vox_ind)
    } else {
      newdata
    }
    
    if (!is.null(sub_indices)) {
      assert_that(is.vector(sub_indices))
      mat <- mat[sub_indices,,drop=FALSE]
    }
    
    if (!is.null(object$feature_mask)) {
      mat <- mat[, object$feature_mask,drop=FALSE]
    }

    futile.logger::flog.debug("Regression prediction with data dimensions: %s", paste(dim(mat), collapse=" x "))
    
    preds <- object$model$predict(object$fit, mat)
    if (is.null(preds) || length(preds) == 0) {
      stop("Model prediction returned NULL or empty result")
    }
    
    ret <- list(preds=preds)
    class(ret) <- c("regression_prediction", "prediction", "list")
    ret
    
  }, error = function(e) {
    futile.logger::flog.error("Regression model prediction failed: %s", e$message)
    futile.logger::flog.debug("Input data dimensions: %s", paste(dim(newdata), collapse=" x "))
    stop(sprintf("Prediction failed: %s", e$message))
  })
}


#' @export
#' @method merge_predictions regression_prediction
merge_predictions.regression_prediction <- function(obj1, rest, weights=rep(1,length(rest)+1)/(length(rest)+1)) {
  allobj <- c(obj1, rest)
  assert_that(all(sapply(allobj, function(obj) inherits(obj, "regression_prediction"))))
  
  #preds <- lapply(1:length(allobj), function(i) {
  #  predict(allobj[[i]], newdata, ...)$pred * weights[i]
  #})
  
  preds <- lapply(1:length(allobj), function(i) {
    allobj[[i]]$pred * weights[i]
  })
  
  final_pred <- rowMeans(do.call(cbind, preds))
  ret <- list(preds=final_pred)
  class(ret) <- c("regression_prediction", "prediction", "list")
  ret
}


#' @export
#' @method merge_predictions classification_prediction
merge_predictions.classification_prediction <- function(obj1, rest, weights=rep(1,length(rest)+1)/(length(rest)+1)) {
  allobj <- vector(mode="list", length(rest)+1)
  allobj[[1]] <- obj1
  allobj[2:length(allobj)] <- rest
  
  #allobj <- c(obj1, rest)
  assert_that(all(sapply(allobj, function(obj) inherits(obj, "classification_prediction"))))
  
  #preds <- lapply(1:length(allobj), function(i) {
  #  predict(allobj[[i]], newdata, ...)$prob * weights[i]
  #})
  
  preds <- lapply(1:length(allobj), function(i) {
    allobj[[i]]$prob * weights[i]
  })
  
  prob <- preds[!sapply(preds, function(x) is.null(x))]
  pfinal <- Reduce("+", prob)
  
  cnames <- colnames(pfinal)
  maxids <- apply(pfinal, 1, which.max)
  len <- sapply(maxids, length)
  
  if (any(len == 0)) {
    maxids[len == 0] <- NA
  }
  
  maxids <- unlist(maxids)
  
  pclass <- cnames[maxids]
  ret <- list(class=pclass, probs=pfinal)
  class(ret) <- c("classification_prediction", "prediction", "list")
  ret
  
}


#' Create a Model Fit Object
#'
#' Constructs a model fit object, representing the result of a single model fit to a chunk of data. The object contains information about the model, response variable, model fit, problem type, model parameters, voxel indices, and an optional feature mask.
#'
#' @param model The caret-style model object.
#' @param y The response variable (predictand).
#' @param fit The fitted model.
#' @param model_type The problem type, either "classification" or "regression" (default). Must be one of the provided options.
#' @param param The model parameters.
#' @param vox_ind The voxel indices indicating the data coordinates.
#' @param feature_mask An optional logical mask indicating the selected subset of columns (features).
#'
#' @return An object of class \code{model_fit}, containing the model, response variable, fitted model, problem type, model parameters, voxel indices, and optional feature mask. The object is also assigned a class based on the problem type: \code{class_model_fit} for classification or \code{regression_model_fit} for regression.
#'
#' @keywords internal
#' @noRd
model_fit <- function(model, y, fit, model_type=c("classification", "regression"), param, vox_ind, feature_mask=NULL) {
  model_type=match.arg(model_type)
  
  ret <- list(
    model=model,
    y=y,
    fit=fit,
    model_type=model_type,
    param=param,
    vox_ind=vox_ind,
    feature_mask=feature_mask)
  
  if (model_type == "classification") {
    class(ret) <- c("class_model_fit", "model_fit")
  } else {
    class(ret) <- c("regression_model_fit", "model_fit")
  }
  ret
}

#' Create a Weighted Consensus Model
#'
#' Constructs a weighted consensus model formed as a weighted average of a set of models. The consensus model combines the input models according to their respective weights.
#'
#' @param fits A list of model fits to be combined.
#' @param names An optional list of names, one per model fit (default: numeric indices).
#' @param weights A vector of weights, one per model fit, that sum up to 1 (default: equal weights for all models).
#'
#' @return An object of class \code{weighted_model}, containing the list of model fits, their names, and the assigned weights. The object is also assigned a class `list`.
#'
#' @examples
#' # Create two sample model fits
#' fit1 <- list(model = "model1", y = c(0, 1), fit = "fit1")
#' fit2 <- list(model = "model2", y = c(1, 0), fit = "fit2")
#'
#' # Combine the model fits into a weighted consensus model
#' w_model <- weighted_model(fits = list(fit1, fit2), names = c("model1", "model2"), weights = c(0.6, 0.4))
#'
#' @keywords internal
#' @noRd
weighted_model <- function(fits, names=1:length(fits), weights=rep(1/length(fits), length(fits))) {
  stopifnot(length(weights) == length(fits))
  ret <- fits
  names(ret) <- names
  attr(ret, "weights") <- weights
  class(ret) <- c("weighted_model", "list")
  ret
}

#' a list of model fits
#' 
#' @param fits a list of fits
#' @param names the names of the fits
#' @export
#' @noRd
#' @keywords internal
list_model <- function(fits, names=1:length(fits)) {
  stopifnot(is.list(fits))
  ret <- fits
  names(ret) <- names
  class(ret) <- c("list_model", "list")
  ret
}

#' @export
#' @method predict weighted_model
predict.weighted_model <- function(object, newdata=NULL, ...) {
  if (is.null(newdata)) {
    stop("newdata cannot be null")
  }

  preds <- lapply(object, function(fit) predict(fit, newdata, ...))
  merge_predictions(preds[[1]], preds[2:length(preds)], attr(object, "weights"))
  
}

#' @export
#' @method predict list_model
predict.list_model <- function(object, newdata=NULL,...) {
  if (is.null(newdata)) {
    stop("newdata cannot be null")
  }
  
  res <- lapply(object, function(fit) {
    predict(fit, newdata,...)
  })
  
}



#' Train an MVPA Model
#'
#' This function trains a Multi-Variate Pattern Analysis (MVPA) model on the provided data, taking care of feature selection, parameter tuning, and model fitting.
#'
#' @param obj An object of class \code{mvpa_model}, specifying the MVPA problem.
#' @param train_dat Training data, an instance of class \code{ROIVolume} or \code{ROISurface}.
#' @param y The dependent variable (response variable), either a numeric vector or a factor.
#' @param indices The spatial indices associated with each column.
#' @param wts Optional class weights (if the underlying model supports it).
#' @param ... Additional arguments passed to other methods.
#' @return A model fit object containing the trained model, its fit, the model type (classification or regression), the best tuning parameters, the voxel indices, and the feature mask.
train_model.mvpa_model <- function(obj, train_dat, y, indices, wts=NULL, ...) {
  
  tryCatch({
    futile.logger::flog.debug("Starting train_model with data dimensions: %s", 
                             paste(dim(train_dat), collapse=" x "))
    futile.logger::flog.debug("Response variable levels: %s", 
                             paste(levels(y), collapse=", "))
    
    param <- tune_grid(obj, train_dat, y, len=1)
    futile.logger::flog.debug("Tuning grid parameters: %s", 
                             paste(names(param), collapse=", "))

    if (is.character(y)) {
      y <- as.factor(y)
    }
    
    ## columns that have zero variance
    nzero <- nonzeroVarianceColumns2(train_dat)
    futile.logger::flog.debug("Non-zero variance columns: %d", sum(nzero))
    
    ## columns with NAs
    nacols <- na_cols(train_dat)
    futile.logger::flog.debug("NA columns: %d", sum(nacols))
    
    ## duplicated columns
    dup <- !duplicated(t(train_dat))
    futile.logger::flog.debug("Non-duplicate columns: %d", sum(dup))
    
    ## invalid columns
    nzero <- nzero & dup & !nacols
    futile.logger::flog.debug("Valid columns after filtering: %d", sum(nzero))
    
    if (length(nzero) == 0 || sum(nzero,na.rm=TRUE) < 2) {
      stop(sprintf("training data must have more than one valid feature (found %d)", 
                  sum(nzero,na.rm=TRUE)))
    }
    
    ## feature selection and variable screening
    feature_mask <- if (!is.null(obj$feature_selector)) {
      nz <- which(nzero)
      fsel <- select_features(obj, train_dat[,nz], y)
      mask <- logical(ncol(train_dat))
      mask[nz[fsel]] <- TRUE
      mask
    } else {
      nzero
    }
    
    futile.logger::flog.debug("Features selected: %d", sum(feature_mask))
    
    if (sum(feature_mask) < 2) {
      stop("train_model: training data must have more than one valid feature after feature selection")
    }
    
    train_dat <- train_dat[,feature_mask,drop=FALSE]
    
    ## parameter_tuning
    best_param <- if (!is.vector(param) && !is.null(nrow(param)) && nrow(param) > 1) {
      bp <- tune_model(obj, train_dat, y, wts, param, obj$tune_reps)
      futile.logger::flog.debug("Best tuning parameters: %s", 
                               paste(capture.output(print(bp)), collapse="\n"))
      bp
    } else {
      param
    }
    
    mtype <- if (is.factor(y)) {
      "classification"
    } else if (is.numeric(y)) {
      "regression"
    } else {
      stop("'y' must be a numeric vector or factor")
    }
    
    futile.logger::flog.debug("Fitting model of type: %s", mtype)
    fit <- fit_model(obj, train_dat, y, wts=wts, param=best_param, classProbs=TRUE)
    model_fit(obj$model, y, fit, mtype, best_param, indices, feature_mask)
    
  }, error = function(e) {
    futile.logger::flog.error("train_model failed: %s", e$message)
    futile.logger::flog.debug("Data dimensions: %s", paste(dim(train_dat), collapse=" x "))
    futile.logger::flog.debug("Response levels: %s", paste(levels(y), collapse=", "))
    stop(e$message)  # Re-throw the error after logging
  })
}
</file>

<file path="tests/testthat/test_mvpa_searchlight.R">
library(neuroim2)
library(neurosurf)

gen_regression_dataset <- function(D, nobs, spacing=c(1,1,1), folds=5) {
  mat <- array(rnorm(prod(D)*nobs), c(D,nobs))
  bspace <- NeuroSpace(c(D,nobs), spacing)
  bvec <- NeuroVec(mat, bspace)
  mask <- as.logical(NeuroVol(array(rep(1, prod(D)), D), NeuroSpace(D, spacing)))
  Y <- rnorm(nobs)
  blockVar <- rep(1:folds, length.out=nobs)
  des <- mvpa_design(data.frame(Y=Y), block_var=blockVar, y_train= ~ Y)
  mvpa_dataset(bvec, mask=mask, design=des)
}

gen_dataset_with_test <- function(D, nobs, nlevels, spacing=c(1,1,1), folds=5, splitvar=TRUE) {
  mat <- array(rnorm(prod(D)*nobs), c(D,nobs))
  bspace <- NeuroSpace(c(D,nobs), spacing)
  bvec <- NeuroVec(mat, bspace)
  mask <- as.logical(NeuroVol(array(rep(1, prod(D)), D), NeuroSpace(D, spacing)))
  Y <- sample(factor(rep(letters[1:nlevels], length.out=nobs)))
  Ytest <- rev(Y)
  blockVar <- rep(1:folds, length.out=nobs)
  
  
  if (splitvar) {
    tsplit <- factor(rep(1:5, length.out=length(Y)))
    dframe <- data.frame(Y=Y, Ytest=Ytest, tsplit=tsplit)
    des <- mvpa_design(train_design=dframe, test_design=dframe, block_var=blockVar, y_train= ~ Y, y_test= ~ Ytest, split_by= ~ tsplit)
    mvpa_dataset(train_data=bvec, test_data=bvec, mask=mask, design=des)
  } else {
    des <- mvpa_design(data.frame(Y=Y, Ytest=Ytest), block_var=blockVar, y_train= ~ Y, y_test= ~ Ytest)
    mvpa_dataset(train_data=bvec, test_data=bvec, mask=mask, design=des)
  }
    
}

gen_dataset <- function(D, nobs, nlevels, spacing=c(1,1,1), folds=5) {
  
  mat <- array(rnorm(prod(D)*nobs), c(D,nobs))
  xbad <- array(runif(prod(D)) < .02, D)
  xbad.ind <- which(xbad, arr.ind=TRUE)
  
  for (i in 1:nrow(xbad.ind)) {
    ind <- xbad.ind[i,]
    mat[ind[1], ind[2], ind[3],] <- 0
  }
  
  bspace <- NeuroSpace(c(D,nobs), spacing)
  bvec <- NeuroVec(mat, bspace)
  mask <- as.logical(NeuroVol(array(rep(1, prod(D)), D), NeuroSpace(D, spacing)))
  Y <- sample(factor(rep(letters[1:nlevels], length.out=nobs)))
  blockVar <- rep(1:folds, length.out=nobs)
  
  des <- mvpa_design(data.frame(Y=Y), block_var=blockVar, y_train= ~ Y)
  mvpa_dataset(bvec, mask=mask, design=des)
}

gen_surface_dataset <- function(nobs, nlevels, folds=5) {
  library(neurosurf)
  fname <- system.file("extdata/std.lh.smoothwm.asc", package="neuroim2")
  geom <- read_surf_geometry(fname)
  nvert <- nrow(vertices(geom))
  mat <- matrix(rnorm(nvert*nobs), nvert, nobs)
  
  bvec <- NeuroSurfaceVector(geom, 1:nvert, mat)
  Y <- sample(factor(rep(letters[1:nlevels], length.out=nobs)))
  blockVar <- rep(1:folds, length.out=nobs)
  
  des <- mvpa_design(train_design=data.frame(Y=Y, blockVar=blockVar), y_train= ~ Y, block_var=~blockVar)
  mask <- rep(1, nvert)
  dataset <- mvpa_surface_dataset(bvec, mask=mask, design=des, name="lh")
  
  
}


test_that("standard mvpa_searchlight runs without error", {
  
  dataset <- gen_sample_dataset(c(8,8,8), 100, blocks=3)
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval)
  res <- run_searchlight(mspec,radius=8, method="standard")
  expect_true(!is.null(res))
  
})

test_that("standard mvpa_searchlight with boot_lda_thomaz runs without error", {
  
  dataset <- gen_sample_dataset(c(8,8,8), 100, blocks=3)
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("lda_thomaz")
  grid <- data.frame(nreps=2, frac=.5)
  mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval, tune_grid=grid)
  res <- run_searchlight(mspec,radius=8, method="standard")
  expect_true(!is.null(res))
  
})

test_that("standard mvpa_searchlight with boot_lda_thomaz and NA columns runs without error", {
  
  dataset <- gen_sample_dataset(c(8,8,8), 100, blocks=3, na_cols=25)
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("lda_thomaz")
  grid <- data.frame(nreps=2, frac=.5)
  mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval, tune_grid=grid)
  res <- run_searchlight(mspec,radius=8, method="standard")
  expect_true(!is.null(res))
  
})

test_that("standard mvpa_searchlight and custom cross-validation runs without error", {
  
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3)
  sample_set <- replicate(5, {
    list(train=sample(1:80), test=sample(81:100))
  }, simplify=FALSE)
  cval <- custom_cross_validation(sample_set)
 
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval)
  res <- run_searchlight(mspec,radius=4, method="standard")
  expect_true(!is.null(res))
  
})

test_that("standard surface-based mvpa_searchlight runs without error", {
  
  dataset <- gen_sample_dataset(D=0, nobs=100,data_mode="surface")
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design,model_type="classification", crossval=cval)
  res <- run_searchlight(mspec, radius=8, method="standard")
  expect_true(!is.null(res))
  
})

test_that("randomized surface-based mvpa_searchlight runs without error", {
  
  dataset <- gen_sample_dataset(D=100, nobs=100, data_mode="surface")
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design,model_type="classification", crossval=cval)
  res <- run_searchlight(mspec, radius=8, method="randomized", niter=4)
  expect_true(!is.null(res))
  
  
})

test_that("randomized mvpa_searchlight runs without error", {
  dataset <- gen_sample_dataset(c(12,12,12), 100, nlevels=2)
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, 
                      model_type="classification", crossval=cval)
  res <- run_searchlight(mspec,radius=4, method="randomized", niter=4)
  expect_true(!is.null(res))

})

test_that("mvpa_searchlight with sda_boot", {
  
  dataset <- gen_sample_dataset(c(8,8,8), 100, nlevels=6, blocks=3)
  cval <- blocked_cross_validation(dataset$design$block_var)
  model <- load_model("sda_boot")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval)
  res <- run_searchlight(mspec,radius=6, method="standard", niter=4)
  expect_true(!is.null(res))
  
  
})


# test_that("clustered mvpa_searchlight runs without error", {
#   
#   dataset <- gen_dataset(c(5,5,1), 100, 2)
#   crossVal <- blocked_cross_validation(dataset$blockVar)
#   model <- load_model("sda_notune", list(tuneGrid=NULL))
#   res <- mvpa_clustered_searchlight(dataset, model, crossVal, nclusters=c(2,3,4))
#   
# })

test_that("randomized mvpa_searchlight runs with custom_performance", {
   
   custom <- function(x) {
     cnames <- colnames(x$probs)
     y <- x$observed
     
     p1 <- x$probs[cbind(1:nrow(x$probs), as.integer(y))]
     ret <- c(m1 = mean(p1), m2=max(p1))
     ret
   }
   
   dataset <- gen_sample_dataset(c(5,5,3), 100, nlevels=3)
   cval <- blocked_cross_validation(dataset$design$block_var)
   model <- load_model("sda_notune")
   mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", 
                       crossval=cval, performance=custom)
   res <- run_searchlight(mspec,radius=3, method="randomized")
   expect_true(!is.null(res))
   

})


test_that("randomized mvpa_searchlight with bootstrap crossvalidation works", {
  
  
  dataset <- gen_sample_dataset(c(5,5,3), 100, nlevels=3)
  
  custom <- function(x) {
    cnames <- colnames(x$probs)
    y <- x$observed
    
    p1 <- x$probs[cbind(1:nrow(x$probs), as.integer(y))]
    ret <- c(m1 = mean(p1), m2=max(p1))
    ret
  }
  
  wts <- runif(100)
  cval <- bootstrap_blocked_cross_validation(dataset$design$block_var, 
  weights=wts)
  model <- load_model("sda_notune")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", 
                      crossval=cval, performance=custom)
  res <- run_searchlight(mspec,radius=3, method="randomized")
  expect_true(!is.null(res))
  
  
})

test_that("standard mvpa_searchlight and tune_grid runs without error", {
  
  dataset <- gen_sample_dataset(c(3,3,3), 50, nlevels=2, blocks=3)
  cval <- blocked_cross_validation(dataset$design$block_var)
  tuneGrid <- expand.grid(lambda=c(.1,.8), diagonal=c(TRUE, FALSE))
  model <- load_model("sda")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, tune_grid=tuneGrid)
  res <- run_searchlight(mspec, radius=3, method="standard")
  expect_true(!is.null(res))
  
})

test_that("standard mvpa_searchlight and tune_grid with two-fold cross-validation runs without error", {
  
  dataset <- gen_sample_dataset(c(2,2,6), 100, nlevels=2, blocks=2)
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  tuneGrid <- expand.grid(lambda=c(.1,.8), diagonal=c(TRUE, FALSE))
  model <- load_model("sda")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, tune_grid=tuneGrid)
  res <- run_searchlight(mspec, radius=3, method="standard")
  expect_true(!is.null(res))
  
})

test_that("randomized mvpa_searchlight and tune_grid runs without error", {
  
  dataset <- gen_sample_dataset(c(6,6,6), 100, nlevels=2, blocks=2)
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  tuneGrid <- expand.grid(lambda=c(.1,.8), diagonal=c(TRUE, FALSE))
  model <- load_model("sda")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval, tune_grid=tuneGrid)
  res <- run_searchlight(mspec, radius=3, method="randomized")
  expect_true(!is.null(res))
  
})

test_that("randomized mvpa_searchlight works with regression", {
  library(spls)
  dataset <- gen_sample_dataset(c(6,6,6), 100, blocks=3, response_type="continuous")
  cval <- blocked_cross_validation(dataset$design$block_var)
  tuneGrid <- expand.grid(K=3, eta=.5, kappa=.5)
  model <- load_model("spls")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="regression", 
                      crossval=cval, tune_grid=tuneGrid)
  res <- run_searchlight(mspec, radius=6, niter=2,method="randomized")
  expect_true(!is.null(res))
})

test_that("randomized mvpa_searchlight works with xgboost", {
  
  dataset <- gen_sample_dataset(c(6,6,6), 100, nlevels=5, blocks=5)
  cval <- blocked_cross_validation(dataset$design$block_var)
  tuneGrid <- expand.grid(max_depth=2, eta=.5, nrounds=100,gamma=0,colsample_bytree=.6, min_child_weight=1, subsample=.5)
  model <- load_model("xgbTree")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, crossval=cval, tune_grid=tuneGrid)
  res <- run_searchlight(mspec, radius=5, niter=2,method="randomized")
  expect_true(!is.null(res))
  
})

test_that("mvpa_searchlight works with testset", {
  require("sda")
  dataset <- gen_sample_dataset(c(4,4,4), 100, response_type="categorical", data_mode="image", 
                                blocks=5, nlevels=4, external_test=TRUE)
  
  cval <- blocked_cross_validation(dataset$design$block_var)

  model <- load_model("sda")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", crossval=cval)
  res <- run_searchlight( mspec, radius=6, method="randomized")
  expect_true(!is.null(res))
})


 
test_that("mvpa_searchlight works with split_var", {
  dataset <- gen_sample_dataset(c(5,5,5), 100, blocks=3, split_by=factor(rep(1:4, each=25)))

  crossVal <- blocked_cross_validation(dataset$design$block_var)
  tuneGrid <- expand.grid(alpha=.5, lambda=c(.1))
  model <- load_model("glmnet")
  mspec <- mvpa_model(model, dataset$dataset, dataset$design, model_type="classification", 
                      crossval=crossVal, class_metrics=FALSE)
  res <- run_searchlight(mspec, radius=3, niter=2,method="randomized")
  expect_true(!is.null(res))
 
})


# test_that("mvpa_searchlight on real data set", {
#   tdat <- c(
#     system.file("extdata", "sub-1005_task-localizer_run-01_bold_space-MNI_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-02_bold_space-MNI_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-03_bold_space-MNI_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-04_bold_space-MNI_preproc_betas_small.nii.gz", package="rMVPA")
#   )
#   
#   tdes <- c(
#     system.file("extdata", "sub-1005_task-localizer_run-01_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-02_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-03_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-04_events.tsv", package="rMVPA")
#   )
#   mask <- neuroim2::read_vol(system.file("extdata", "sub-1005-MNI152NLin2009cAsym_small_global_mask.nii", package="rMVPA"))
#   
#   tvec <- neuroim2::read_vec(tdat)
#   des <- do.call(rbind, lapply(tdes,read.table, header=TRUE))
#   dset <- mvpa_dataset(tvec, mask=mask)  
#   mdes <- mvpa_design(des, y_train = ~ BlockType, block_var=~ Run)
#   mod <- mvpa_model(load_model("sda_notune"), dset,mdes,crossval=blocked_cross_validation(des$Run))
#   res <- run_searchlight(mod, radius=16, niter=2,method="randomized")
#   expect_true(!is.null(res))
#   
# })
# 
# test_that("mvpa_searchlight on real data set", {
#   tdat <- c(
#     system.file("extdata", "sub-1005_task-localizer_run-01_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-02_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-03_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-04_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA")
#   )
#   
#   tdes <- c(
#     system.file("extdata", "sub-1005_task-localizer_run-01_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-02_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-03_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-04_events.tsv", package="rMVPA")
#   )
#   mask <- neuroim2::read_vol(system.file("extdata", "sub-1005-MNI152NLin2009cAsym_small_global_mask.nii", package="rMVPA"))
#   
#   tvec <- neuroim2::read_vec(tdat)
#   des <- do.call(rbind, lapply(tdes,read.table, header=TRUE))
#   dset <- mvpa_dataset(tvec, mask=mask)  
#   mdes <- mvpa_design(des, y_train = ~ BlockType, block_var=~ Run)
#   mod <- mvpa_model(load_model("sda_notune"), dset,mdes,crossval=blocked_cross_validation(des$Run))
#   res <- run_searchlight(mod, radius=16, niter=3,method="randomized")
#   expect_true(!is.null(res))
#   
# })
# 
# test_that("mvpa_searchlight on real data set with testset", {
#   tdat1 <- c(
#     system.file("extdata", "sub-1005_task-localizer_run-01_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-02_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-03_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-04_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA")
#   )
#   
#   tdat2 <- c(
#     system.file("extdata", "sub-1005_task-wm_run-01_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-wm_run-02_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-wm_run-03_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-wm_run-04_bold_space-MNI152NLin2009cAsym_preproc_betas_small.nii.gz", package="rMVPA")
#   )
#   
#   tdes1 <- c(
#     system.file("extdata", "sub-1005_task-localizer_run-01_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-02_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-03_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-localizer_run-04_events.tsv", package="rMVPA")
#   )
#   
#   tdes2 <- c(
#     system.file("extdata", "sub-1005_task-wm_run-01_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-wm_run-02_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-wm_run-03_events.tsv", package="rMVPA"),
#     system.file("extdata", "sub-1005_task-wm_run-04_events.tsv", package="rMVPA")
#   )
#   
#   
#   mask <- neuroim2::read_vol(system.file("extdata", "sub-1005-MNI152NLin2009cAsym_small_global_mask.nii", package="rMVPA"))
#   
#   tvec1 <- neuroim2::read_vec(tdat1)
#   tvec2 <- neuroim2::read_vec(tdat2)
#   des1 <- do.call(rbind, lapply(tdes1,read.table, header=TRUE))
#   des2 <- do.call(rbind, lapply(tdes2,read.table, header=TRUE))
#   library(dplyr)
#   
#   des3 <- des2 %>% mutate(combo = 
#                     case_when(
#                       (Cue == "Faces" & ToBeIgnored == "Scenes") | (Cue == "Scenes" & ToBeIgnored == "Faces") ~ "Faces-Scenes",
#                       (Cue == "Bodies" & ToBeIgnored == "Scenes") | (Cue == "Scenes" & ToBeIgnored == "Bodies") ~ "Bodies-Scenes",
#                       (Cue == "Objects" & ToBeIgnored == "Scenes") | (Cue == "Scenes" & ToBeIgnored == "Objects") ~ "Objects-Scenes",
#                       (Cue == "Objects" & ToBeIgnored == "Faces") | (Cue == "Faces" & ToBeIgnored == "Objects") ~ "Faces-Objects",
#                       (Cue == "Faces" & ToBeIgnored == "Bodies") | (Cue == "Bodies" & ToBeIgnored == "Faces") ~ "Faces-Bodies",
#                       (Cue == "Objects" & ToBeIgnored == "Bodies") | (Cue == "Bodies" & ToBeIgnored == "Objects") ~ "Objects-Bodies")
#   )
#                       
#                       
#   
#   dset <- mvpa_dataset(tvec1, tvec2, mask=mask)  
#   mdes <- mvpa_design(train_design=des1, y_train = ~ BlockType, test_design=des3, y_test = ~ Cue, block_var=~ Run, split_by = ~ combo)
#   mod <- mvpa_model(load_model("sda_notune"), dset,mdes,
#                     crossval=blocked_cross_validation(des1$Run), class_metrics=FALSE)
#   res <- run_searchlight(mod, radius=16, niter=3,method="randomized", 
#                          combiner=function(mspec, good, bad) {
#                            good
#                          })
#     expect_true(!is.null(res))
# })
</file>

<file path="R/crossval.R">
#' @noRd
#' @keywords internal
check_len <- function(y, block_var) {
  futile.logger::flog.debug("Checking length of y and block_var")
  futile.logger::flog.debug("y: %s", paste(dim(y), collapse=" x "))
  futile.logger::flog.debug("block_var: %s", length(block_var))
  if (is.vector(y)) {
    if (!length(block_var) == length(y)) {
      stop("length of `block_var` must be equal to length(y)", call. = FALSE)
    }
  } else if (is.matrix(y)) {
    if (!nrow(y) == length(block_var)) {
      stop("number of rows in `y` must be equal to length(block_var)", call. = FALSE)
    }
  }
}


#' @noRd
#' @keywords internal
subset_y <- function(y, idx) {
  if (is.vector(y) || is.factor(y)) {
    y[idx]
  } else if (is.matrix(y)) {
    y[idx,,drop=FALSE]
  }
}

#' K-fold Cross-Validation Data Preparation
#'
#' This function prepares the data for k-fold cross-validation by dividing the
#' dataset into k folds. It creates subsets of training and testing data for
#' each fold without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param k An integer specifying the number of folds for cross-validation.
#' @param id A character string specifying the identifier for the output data frame.
#' @return A tibble containing the training and testing data, response vectors, and fold IDs for each fold.
#' @examples
#' data <- iris[,-5]
#' y <- iris$Species
#' result <- crossv_k(data, y, k = 5)
#' @importFrom modelr resample
#' @export
crossv_k <- function(data, y, k = 5, id = ".id") {
  if (!is.numeric(k) || length(k) != 1) {
    stop("`k` must be a single integer.", call. = FALSE)
  }
  
  
  n <- nrow(data)
  folds <- sample(rep(1:k, length.out = n))
  
  idx <- seq_len(n)
  fold_idx <- split(idx, folds)
  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, setdiff(idx, test)),
      test = modelr::resample(data, test)
    )
  }
  
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <- gen_id(k)
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' Repeated Two-Fold Cross-Validation Data Preparation
#'
#' This function prepares the data for repeated two-fold cross-validation by
#' dividing the dataset into two folds based on the provided block variable.
#' It creates subsets of training and testing data for each repetition without
#' performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param block_ind A vector containing the ordered integer IDs of the blocks (optional).
#' @param id A character string specifying the identifier for the output data frame.
#' @param nreps An integer specifying the number of repetitions for two-fold cross-validation.
#' @return A tibble containing the training and testing data, response vectors, and fold IDs for each repetition.
#' @examples
#' X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
#' y <- rep(letters[1:4], 25)
#' block_var <- rep(1:4, each = 25)
#' cv <- crossv_twofold(X, y, block_var, nreps = 10)
#' @noRd
crossv_twofold <- function(data, y, block_var, block_ind=NULL, id = ".id", nreps=15) {
  ## every time this is called, it regenerates new indices
  if (nreps < 2) {
    stop("'nreps' must be at least 2")
  }
  
  check_len(y, block_var)
  
  if (is.null(block_ind)) {
    block_ind <- seq(1, length(sort(unique(block_var))))
  }

  nhalf <- floor(length(block_ind)/2)
  assert_that(nhalf > 0)
  
  fold_sets <- utils::combn(block_ind, nhalf)
  nreps <- min(nreps, ncol(fold_sets))
  
 
  cols <- as.integer(seq(1, ncol(fold_sets), length.out=nreps))
  #sample(1:ncol(fold_sets), nreps)
 
  fold_idx <- lapply(1:nreps, function(i) {
    bind <- fold_sets[, cols[i]]
    which(block_var %in% bind)
  })
  
  idx <- seq_len(nrow(data))
  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, test)
    )
  }
  
  ## this could return a function that when given data, returns a tibble
  ## fun <- function(data) {... for every rows, mutate(train = ..., test = ...)} 
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <-gen_id(length(fold_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")
  
  
}


#' Block Cross-Validation Data Preparation
#'
#' This function prepares the data for block cross-validation by dividing the dataset
#' based on the provided block variable. It creates subsets of training and testing
#' data for each block without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param id A character string specifying the identifier for the output data frame.
#' @return A tibble containing the training and testing data, response vectors, and block IDs for each fold.
#' @examples
#' X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
#' y <- rep(letters[1:4], 25)
#' block_var <- rep(1:4, each = 25)
#' cv <- crossv_block(X, y, block_var)
#' @export
crossv_block <- function(data, y, block_var, id = ".id") {
 
  check_len(y, block_var)
  
  idx <- seq_len(nrow(data))
  fold_idx <- split(idx, block_var)

  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, test)
    )
  }
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <- gen_id(length(fold_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' Block Bootstrap Cross-Validation Data Preparation
#'
#' This function prepares the data for block bootstrap cross-validation by dividing the dataset
#' based on the provided block variable. It creates subsets of training and testing
#' data for each block using bootstrap sampling within the training blocks, without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param nreps An integer specifying the number of bootstrap repetitions.
#' @param id A character string specifying the identifier for the output data frame.
#' @param weights An optional numeric vector of weights to be used for bootstrap sampling.
#'
#' @details
#' The function first checks if the length of the `block_var` vector matches the length of the response vector `y`.
#' It then creates a list of block indices and ensures there is more than one block to bootstrap. If weights are provided,
#' the function splits the weights according to the block variable.
#'
#' The function performs bootstrap sampling within the training blocks but keeps the test set fixed.
#' For each block, it generates a list of training indices using bootstrap sampling and creates the corresponding
#' training and testing data sets.
#'
#' @return A tibble containing the training and testing data, response vectors, and block IDs for each fold.
#'
#' @keywords internal
crossv_bootstrap_block <- function(data, y, block_var, nreps=5, id = ".id", weights=NULL) {
  check_len(y, block_var)
  
  idx <- seq_len(nrow(data))
  block_idx <- split(idx, block_var)
  
  
  assert_that(length(block_idx) > 1, msg="crossv_bootstrap_block: must have more than one block to bootstrap.")
  
 
  ## alter so that you bootstrap within the training blocks but test set is fixed
  fold_idx <- if (!is.null(weights)) {
    block_wts <- split(weights, block_var)
    fold_idx <- lapply(seq_along(block_idx), function(heldout) {
      replicate(nreps, sample(unlist(block_idx[-heldout]), replace=TRUE, prob=unlist(block_wts[-heldout])), simplify=FALSE)
    })
  } else {
    fold_idx <- lapply(seq_along(block_idx), function(heldout) {
      replicate(nreps, sample(unlist(block_idx[-heldout]), replace=TRUE), simplify=FALSE)
    })
  }
  

  
  #fold_idx <- lapply(unlist(fold_idx, recursive=FALSE), sort)
  #fold_idx <- lapply(fold_idx, function(fidx) setdiff(idx, fidx))
  
  
  fold <- function(tidx, block) {
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, block_idx[[block]]),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, block_idx[[block]])
    )
  }
  
  cols <- unlist(lapply(seq_along(fold_idx), function(i) {
    lapply(fold_idx[[i]], function(tidx) {
      fold(tidx, i)
    })
  }), recursive=FALSE)
  
  cols <- purrr::transpose(cols)
  cols[[id]] <- gen_id(nreps * length(block_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' Sequential Block Cross-Validation Data Preparation
#'
#' This function prepares the data for sequential block cross-validation by dividing the dataset
#' based on the provided block variable. It creates subsets of training and testing
#' data for each block using sequential sampling within the blocks, without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param nfolds An integer specifying the number of folds for cross-validation.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param nreps An integer specifying the number of repetitions for each fold.
#' @param block_ind An optional integer vector specifying the ordered ids of the blocks.
#' @param id A character string specifying the identifier for the output data frame.
#'
#' @details
#' The function first checks if the length of the `block_var` vector matches the length of the response vector `y`.
#' It then creates a list of block indices and generates a fold sequence using the provided `nfolds` and `nreps` parameters.
#'
#' For each repetition and fold, the function identifies the indices corresponding to the test data and creates the
#' corresponding training and testing data sets.
#'
#' @return A tibble containing the training and testing data, response vectors, and fold IDs for each fold and repetition.
#'
#' @examples
#' X <- data.frame(x1=rnorm(100), x2=rnorm(100))
#' y <- rep(letters[1:4], 25)
#' block_var <- rep(1:4, each=25)
#' cv <- crossv_seq_block(X,y,2, block_var)
#' @noRd
crossv_seq_block <- function(data, y, nfolds, block_var, nreps=4, block_ind = NULL, id = ".id") {
  check_len(y, block_var)
  
  idx <- seq_len(nrow(data))
  block_idx <- split(idx, block_var)
  
  if (is.null(block_ind)) {
    block_ind <- seq(1, length(sort(unique(block_var))))
  }
  
  foldseq <- replicate(nreps, {
    unlist(lapply(block_idx, function(id) {
      as.integer(as.character(cut(id, nfolds, labels=sample(1:nfolds))))
    }))
    
  }, simplify=FALSE)
  
  fold_idx <- unlist(lapply(1:nreps, function(i) {
    lapply(1:nfolds, function(j) which(foldseq[[i]] == j))
  }), recursive=FALSE)
  
  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, test)
    )
  }
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <- gen_id(length(fold_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")

}


#' bootstrap_blocked_cross_validation
#' 
#' Bootstrap Blocked Cross-Validation Specification
#'
#' This function constructs a cross-validation specification using a predefined blocking variable
#' and creates bootstrap resamples within the blocks.
#'
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param nreps An integer specifying the number of repetitions for each fold.
#' @param weights A numeric vector of the same length as `block_var`, representing the weights for each sample.
#'        Higher weights indicate that observations will be sampled more often. If not provided, all samples are treated as equally likely.
#'
#' @details
#' The function first checks if the provided weights are non-negative and normalizes them to sum to 1.
#' It then constructs a list containing the block variable, number of folds, block indices, number of repetitions, and weights.
#' The output list is assigned the class `"bootstrap_blocked_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the cross-validation specification, with class attributes "bootstrap_blocked_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' block_var <- rep(1:5, each=50)
#' weights <- runif(length(block_var))
#' weights[1] = 0
#' cval <- bootstrap_blocked_cross_validation(block_var, weights=weights)
#' X <- matrix(rnorm(length(block_var) * 10), length(block_var), 10)
#' y <- rep(letters[1:5], length.out=length(block_var))
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
bootstrap_blocked_cross_validation <- function(block_var, nreps=10, weights=NULL) {
  if (!is.null(weights)) {
    assert_that(length(weights) == length(block_var))
    assert_that(all(weights >= 0))  
    weights <- weights/sum(weights)
  }

  ret <- list(block_var=block_var, nfolds=length(unique(block_var)), 
              block_ind=sort(unique(block_var)), nreps=nreps, weights=weights)
  class(ret) <- c("bootstrap_blocked_cross_validation", "cross_validation", "list")
  ret
}


#' Blocked Cross-Validation Specification
#'
#' This function constructs a cross-validation specification using a predefined blocking variable.
#'
#' @param block_var An integer vector defining the cross-validation blocks.
#'
#' @details
#' The function constructs a list containing the block variable, number of folds, and block indices.
#' The output list is assigned the class `"blocked_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the cross-validation specification, with class attributes "blocked_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' block_var <- rep(1:5, each=50)
#' cval <- blocked_cross_validation(block_var)
#' X <- matrix(rnorm(length(block_var) * 10), length(block_var), 10)
#' y <- rep(letters[1:5], length.out=length(block_var))
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
blocked_cross_validation <- function(block_var) {
  ret <- list(block_var=block_var, nfolds=length(unique(block_var)), block_ind=sort(unique(block_var)))
  class(ret) <- c("blocked_cross_validation", "cross_validation", "list")
  ret
}



#' Sequential Blocked Cross-Validation Specification
#'
#' This function constructs a cross-validation specification using a predefined blocking variable, dividing each block into a specified number of folds.
#'
#' @param block_var An integer vector indicating the cross-validation blocks. Each block is indicated by a unique integer.
#' @param nfolds The number of folds to divide each sequence of trials within a block.
#' @param nreps The number of repetitions for the cross-validation procedure.
#'
#' @details
#' The function constructs a list containing the block variable, number of folds, number of repetitions, and block indices.
#' The output list is assigned the class `"sequential_blocked_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the cross-validation specification, with class attributes "sequential_blocked_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' block_var <- rep(1:5, each=50)
#' nfolds <- 2
#' nreps <- 4
#' cval <- sequential_blocked_cross_validation(block_var, nfolds, nreps)
#' X <- matrix(rnorm(length(block_var) * 10), length(block_var), 10)
#' y <- rep(letters[1:5], length.out=length(block_var))
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
sequential_blocked_cross_validation <- function(block_var, nfolds=2, nreps=4) {
  block_var <- as.integer(as.character(block_var))
  ret <- list(block_var=block_var, nfolds=nfolds, nreps=nreps, block_ind=sort(unique(block_var)))
  class(ret) <- c("sequential_blocked_cross_validation", "cross_validation", "list")
  ret
}



#' Custom Cross-Validation Specification
#'
#'
#' This function constructs a cross-validation specification that uses a user-supplied set of training and test indices.
#'
#' @param sample_set A list of training and test sample indices. Each element of the list must be a named list with two elements: "train" and "test".
#'
#' @details
#' The custom_cross_validation class allows users to define their own cross-validation structure by providing a set of training and test indices. This can be useful in situations where the standard cross-validation methods (e.g., k-fold, leave-one-out) do not adequately represent the desired validation structure.
#'
#' The function constructs a list containing the sample set and the number of folds, derived from the length of the sample set. The output list is assigned the class `"custom_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the custom cross-validation specification, with class attributes "custom_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' sample_set <- list(
#'   list(train = 1:80, test = 81:100),
#'   list(train = 1:60, test = 61:100),
#'   list(train = 1:40, test = 41:100)
#' )
#' cval <- custom_cross_validation(sample_set)
#' X <- matrix(rnorm(100 * 10), 100, 10)
#' y <- rep(letters[1:4], length.out=100)
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
custom_cross_validation <- function(sample_set) {
  assert_that(is.list(sample_set))
  for (el in sample_set) {
    assert_that(all(names(el) == c("train", "test")))
  }
  
  ret <- list(sample_set=sample_set, nfolds=length(sample_set))
  class(ret) <- c("custom_cross_validation", "cross_validation", "list")
  ret
  
}
  
  
#' twofold_blocked_cross_validation
#'
#' Construct a cross-validation specification that randomly partitions the input set into two sets of blocks.
#'
#' This function creates a cross-validation scheme for cases where data is organized into blocks, and these blocks
#' are divided into two groups for evaluation. This approach can be useful when there is an inherent structure or
#' dependency within the blocks, and separating them can help to avoid biased estimates of model performance.
#' It returns an object of class "twofold_blocked_cross_validation", "cross_validation", and "list".
#'
#' @param block_var An integer vector representing the cross-validation blocks. Each block is indicated by a unique integer.
#' @param nreps An integer specifying the number of repetitions for the twofold split.
#' @return An object of class "twofold_blocked_cross_validation", "cross_validation", and "list" containing the block_var,
#'   nfolds (fixed at 2 for this function), nreps, and block_ind.
#' @export
#' @examples
#' blockvar <- rep(1:5, each=10)
#' nreps <- 5
#' cval <- twofold_blocked_cross_validation(blockvar, nreps=nreps)
#' samples <- crossval_samples(cval, as.data.frame(matrix(rnorm(50*50),50,50)), y=rep(letters[1:5],10))
#' stopifnot(nrow(samples) == nreps)
twofold_blocked_cross_validation <- function(block_var, nreps=10) {
  block_var <- as.integer(as.character(block_var))
  ret <- list(block_var=block_var, nfolds=2, nreps=nreps, block_ind=sort(unique(block_var)))
  class(ret) <- c("twofold_blocked_cross_validation", "cross_validation", "list")
  ret
}

#' kfold_cross_validation
#'
#' Construct a cross-validation specification that randomly partitions the input set into \code{nfolds} folds.
#'
#' This function creates a k-fold cross-validation scheme for cases where data needs to be split into a specified
#' number of folds for evaluation. It returns an object of class "kfold_cross_validation", "cross_validation", and "list".
#'
#' @param len An integer representing the number of observations.
#' @param nfolds An integer specifying the number of cross-validation folds.
#' @return An object of class "kfold_cross_validation", "cross_validation", and "list" containing the block_var and nfolds.
#' @examples
#' cval <- kfold_cross_validation(len=100, nfolds=10)
#' samples <- crossval_samples(cval, data=as.data.frame(matrix(rnorm(100*10), 100, 10)), y=rep(letters[1:5],20))
#' stopifnot(nrow(samples) == 10)
#' @export
kfold_cross_validation <- function(len, nfolds=10) {
  block_var <- sample(rep(seq(1, nfolds), length.out=len))
  ret <- list(block_var=block_var, nfolds=nfolds)
  class(ret) <- c("kfold_cross_validation", "cross_validation", "list")
  ret
}


# nest <- function(cval) {
#   clist <- lapply(cval$block_ind, function(i) {
#     blocked_cross_validation(cval$block_var, exclude=i)
#   })
#   
#   class(clist) = c("nested_blocked_cross_validation", "list")
#   clist
# }

#' @export
crossval_samples.sequential_blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_seq_block(data, y, nfolds=obj$nfolds, block_var=obj$block_var, nreps=obj$nreps, block_ind=obj$block_ind)
}

#' @export
crossval_samples.kfold_cross_validation <- function(obj, data,y,...) { 
  crossv_k(data, y, obj$nfolds)
}

#' @export
crossval_samples.blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_block(data, y, obj$block_var)
}

#' @export
crossval_samples.bootstrap_blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_bootstrap_block(data, y, block_var=obj$block_var, nreps=obj$nreps, weights=obj$weights)
}

#' @export
#' @importFrom modelr resample
crossval_samples.custom_cross_validation <- function(obj, data, y, id = ".id",...) {
  fold <- function(train, test) {
    list(
      ytrain = y[train],
      ytest = y[test],
      train = modelr::resample(data, train),
      test = modelr::resample(data, test)
    )
  }
  
  cols <- purrr::transpose(purrr::map(obj$sample_set, function(el) fold(el$train, el$test)))
  cols[[id]] <- gen_id(length(obj$sample_set))
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' @export
crossval_samples.twofold_blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_twofold(data, y, obj$block_var, obj$block_ind, nreps=obj$nreps)
}


#' @export
#' @method print blocked_cross_validation
print.blocked_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Blocked Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Dataset Information"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  └─ Number of Folds: "), number_style(x$nfolds), "\n")
  
  # Block statistics
  block_sizes <- table(x$block_var)
  cat(section_style("└─ Block Information"), "\n")
  cat(info_style("   ├─ Total Blocks: "), number_style(length(block_sizes)), "\n")
  cat(info_style("   ├─ Mean Block Size: "), 
      number_style(format(mean(block_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(block_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("   └─ Block Sizes: "), 
      number_style(paste0(names(block_sizes), ": ", block_sizes, collapse=", ")), "\n\n")
}

#' @export
#' @method print twofold_blocked_cross_validation
print.twofold_blocked_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Two-Fold Blocked Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  ├─ Number of Folds: "), number_style("2"), "\n")
  cat(info_style("│  └─ Repetitions: "), number_style(x$nreps), "\n")
  
  # Block statistics
  block_sizes <- table(x$block_var)
  cat(section_style("└─ Block Information"), "\n")
  cat(info_style("   ├─ Total Blocks: "), number_style(length(block_sizes)), "\n")
  cat(info_style("   ├─ Mean Block Size: "), 
      number_style(format(mean(block_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(block_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("   └─ Block Sizes: "), 
      number_style(paste0(names(block_sizes), ": ", block_sizes, collapse=", ")), "\n\n")
}

#' @export
#' @method print bootstrap_blocked_cross_validation
print.bootstrap_blocked_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Bootstrap Blocked Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  └─ Bootstrap Repetitions: "), number_style(x$nreps), "\n")
  
  # Block statistics
  block_sizes <- table(x$block_var)
  cat(section_style("├─ Block Information"), "\n")
  cat(info_style("│  ├─ Total Blocks: "), number_style(length(block_sizes)), "\n")
  cat(info_style("│  ├─ Mean Block Size: "), 
      number_style(format(mean(block_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(block_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("│  └─ Block Sizes: "), 
      number_style(paste0(names(block_sizes), ": ", block_sizes, collapse=", ")), "\n")
  
  # Weight information if present
  cat(section_style("└─ Sampling Weights"), "\n")
  if (!is.null(x$weights)) {
    cat(info_style("   ├─ Status: "), crayon::green("Present"), "\n")
    cat(info_style("   ├─ Range: "), 
        number_style(sprintf("[%.3f, %.3f]", min(x$weights), max(x$weights))), "\n")
    cat(info_style("   └─ Non-zero Weights: "), 
        number_style(sum(x$weights > 0)), 
        stat_style(" ("), 
        number_style(sprintf("%.1f%%", 100*mean(x$weights > 0))),
        stat_style(")"), "\n\n")
  } else {
    cat(info_style("   └─ Status: "), crayon::red("None"), " (uniform sampling)\n\n")
  }
}

#' @export
#' @method print kfold_cross_validation
print.kfold_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ K-Fold Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  ├─ Number of Folds: "), number_style(x$nfolds), "\n")
  
  # Fold statistics
  fold_sizes <- table(x$block_var)
  cat(section_style("└─ Fold Information"), "\n")
  cat(info_style("   ├─ Mean Fold Size: "), 
      number_style(format(mean(fold_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(fold_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("   └─ Fold Sizes: "), 
      number_style(paste0("Fold ", names(fold_sizes), ": ", fold_sizes, collapse=", ")), 
      "\n\n")
}
</file>

<file path="R/mvpa_model.R">
#' @keywords internal
#' @noRd
wrap_result <- function(result_table, design, fit=NULL) {
  
  observed <- y_test(design)
  
  ## It could happen that not all design rows are actually tested, which is why we find the unique set of test indices
  testind <- unique(sort(unlist(result_table$test_ind)))
  
  if (is.factor(observed)) {
    prob <- matrix(0, length(testind), length(levels(observed)))
    colnames(prob) <- levels(observed)
    
    for (i in seq_along(result_table$probs)) {
      p <- as.matrix(result_table$probs[[i]])
      tind <- match(result_table$test_ind[[i]], testind)
      prob[tind,] <- prob[tind,] + p
    }
    
    ## probs must sum to one, can divide by sum.
    prob <- t(apply(prob, 1, function(vals) vals / sum(vals)))
    maxid <- max.col(prob)
    pclass <- levels(observed)[maxid]
    
    ## storing observed, testind, test_design 
    classification_result(observed[testind], pclass, prob, testind=testind, design$test_design, fit)
  } else {
    
    testind <- unique(sort(unlist(result_table$test_ind)))
    preds <- numeric(length(testind))
    
    for (i in seq_along(result_table$preds)) {
      #tind <- result_table$test_ind[[i]]
      tind <- match(result_table$test_ind[[i]], testind)
      preds[tind] <- preds[tind] + result_table$preds[[i]]
    }
    
    ## TODO check me
    counts <- table(sort(unlist(result_table$test_ind)))
    preds <- preds/counts
    regression_result(observed, preds, testind=testind, test_design=design$test_design, fit)
  }
}



#' @keywords internal
merge_results.mvpa_model <- function(obj, result_set, indices, id, ...) {
  # Check if any errors occurred during the process
  if (any(result_set$error)) {
    emessage <- result_set$error_message[which(result_set$error)[1]]
    tibble::tibble(result=list(NULL), indices=list(indices), performance=list(NULL), error=TRUE, error_message=emessage)
  } else {
    # If no errors, wrap the result and compute performance if required
    cres <- if (obj$return_fit) {
      predictor <- weighted_model(result_set$fit)
      wrap_result(result_set, obj$design, predictor)
    } else {
      wrap_result(result_set, obj$design)
    }
    
    if (obj$compute_performance) {
      tibble::tibble(result=list(cres), indices=list(indices),
                     performance=list(compute_performance(obj, cres)),
                     id=id, error=FALSE, error_message="~")
    } else {
      tibble::tibble(result=list(cres), indices=list(indices),
                     performance=list(NULL),
                     id=id, error=FALSE, error_message="~")
    }
  }
 
}


#' @noRd
format_result.mvpa_model <- function(obj, result, error_message=NULL, context, ...) {
  if (!is.null(error_message)) {
    return(tibble::tibble(class=list(NULL), probs=list(NULL), y_true=list(context$ytest),
                          fit=list(NULL), error=TRUE, error_message=error_message))
  } else {
    pred <- predict(result, tibble::as_tibble(context$test, .name_repair=.name_repair), NULL)
    plist <- lapply(pred, list)
    plist$y_true <- list(context$ytest)
    plist$test_ind <- list(as.integer(context$test))
    plist$fit <- if (obj$return_fit) list(result) else list(NULL)
    plist$error <- FALSE
    plist$error_message <- "~"
    tibble::as_tibble(plist, .name_repair=.name_repair)
  }
}



#' @keywords internal
get_multiclass_perf <- function(split_list=NULL, class_metrics=TRUE) {
  function(result) {
    performance(result, split_list, class_metrics)
  }
}

#' @keywords internal
get_binary_perf <- function(split_list=NULL) {
  function(result) {
    performance(result, split_list)
  }
}

#' @keywords internal
get_regression_perf <- function(split_list=NULL) {

  function(result) {
    performance(result, split_list)
  }
}


#' @keywords internal
get_custom_perf <- function(fun, split_list) {
  function(result) {
    custom_performance(result, fun, split_list)
  }
}


#' @keywords internal
compute_performance.mvpa_model <- function(obj, result) {
  obj$performance(result)
}

#' @export
tune_grid.mvpa_model <- function(obj, x,y,len=1) {
  if (is.null(obj$tune_grid)) {
    obj$tune_grid <- obj$model$grid(x,y,len)
    obj$tune_grid
  } else {
    obj$tune_grid
  }
}


#' @export
select_features.mvpa_model <- function(obj, X, Y,...) {
  if (!is.null(obj$feature_selector)) {
    select_features(obj$feature_selector, X, Y,...)
  } else {
    ## todo check 'length' function in ROIVector
    rep(TRUE, length(X))
  }
}


#' @export
crossval_samples.mvpa_model <- function(obj,data,y,...) { 
  crossval_samples(obj$crossval,data,y) 
}

#' @export
has_test_set.mvpa_model <- function(obj) {
  # Use the stored flag rather than checking for existence of design$y_test
  isTRUE(obj$has_test_set)
}

#' @export
has_test_set.model_spec <- function(obj) {
  # Use the stored flag rather than checking for existence of design$y_test
  isTRUE(obj$has_test_set)
}



#' @export
y_train.mvpa_model <- function(obj) y_train(obj$design)


#' @export
y_test.mvpa_model <- function(obj) y_test(obj$design)


#' @param name A character string indicating the name of the model.
#' @param dataset An `mvpa_dataset` instance.
#' @param design An `mvpa_design` instance.
#' @param return_predictions A \code{logical} indicating whether to return row-wise predictions for each voxel set (defaults to TRUE).
#' @param tune_reps The number of replications used during parameter tuning. Only relevant if `tune_grid` is supplied.
#' @param ... Additional arguments to be passed to the model, including `has_test_set` flag.
#' @noRd
#' @export
create_model_spec <- function(name, dataset, design, return_predictions=FALSE, 
                              compute_performance=FALSE, tune_reps=FALSE, ...) {
  # Automatically detect test set presence from dataset and design if not passed as parameter
  args <- list(...)
  if (is.null(args$has_test_set)) {
    args$has_test_set <- !is.null(dataset$test_data) && !is.null(design$y_test)
  }
  
  ret <- list(name=name, dataset=dataset, design=design, 
              return_predictions=return_predictions, compute_performance=compute_performance, 
              tune_reps=tune_reps)
  
  # Add the remaining arguments
  ret <- c(ret, args)
  
  class(ret) <- c(name, "model_spec", "list")
  ret
}

#' Create an MVPA Model
#'
#' Create an MVPA model based on a caret-based classification or regression model.
#'
#' @param model A caret-based classification or regression model.
#' @param dataset An `mvpa_dataset` instance.
#' @param design An `mvpa_design` instance.
#' @param model_type A character string indicating the problem type: "classification" or "regression".
#' @param crossval An optional `cross_validation` instance.
#' @param feature_selector An optional `feature_selector` instance.
#' @param tune_grid An optional parameter tuning grid as a `data.frame`.
#' @param tune_reps The number of replications used during parameter tuning. Only relevant if `tune_grid` is supplied.
#' @param performance An optional custom function for computing performance metrics.
#' @param class_metrics A logical flag indicating whether to compute performance metrics for each class.
#' @param compute_performance A \code{logical} indicating whether to compute and store performance measures for each voxel set (defaults to TRUE).
#' @param return_predictions A \code{logical} indicating whether to return row-wise predictions for each voxel set (defaults to TRUE).
#' @param return_fits A \code{logical} indicating whether to return the model fit for each voxel set (defaults to FALSE).
#'
#' @export
#'
#' @details
#'
#' If `performance` is supplied, it must be a function that takes one argument and returns a named list of scalar values. 
#' The argument the function takes is a class deriving from `classification_result` appropriate for the problem at hand.
#' See example below.
#'
#' @examples
#'
#' mod <- load_model("sda")
#' traindat <- neuroim2::NeuroVec(array(rnorm(6*6*6*100), c(6,6,6,100)), neuroim2::NeuroSpace(c(6,6,6,100)))
#' mask <- neuroim2::LogicalNeuroVol(array(rnorm(6*6*6)>-.2, c(6,6,6)), neuroim2::NeuroSpace(c(6,6,6)))
#'
#' mvdset <- mvpa_dataset(traindat,mask=mask)
#' design <- data.frame(fac=rep(letters[1:4], 25), block=rep(1:10, each=10))
#' cval <- blocked_cross_validation(design$block)
#' mvdes <- mvpa_design(design, ~ fac, block_var=~block)
#'
#' custom_perf <- function(result) {
#'   c(accuracy=sum(result$observed == result$predicted)/length(result$observed))
#' }
#' mvpmod <- mvpa_model(mod, dataset=mvdset, design=mvdes, crossval=cval, performance=custom_perf)
#' ret <- run_searchlight(mvpmod)
#' stopifnot("accuracy" %in% names(ret))
mvpa_model <- function(model, 
                       dataset,
                       design,
                       model_type=c("classification", "regression"), 
                       crossval=NULL, 
                       feature_selector=NULL, 
                       tune_grid=NULL, 
                       tune_reps=15,
                       performance=NULL,
                       class_metrics=TRUE,
                       compute_performance=TRUE,
                       return_predictions=TRUE,
                       return_fits=FALSE) {
                       
  
  assert_that(!is.null(model$fit))
  
  assert_that(inherits(design, "mvpa_design"))
  assert_that(inherits(dataset, "mvpa_dataset"))
  assert_that(is.logical(class_metrics))
  
  if (is.null(dataset$test_data) && !is.null(design$y_test)) {
    stop("mvpa_model: design has `y_test` dataset must have `test_data`")
  }
  
  if (!is.null(dataset$test_data) && is.null(design$y_test)) {
    stop("mvpa_model: if dataset has `test_data` design must have `y_test`")
  }
  
  # Determine if we have a test set based on both dataset and design
  has_test <- !is.null(design$y_test) && !is.null(dataset$test_data)
  
  perf <- if (!is.null(performance) && is.function(performance)) {
    #assert_that(is.function(performance)) 
    get_custom_perf(performance, design$split_groups)
  } else if (is.numeric(design$y_train)) {
    get_regression_perf(design$split_groups)
  } else if (length(levels(design$y_train)) > 2) {
    get_multiclass_perf(design$split_groups, class_metrics)
  } else if (length(levels(design$y_train)) == 2) {
    get_binary_perf(design$split_groups)
  } else {
    flog.info("class of design$ytrain: %s", class(design$ytrain))
    stop("performance method not found")
  }
  

  model_type <- match.arg(model_type)
  
  if (is.null(crossval) && !is.null(design$block_var)) {
    crossval <- blocked_cross_validation(design$block_var)
  }
  
  assertthat::assert_that(!is.null(crossval))
  
  ## TODO check that crossval is compatible with design
  ## TODO check that mvpa_design is compatible with mvpa_dataset (n training obs == length(y_train))
  
  #assert_that(length(y_train(design)) == )
  ret <- create_model_spec("mvpa_model", 
              dataset=dataset,
              design=design,
              model=model,
              model_type=model_type,
              model_name=model$label,
              tune_grid=tune_grid,
              tune_reps=tune_reps,
              feature_selector=feature_selector,
              crossval=crossval,
              performance=perf,
              compute_performance=compute_performance,
              return_predictions=return_predictions,
              return_fits=return_fits,
              has_test_set=has_test)  # Add flag for test set presence
  
  ret
  
}


#' @export
has_crossval.mvpa_model <- function(obj) {
  !is.null(obj$crossval)
}

#' @export
has_crossval.model_spec <- function(obj) {
  !is.null(obj$crossval)
}

#' @export
#' @method print mvpa_model
print.mvpa_model <- function(x,...) {
  cat("mvpa_model object. \n")
  cat("model: ", x$model_name, "\n")
  cat("model type: ", x$model_type, "\n")
  if (!is.null(x$tune_grid)) {
    cat("tune_reps: ", x$tune_reps, "\n")
    cat("tune_grid: ", "\n")
    print(x$tune_grid)
  }
  
  print(x$crossval)
  print(x$dataset)
  print(x$design)
}
  

#' Default method for strip_dataset generic
#' 
#' Removes the `dataset` element from a model specification object.
#' 
#' @param obj The model specification object.
#' @param ... Additional arguments (ignored).
#' @return The object with `obj$dataset` set to NULL.
#' @export
strip_dataset.default <- function(obj, ...) {
  if (!is.null(obj$dataset)) {
    futile.logger::flog.debug("Stripping dataset from model specification.")
    obj$dataset <- NULL
  } else {
    futile.logger::flog.debug("Dataset already NULL or missing in model specification.")
  }
  obj
}
</file>

<file path="R/vector_rsa_model.R">
#' Construct a design for a vectorized RSA model
#'
#' This function constructs a design for an RSA model using a single distance matrix, labels, and blocks.
#'
#' @param D A representational dissimilarity matrix with row.names indicating labels.
#' @param labels character vector of labels corresponding to rows in another dataset X.
#' @param block_var A vector indicating the block (strata) each label belongs to. Must be the same length as `labels`.
#'
#' @return A list containing the elements of the RSA design, class attributes "vector_rsa_design" and "list".
#' @details
#' The function verifies that all `labels` appear in `rownames(D)`. It also creates an expanded version
#' of the dissimilarity matrix (`Dexpanded`) matching the order of `labels`, and precomputes
#' cross-block information for later use.
#' 
#' @export
vector_rsa_design <- function(D, labels, block_var) {
  # Verify that all labels are present in row.names of D
  assertthat::assert_that(
    all(labels %in% rownames(D)),
    msg = "All labels must be present in the row.names of D."
  )
  
  # Fail fast if the number of rows in D is not equal to length(labels)
  #assertthat::assert_that(
  #  nrow(D) == length(labels),
  #  msg = "The number of rows in D must match the number of labels."
  #)
  
  # Ensure length consistency
  assertthat::assert_that(
    length(labels) == length(block_var),
    msg = "Length of labels and block_var must match."
  )
  
  # Build the design object
  design <- list(
    D = D,
    labels = labels,
    block = block_var
  )
  
  # Attach precomputed expansions
  design$model_mat <- vector_rsa_model_mat(design)
  
  class(design) <- c("vector_rsa_design", "list")
  return(design)
}


#' @noRd
vector_rsa_model_mat <- function(design) {
  # Reorder D to match 'labels' in the correct order
  row_idx <- match(design$labels, rownames(design$D))
  Dexpanded <- design$D[row_idx, row_idx, drop=FALSE]
  
  # Precompute cross-block data: for each block, which labels/indices are outside that block
  unique_blocks <- sort(unique(design$block))
  cross_block_data <- lapply(unique_blocks, function(b) {
    # Indices of everything *not* in block b
    inds_not_b <- which(design$block != b)
    list(
      other_labels = design$labels[inds_not_b],
      indices      = inds_not_b,
      block        = b
    )
  })
  names(cross_block_data) <- unique_blocks
  
  # Return as a list
  list(
    Dexpanded        = Dexpanded,
    cross_block_data = cross_block_data
  )
}


#' Create a vectorized RSA model
#'
#' This function integrates a vector_rsa_design and an mvpa_dataset to create a vectorized RSA model.
#' 
#' @param dataset An \code{mvpa_dataset} object.
#' @param design A \code{vector_rsa_design} object.
#' @param distfun A \code{distfun} (distance function) for computing pairwise dissimilarities among image rows.
#' @param rsa_simfun A character string specifying the similarity function to use for RSA, 
#'                   one of \code{"pearson"} or \code{"spearman"}.
#' @param nperm Integer, number of permutations for statistical testing (default: 0).
#' @param save_distributions Logical, whether to save full permutation distributions (default: FALSE).
#' @param return_predictions Logical, whether to return per-observation similarity scores (default: FALSE).
#'
#' @return A \code{vector_rsa_model} object (S3 class) containing references to the dataset, design, and function parameters.
#'
#' @details
#' The model references the already-precomputed cross-block data from the design. 
#' If `return_predictions` is TRUE, the output of `run_regional` or `run_searchlight` 
#' will include a `prediction_table` tibble containing the observation-level RSA scores.
#' 
#' @export
vector_rsa_model <- function(dataset, design, 
                           distfun = cordist(), 
                           rsa_simfun = c("pearson", "spearman"),
                           nperm=0, 
                           save_distributions=FALSE,
                           return_predictions=FALSE) {
  rsa_simfun <- match.arg(rsa_simfun)
  
  assertthat::assert_that(inherits(dataset, "mvpa_dataset"))
  assertthat::assert_that(inherits(design, "vector_rsa_design"),
                          msg = "Input must be a 'vector_rsa_design' object.")
  
  # Create the model spec, passing permutation and prediction parameters
  create_model_spec(
    "vector_rsa_model",
    dataset = dataset,
    design  = design,
    distfun = distfun,
    rsa_simfun = rsa_simfun,
    nperm = nperm,
    compute_performance = TRUE, # Assume performance is always computed
    save_distributions = save_distributions,
    return_predictions = return_predictions # Pass the new flag
  )
}


#' Train a vector RSA model
#'
#' @param obj An object of class \code{vector_rsa_model}.
#' @param train_dat A data frame or matrix representing the training subset (e.g., voxel intensities).
#' @param y Not used in vector RSA (here for consistency with other train_model generics).
#' @param indices The spatial indices of the training data (ROI, searchlight, etc.).
#' @param ... Additional arguments.
#' 
#' @return A structure containing "scores" or similar second-order similarity results.
#' @export
train_model.vector_rsa_model <- function(obj, train_dat, y, indices, ...) {
  # "Training" here is effectively computing RSA-based metrics
  scores <- compute_trial_scores(obj, train_dat)
  return(scores)
}


#' Compute trial scores for vector RSA
#'
#' @keywords internal
#' @noRd
compute_trial_scores <- function(obj, X) {
 
  # Retrieve relevant design expansions
  precomputed <- obj$design$model_mat
  dissimilarity_matrix <- precomputed$Dexpanded
  cross_block_data     <- precomputed$cross_block_data
  
  # Convert X to matrix if needed
  X <- as.matrix(X)
  
  # This function computes second-order similarity:
  #   second_order_similarity(distfun, X, D, block_var, rsa_simfun)
  # 'distfun' can compute distances on X, 'D' is the reference matrix
  # 'block_var' is in obj$design$block, and 'rsa_simfun' is the correlation method.
  
  second_order_similarity(
    distfun   = obj$distfun,
    X         = X,
    Dref      = dissimilarity_matrix,
    block_var = obj$design$block,
    method    = obj$rsa_simfun
  )
}


#' Compute Second-Order Similarity Scores
#'
#' Calculates correlation-based \emph{second order similarity} between:
#' \itemize{
#'   \item A \strong{full NxN distance matrix} computed from \code{X} via \code{distfun}, and 
#'   \item A \code{Dref} matrix (the "reference" dissimilarities).
#' }
#' For each row \code{i}, this excludes same-block comparisons by selecting \code{which(block_var != block_var[i])}.
#'
#' @param distfun An S3 distance object (see \code{\link{create_dist}}) 
#'   specifying how to compute a pairwise distance matrix from \code{X}.
#' @param X A numeric matrix (rows = observations, columns = features).
#' @param Dref A numeric NxN reference matrix of dissimilarities (e.g., from an ROI mask or a prior).
#' @param block_var A vector indicating block/group memberships for each row in \code{X}.
#' @param method Correlation method: "pearson" or "spearman".
#'
#' @return A numeric vector of length \code{nrow(X)}, where each entry is
#' the correlation (using \code{method}) between \code{distance_matrix[i, valid]} and
#' \code{Dref[i, valid]}, with \code{valid = which(block_var != block_var[i])}.
#'
#' @details
#' This function first calls \code{pairwise_dist(distfun, X)}, obtaining an NxN matrix 
#' of \emph{all} pairwise distances. It does not do block-based exclusion internally. 
#' Instead, for each row \code{i}, it excludes same-block rows from the correlation 
#' by subsetting the distances to \code{valid_indices}.
#'
#' @examples
#' # Suppose we have X (10x5), a reference D (10x10), block var, and a correlation distfun:
#' X <- matrix(rnorm(50), 10, 5)
#' D <- matrix(runif(100), 10, 10)
#' block <- rep(1:2, each=5)
#' dist_obj <- cordist(method="pearson")
#' scores <- second_order_similarity(dist_obj, X, D, block, method="spearman")
#'
#' @export
second_order_similarity <- function(distfun, X, Dref, block_var, method=c("pearson", "spearman")) {
  method <- match.arg(method)
  
  # 1) Compute a full NxN distance matrix from X
  distance_matrix <- pairwise_dist(distfun, X)
  
  n <- nrow(X)
  scores <- numeric(n)
  
  # 2) For each row i, exclude same-block comparisons
  for (i in seq_len(n)) {
    valid_indices <- which(block_var != block_var[i])
    if (length(valid_indices) > 0) {
      x_vec <- distance_matrix[i, valid_indices]
      ref_vec <- Dref[i, valid_indices]
      if (length(x_vec) > 1) {
        scores[i] <- cor(x_vec, ref_vec, method = method)
      } else {
        scores[i] <- NA
      }
    } else {
      scores[i] <- NA
    }
  }
  
  scores
}


#' Print Method for vector_rsa_model
#'
#' @param x An object of class \code{vector_rsa_model}.
#' @param ... Additional arguments (ignored).
#' @export
print.vector_rsa_model <- function(x, ...) {
  # Ensure that crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define styles using crayon
  header_style  <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style    <- crayon::white
  number_style  <- crayon::green
  method_style  <- crayon::bold$blue
  label_style   <- crayon::magenta
  italic_style  <- crayon::italic$blue
  
  # Create a border line for header/footer
  border <- header_style(strrep("=", 50))
  
  # Header
  cat(border, "\n")
  cat(header_style("          Vectorized RSA Model          \n"))
  cat(border, "\n\n")
  
  # Dataset information
  if (!is.null(x$dataset)) {
    dims <- dim(x$dataset$train_data)
    dims_str <- if (!is.null(dims)) paste(dims, collapse = " x ") else "Unknown"
    cat(section_style("Dataset:\n"))
    cat(info_style("  ├─ Data Dimensions: "), number_style(dims_str), "\n")
    if (!is.null(x$dataset$mask)) {
      cat(info_style("  └─ Mask Length:     "), number_style(length(x$dataset$mask)), "\n")
    } else {
      cat(info_style("  └─ Mask:            "), crayon::red("None"), "\n")
    }
    cat("\n")
  }
  
  # Design information
  if (!is.null(x$design)) {
    n_labels <- length(x$design$labels)
    n_blocks <- length(unique(x$design$block))
    Ddim <- dim(x$design$model_mat$Dexpanded)
    cat(section_style("Design:\n"))
    cat(info_style("  ├─ Number of Labels:    "), number_style(n_labels), "\n")
    cat(info_style("  ├─ Number of Blocks:    "), number_style(n_blocks), "\n")
    cat(info_style("  └─ Dissimilarity Matrix:"), number_style(paste0(Ddim[1], " x ", Ddim[2])), "\n\n")
  }
  
  # Model Specification
  cat(section_style("Model Specification:\n"))
  cat(info_style("  ├─ Distance Function:   "), method_style(deparse(substitute(x$distfun))), "\n")
  cat(info_style("  └─ RSA Similarity Func: "), method_style(x$rsa_simfun), "\n\n")
  
  # Footer
  cat(border, "\n")
}

#' Print Method for vector_rsa_design
#'
#' @param x A vector_rsa_design object.
#' @param ... Additional arguments (ignored).
#' @export
print.vector_rsa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define crayon styles
  header_style  <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style    <- crayon::white
  number_style  <- crayon::green
  label_style   <- crayon::magenta
  italic_style  <- crayon::italic$blue
  
  # Create a border line
  border <- header_style(strrep("=", 50))
  
  # Header
  cat(border, "\n")
  cat(header_style("         Vectorized RSA Design          \n"))
  cat(border, "\n\n")
  
  # Distance Matrix Information
  if (!is.null(x$D)) {
    Ddim <- dim(x$D)
    cat(section_style("Distance Matrix:\n"))
    cat(info_style("  ├─ Original Dimensions: "), number_style(paste0(Ddim[1], " x ", Ddim[2])), "\n")
  }
  
  # Labels Information
  if (!is.null(x$labels)) {
    n_labels <- length(x$labels)
    cat(section_style("Labels:\n"))
    cat(info_style("  ├─ Total Labels: "), number_style(n_labels), "\n")
    # Display the first few labels as a sample
    sample_labels <- paste(head(x$labels, 5), collapse = ", ")
    if(n_labels > 5) sample_labels <- paste0(sample_labels, ", ...")
    cat(info_style("  └─ Sample:         "), label_style(sample_labels), "\n")
  }
  
  # Block Information
  if (!is.null(x$block)) {
    unique_blocks <- sort(unique(x$block))
    n_blocks <- length(unique_blocks)
    cat(section_style("Blocks:\n"))
    cat(info_style("  ├─ Number of Blocks: "), number_style(n_blocks), "\n")
    cat(info_style("  └─ Block Labels:     "), label_style(paste(unique_blocks, collapse = ", ")), "\n")
  }
  
  # Expanded D Matrix Information
  if (!is.null(x$model_mat) && !is.null(x$model_mat$Dexpanded)) {
    Dexp_dim <- dim(x$model_mat$Dexpanded)
    cat(section_style("Expanded D Matrix:\n"))
    cat(info_style("  └─ Dimensions:       "), number_style(paste0(Dexp_dim[1], " x ", Dexp_dim[2])), "\n")
  }
  
  # Footer
  cat("\n", border, "\n")
}

#' Evaluate model performance for vector RSA
#'
#' Computes the mean second-order similarity score and handles permutation testing.
#'
#' @param object The vector RSA model specification.
#' @param predicted Ignored (vector RSA doesn't predict in the typical sense).
#' @param observed The computed second-order similarity scores (vector from train_model).
#' @param nperm Number of permutations from the model spec.
#' @param save_distributions Logical, whether to save full permutation distributions.
#' @param ... Additional arguments.
#'
#' @return A list containing the mean RSA score (`rsa_score`), raw scores, and 
#'   optional permutation results (`p_values`, `z_scores`, `permutation_distributions`).
#' @importFrom stats sd
#' @export
evaluate_model.vector_rsa_model <- function(object,
                                             predicted, # Ignored
                                             observed,  # These are the scores from train_model
                                             nperm = 0,
                                             save_distributions = FALSE,
                                             ...) 
{
  # Primary metric: mean of the second-order similarity scores
  mean_rsa_score <- mean(observed, na.rm = TRUE)

  perm_results <- NULL
  if (nperm > 0) {
    # Retrieve original training data X
    X_orig <- object$dataset$train_data
    perm_means <- numeric(nperm)
    # Perform permutations
    for (p in seq_len(nperm)) {
      # Permute the rows of X (shuffles condition assignments)
      X_perm <- X_orig[sample(nrow(X_orig)), , drop = FALSE]
      # Recompute trial scores under the null
      scores_perm <- compute_trial_scores(object, X_perm)
      # Store the mean RSA score for this permutation
      perm_means[p] <- mean(scores_perm, na.rm = TRUE)
    }
    # Calculate p-value (one-sided: how many null >= observed)
    count_ge <- sum(perm_means >= mean_rsa_score, na.rm = TRUE)
    p_val <- (count_ge + 1) / (nperm + 1)
    # Compute z-score
    perm_mean <- mean(perm_means, na.rm = TRUE)
    perm_sd   <- sd(perm_means, na.rm = TRUE)
    z_score <- if (!is.na(perm_sd) && perm_sd > 0) {
      (mean_rsa_score - perm_mean) / perm_sd
    } else {
      NA_real_
    }
    # Assemble results
    perm_list <- list(
      p_values = setNames(p_val, "rsa_score"),
      z_scores = setNames(z_score, "rsa_score")
    )
    if (save_distributions) {
      perm_list$permutation_distributions <- list(
        rsa_score = perm_means
      )
    }
    perm_results <- perm_list
  }

  list(
    rsa_score           = mean_rsa_score,
    permutation_results = perm_results
  )
}
#' Merge results for vector RSA model
#'
#' Aggregates results (scores) and calls evaluate_model.
#' Vector RSA typically doesn't involve folds in the same way as classifiers,
#' so this mainly formats the output of train_model for the specific ROI/sphere.
#'
#' @param obj The vector RSA model specification (contains nperm etc.).
#' @param result_set A tibble from the processor. Expected to contain the output
#'   of `train_model.vector_rsa_model` (the scores vector) likely within `$result[[1]]`.
#' @param indices Voxel indices for the current ROI/searchlight sphere.
#' @param id Identifier for the current ROI/searchlight center.
#' @param ... Additional arguments.
#'
#' @return A tibble row with the final performance metrics for the ROI/sphere.
#' @importFrom tibble tibble
#' @importFrom futile.logger flog.error flog.warn
#' @export
merge_results.vector_rsa_model <- function(obj, result_set, indices, id, ...) {
  
  # Check for errors from previous steps (processor/train_model)
  if (any(result_set$error)) {
    emessage <- result_set$error_message[which(result_set$error)[1]]
    # Return standard error tibble structure
    return(
      tibble::tibble(
        result       = list(NULL), 
        indices      = list(indices),
        performance  = list(NULL), 
        id           = id,
        error        = TRUE,
        error_message= emessage
      )
    )
  }
  
  # Extract the scores computed by train_model. 
  if (!"result" %in% names(result_set) || length(result_set$result) == 0 || is.null(result_set$result[[1]])) {
     error_msg <- "merge_results (vector_rsa): result_set missing or has NULL/empty 'result' field where scores were expected."
     futile.logger::flog.error("ROI/Sphere ID %s: %s", id, error_msg)
     # Create NA performance matrix
     perf_names <- "rsa_score"
     if (obj$nperm > 0) {
         perf_names <- c(perf_names, "p_rsa_score", "z_rsa_score")
     }
     perf_mat <- matrix(NA_real_, nrow=1, ncol=length(perf_names), 
                        dimnames=list(NULL, perf_names))
     return(tibble::tibble(result=list(NULL), indices=list(indices), performance=list(perf_mat), 
                           id=id, error=TRUE, error_message=error_msg))
  }
  
  scores <- result_set$result[[1]]
  
  # Validate the extracted scores
  if (!is.numeric(scores)) {
      error_msg <- sprintf("merge_results (vector_rsa): Extracted scores are not numeric for ROI/Sphere ID %s.", id)
      futile.logger::flog.error(error_msg)
      # Create NA performance matrix
      perf_names <- "rsa_score"
      if (obj$nperm > 0) {
          perf_names <- c(perf_names, "p_rsa_score", "z_rsa_score")
      }
      perf_mat <- matrix(NA_real_, nrow=1, ncol=length(perf_names), 
                         dimnames=list(NULL, perf_names))
      return(tibble::tibble(result=list(NULL), indices=list(indices), performance=list(perf_mat), 
                            id=id, error=TRUE, error_message=error_msg))
  }
  
  # Call evaluate_model to compute summary performance and permutations
  perf <- evaluate_model.vector_rsa_model(
    object    = obj,           
    predicted = NULL,          
    observed  = scores,        
    nperm     = obj$nperm,     
    save_distributions = obj$save_distributions
  )
  
  # --- Collate performance matrix --- 
  base_metrics <- c(perf$rsa_score)
  base_names <- c("rsa_score")
  
  if (!is.null(perf$permutation_results)) {
      perm_p_values <- perf$permutation_results$p_values
      perm_z_scores <- perf$permutation_results$z_scores
      if (is.null(names(perm_p_values)) || is.null(names(perm_z_scores))){
           p_names <- paste0("p_", base_names)
           z_names <- paste0("z_", base_names)
      } else {
          p_names <- paste0("p_", names(perm_p_values))
          z_names <- paste0("z_", names(perm_z_scores))
      }
      perf_values <- c(base_metrics, perm_p_values, perm_z_scores)
      perf_names <- c(base_names, p_names, z_names)
  } else {
      perf_values <- base_metrics
      perf_names <- base_names
  }
  perf_mat <- matrix(perf_values, nrow = 1, ncol = length(perf_values), dimnames = list(NULL, perf_names))
  perf_mat <- perf_mat[, colSums(is.na(perf_mat)) < nrow(perf_mat), drop = FALSE]

  # --- Prepare results structure based on return_predictions flag --- 
  result_data <- if (isTRUE(obj$return_predictions)) {
      # Return scores structured for later assembly into prediction_table
      # Wrap scores in a list with a standard name
      list(rsa_scores = scores)
  } else {
      NULL # Return NULL if predictions are not requested
  }
  
  # Return the final tibble structure expected by the framework
  tibble::tibble(
    result      = list(result_data), # Store list(rsa_scores=scores) or NULL here
    indices     = list(indices),
    performance = list(perf_mat),
    id          = id,
    error       = FALSE,
    error_message = "~" 
  )
}
</file>

<file path="R/allgeneric.R">
#' Get Unique Region IDs
#'
#' Extract unique region IDs from a region mask, handling both volume and surface data.
#'
#' @param region_mask A region mask object (NeuroVol or NeuroSurface)
#' @param ... Additional arguments passed to methods
#'
#' @return A sorted vector of unique region IDs greater than 0
#' @keywords internal
get_unique_regions <- function(region_mask, ...) {
  UseMethod("get_unique_regions")
}

#' Strip Dataset from Model Specification
#' 
#' Removes the potentially large dataset component from a model specification object
#' to avoid copying it during parallel processing.
#' 
#' @param obj The model specification object.
#' @param ... Additional arguments.
#' @return The model specification object with the `dataset` element removed or set to NULL.
#' @export
strip_dataset <- function(obj, ...) {
  UseMethod("strip_dataset")
} 

#' Select Features
#'
#' Given a \code{feature_selection} specification object and a dataset, returns the set of selected features as a binary vector.
#'
#' @param obj The \code{feature_selection} object specifying the feature selection method and its parameters.
#' @param X The dataset containing the training features. This can be a matrix or a \code{ROIVolume} or \code{ROISurface} object.
#' @param Y The dependent variable as a factor or numeric variable.
#' @param ... Additional arguments to be passed to the method-specific function.
#' 
#' @return A logical vector indicating the columns of \code{X} matrix that were selected.
#' 
#' @examples 
#' fsel <- feature_selector("FTest", "top_k", 2)
#' coords <- rbind(c(1,1,1), c(2,2,2), c(3,3,3))
#' ROI <- neuroim2::ROIVec(neuroim2::NeuroSpace(c(10,10,10)), coords=coords, matrix(rnorm(100*3), 100, 3))
#' Y <- factor(rep(c("a", "b"), each=50))
#' featureMask <- select_features(fsel, neuroim2::values(ROI), Y)
#' sum(featureMask) == 2
#'
#' fsel2 <- feature_selector("FTest", "top_p", .1)
#' featureMask <- select_features(fsel2, neuroim2::values(ROI), Y)
#' 
#' @export
select_features <- function(obj, X, Y, ...) {
  UseMethod("select_features")
}

#' Format Result Object
#'
#' @param obj The result object to be formatted.
#' @param result The result object to be formatted.
#' @param error_message An optional error message.
#' @param ... Additional arguments to be passed to the method-specific function.
#' @export
format_result <- function(obj, result, error_message, ...) {
  UseMethod("format_result")
}


#' Merge Multiple Results
#'
#' @param obj The base object containing merge specifications
#' @param result_set List of results to be merged
#' @param indices List of indices corresponding to each result
#' @param id Identifier for the merged result
#' @param ... Additional arguments passed to specific merge methods
#'
#' @return A merged result object containing:
#' \itemize{
#'   \item Combined results from all input objects
#'   \item Associated indices
#'   \item Merged metadata
#' }
#'
#' @export
merge_results <- function(obj, result_set, indices, id, ...) {
  UseMethod("merge_results")
}

#' Run Future
#'
#' Run a future-based computation defined by the object and frame.
#'
#' @param obj An object specifying the computation.
#' @param frame A data frame or environment containing data for the computation.
#' @param processor A function or object specifying how to process the frame.
#' @param ... Additional arguments passed to the method-specific function.
#' @noRd
run_future <- function(obj, frame, processor, ...) {
  UseMethod("run_future")
}

#' Process ROI
#'
#' Process a region of interest (ROI) with possible cross-validation.
#'
#' @param mod_spec The model specification object.
#' @param roi The region of interest data.
#' @param rnum A numeric or string identifier for the ROI.
#' @param ... Additional arguments passed to the method-specific function.
#' @keywords internal
#' @noRd
process_roi <- function(mod_spec, roi, rnum, ...) {
  UseMethod("process_roi")
}

#' Default Process ROI Method
#'
#' Default implementation for processing ROIs when no custom ROI processor is provided.
#'
#' @inheritParams process_roi
#' @keywords internal
#' @noRd
process_roi.default <- function(mod_spec, roi, rnum, ...) {
  if (!is.null(mod_spec$process_roi)) {
    mod_spec$process_roi(mod_spec, roi, rnum, ...)
  } else if (has_test_set(mod_spec)) {
    external_crossval(mod_spec, roi, rnum, ...)
  } else if (has_crossval(mod_spec)) {
    #print("internal crossval")
    internal_crossval(mod_spec, roi, rnum, ...)
  } else {
    process_roi_default(mod_spec, roi, rnum, ...)
  }
}

#' Default ROI Processing Helper
#'
#' @param mod_spec The model specification object.
#' @param roi The ROI containing training data.
#' @param rnum The region number or identifier.
#' @param ... Additional arguments passed to specific methods.
#' @keywords internal
#' @noRd
#' @importFrom neuroim2 indices values
#' @importFrom tibble as_tibble tibble
#' @importFrom futile.logger flog.warn
process_roi_default <- function(mod_spec, roi, rnum, ...) {
  # This helper is called by process_roi.default for models 
  # that don't use internal cross-validation.
  # It runs train_model and then passes the result to merge_results
  # for final performance computation and formatting.
  #browser()
  xtrain <- tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair=.name_repair)
  ind <- indices(roi$train_roi)
  
  # Run train_model
  # Need to pass y=NULL and indices=ind based on train_model.vector_rsa_model signature
  train_result_obj <- try(train_model(mod_spec, xtrain, y = NULL, indices=ind, ...)) 
  
  # Prepare a result set structure for merge_results
  if (inherits(train_result_obj, "try-error")) {
    # If training failed, create an error result set for merge_results
    error_msg <- attr(train_result_obj, "condition")$message
    result_set <- tibble::tibble(
      result = list(NULL), # No result from train_model
      error = TRUE,
      error_message = ifelse(is.null(error_msg), "Unknown training error", error_msg)
      # We don't need to mimic all columns internal_crossval might produce,
      # only what merge_results requires for error handling.
    )
     futile.logger::flog.warn("ROI %s: train_model failed: %s", rnum, error_msg)
     
  } else {
    # If training succeeded, create a success result set for merge_results
    # Store the *output* of train_model in the 'result' column. 
    # merge_results.vector_rsa_model expects the scores vector here.
     result_set <- tibble::tibble(
       result = list(train_result_obj), # Store train_model output here
       error = FALSE,
       error_message = "~"
       # merge_results will compute the 'performance' column.
     )
  }
  
  # Call merge_results to compute final performance and format the output tibble
  # merge_results handles both success and error cases based on result_set$error
  final_result <- merge_results(mod_spec, result_set, indices=ind, id=rnum)
  return(final_result)
}

#' Train Model
#'
#' Train a classification or regression model.
#'
#' @param obj The model specification object.
#' @param ... Additional arguments to be passed to the method-specific function.
#' @return A trained model object.
#' @export
train_model <- function(obj,...) {
  UseMethod("train_model")
}

#' Training Labels/Response Extraction
#'
#' Extract the training labels or response variable from an object.
#'
#' @param obj The object from which to extract the training response variable.
#' @export
y_train <- function(obj) {
  UseMethod("y_train")
}

#' Test Labels/Response Extraction
#'
#' Extract the test labels or response variable from an object.
#'
#' @param obj The object from which to extract the test response variable.
#' @export
y_test <- function(obj) {
  UseMethod("y_test")
}

#' Test Design Extraction
#'
#' Return the design table associated with the test set from an object.
#'
#' @param obj The object from which to extract the test design table.
#' @export
test_design <- function(obj) {
  UseMethod("test_design")
}

#' Fit Model
#'
#' Fit a classification or regression model.
#'
#' @param obj A model fitting object.
#' @param roi_x An ROI containing the training data.
#' @param y The response vector.
#' @param wts A set of case weights.
#' @param param Tuning parameters.
#' @param lev Factor levels (for classification).
#' @param last Logical indicating if this is the last iteration.
#' @param classProbs Logical indicating if class probabilities should be returned.
#' @param ... Additional arguments to be passed to the method-specific function.
#' 
#' @export
fit_model <- function(obj, roi_x, y, wts, param, lev=NULL, last=FALSE, classProbs=FALSE, ...) {
  UseMethod("fit_model")
}

#' Tune Grid Extraction
#'
#' Extract the parameter grid to optimize for a model.
#'
#' @param obj The model object.
#' @param x The training data.
#' @param y The response vector.
#' @param len The number of elements in the tuning grid.
tune_grid <- function(obj, x,y,len) {
  UseMethod("tune_grid")
}

#' Test Set Availability
#'
#' Check if an object has a test set available.
#'
#' @param obj The object to check for a test set.
#' @export
has_test_set <- function(obj) {
  UseMethod("has_test_set")
}

#' Requires cross-validation to be performed
#' @param obj The model object.
has_crossval <- function(obj) {
  UseMethod("has_crossval")
}

#' @export
has_crossval.default <- function(obj) {
  FALSE
}

#' Compute Performance Metrics
#'
#' Compute performance metrics (accuracy, AUC, RMSE, etc.) for classification/regression results.
#'
#' @param x The classification/regression result object to evaluate.
#' @param ... Additional arguments passed to method-specific performance functions.
#'
#' @return A list of performance metrics.
#' @export
performance <- function(x,...) {
  UseMethod("performance")
}

#' Compute Performance for an Object
#'
#' Delegates calculation of performance metrics to the appropriate method.
#'
#' @param obj The input object.
#' @param result The classification/regression result object to evaluate.
#'
#' @return A list of performance metrics.
#' @export
compute_performance <- function(obj, result) {
  UseMethod("compute_performance")
}

#' Merge Multiple Classification/Regression Results
#'
#' This function merges two or more classification/regression result objects.
#'
#' @param x The first classification/regression result object.
#' @param ... Additional classification/regression result objects.
#'
#' @return A single merged classification/regression result object.
#'
#' @export
merge_classif_results <- function(x, ...) {
  UseMethod("merge_classif_results")
}

#' Get Multiple Data Samples
#'
#' Extract multiple data samples based on a list of voxel/index sets from a dataset object.
#'
#' @param obj The input dataset object.
#' @param vox_list A list of vectors containing voxel indices to extract.
#'
#' @return A list of data samples.
#' @export
get_samples <- function(obj, vox_list) {
  UseMethod("get_samples")
}

#' Extract Sample from Dataset
#'
#' Extract a sample from a given dataset object.
#'
#' @param obj The input dataset object.
#' @param vox The voxel indices/coordinates.
#' @param ... Additional arguments to methods.
#'
#' @return A sample extracted from the dataset.
#' @export
data_sample <- function(obj, vox, ...) {
  UseMethod("data_sample")
}

#' Convert object to ROI
#'
#' Convert the provided object into an ROIVolume or ROISurface object.
#'
#' @param obj The object to be converted.
#' @param data The associated data object.
#' @param ... Additional arguments passed to methods.
#' @return An ROIVolume or ROISurface object.
#' @keywords internal
as_roi <- function(obj, data, ...) {
  UseMethod("as_roi")
}

#' Generate Searchlight Iterator
#'
#' Generate a searchlight iterator suitable for given data.
#'
#' @param obj The input dataset object.
#' @param ... Additional arguments to methods.
#'
#' @return A searchlight iterator object.
#' @export
get_searchlight <- function(obj, ...) {
  UseMethod("get_searchlight")
}

#' Wrap Output
#'
#' Wrap output values into a desired format.
#'
#' @param obj The object used to determine the wrapping method.
#' @param vals The values to be wrapped.
#' @param ... Additional arguments passed to methods.
#' @return A wrapped output object.
#' @keywords internal
wrap_output <- function(obj, vals, ...) {
  UseMethod("wrap_output")
}

#' Merge Predictions
#'
#' Combine predictions from multiple models on the same test set.
#'
#' @param obj1 The first object containing predictions.
#' @param rest Other objects containing predictions.
#' @param ... Additional arguments.
#' @return A combined object with merged predictions.
#' @export
merge_predictions <- function(obj1, rest, ...) {
  UseMethod("merge_predictions")
}

#' Extract Row-wise Subset of a Result
#'
#' Extract a subset of rows from a classification/regression result object.
#'
#' @param x The input result object.
#' @param indices Row indices to extract.
#'
#' @return A new result object with the specified rows.
#' @export
sub_result <- function(x, indices) {
  UseMethod("sub_result")
}

#' Get Number of Observations
#'
#' Retrieve the number of observations in an object.
#'
#' @param x The input object.
#' @return The number of observations.
#' @export
nobs <- function(x) {
  UseMethod("nobs")
}

#' Probability of Observed Class
#'
#' Extract the predicted probability for the observed class.
#'
#' @param x The object from which to extract the probability.
#' @return A vector of predicted probabilities.
#' @export
prob_observed <- function(x) {
  UseMethod("prob_observed")
}

#' Number of Response Categories
#'
#' Get the number of response categories or levels.
#'
#' @param x The object from which to extract the number of categories.
#' @return The number of response categories.
#' @export
nresponses <- function(x) {
  UseMethod("nresponses")
}

#' @keywords internal
#' @noRd
.name_repair = ~ vctrs::vec_as_names(..., repair = "unique", quiet = TRUE)

#' Run Searchlight Analysis
#'
#' Execute a searchlight analysis using multivariate pattern analysis.
#'
#' @param model_spec A \code{mvpa_model} instance containing the model specifications
#' @param radius The searchlight radius in millimeters
#' @param method The type of searchlight, either 'randomized' or 'standard'
#' @param niter The number of searchlight iterations (used only for 'randomized' method)
#' @param ... Extra arguments passed to specific searchlight methods
#'
#' @return A named list of \code{NeuroVol} objects containing performance metrics (e.g., AUC) at each voxel location
#'
#' @examples
#' \donttest{
#'   # Generate sample dataset with categorical response
#'   dataset <- gen_sample_dataset(
#'     D = c(8,8,8),           # 8x8x8 volume
#'     nobs = 100,             # 100 observations
#'     response_type = "categorical",
#'     data_mode = "image",
#'     blocks = 3,             # 3 blocks for cross-validation
#'     nlevels = 2             # binary classification
#'   )
#'   
#'   # Create cross-validation specification using blocks
#'   cval <- blocked_cross_validation(dataset$design$block_var)
#'   
#'   # Load the SDA classifier (Shrinkage Discriminant Analysis)
#'   model <- load_model("sda_notune")
#'   
#'   # Create MVPA model
#'   mspec <- mvpa_model(
#'     model = model,
#'     dataset = dataset$dataset,
#'     design = dataset$design,
#'     model_type = "classification",
#'     crossval = cval
#'   )
#'   
#'   # Run searchlight analysis
#'   results <- run_searchlight(
#'     mspec,
#'     radius = 8,            # 8mm radius
#'     method = "standard"    # Use standard searchlight
#'   )
#'   
#'   # Results contain performance metrics
#'   # Access them with results$performance
#' }
#'
#' @export
run_searchlight <- function(model_spec, radius, method = c("standard", "randomized"), niter = NULL, ...) {
  UseMethod("run_searchlight")
}

#' Region of Interest Based MVPA Analysis
#'
#' Run a separate MVPA analysis for multiple disjoint regions of interest.
#'
#' @param model_spec A \code{mvpa_model} instance containing the model specifications
#' @param region_mask A \code{NeuroVol} or \code{NeuroSurface} object where each region is identified by a unique integer
#' @param coalesce_design_vars If \code{TRUE}, merges design variables into the prediction table (if present and generated). Default is \code{FALSE}.
#' @param processor An optional custom processor function for each region (ROI). If NULL (default), behavior depends on the \code{model_spec} class.
#' @param verbose If \code{TRUE}, print progress messages during iteration (default is \code{FALSE}).
#' @param ... Extra arguments passed to specific regional analysis methods (e.g., `return_fits`, `compute_performance`).
#'
#' @return A \code{regional_mvpa_result} object (list) containing:
#'   \item{performance_table}{A tibble of performance metrics for each region (if computed).}
#'   \item{prediction_table}{A tibble with detailed predictions for each observation/region (if generated).}
#'   \item{vol_results}{A list of volumetric maps representing performance metrics across space (if computed).}
#'   \item{fits}{A list of fitted model objects for each region (if requested via `return_fits=TRUE`).}
#'   \item{model_spec}{The original model specification object provided.} # Note: Original documentation said 'performance', clarified here.
#' 
#' @examples
#' \donttest{
#'   # Generate sample dataset (3D volume with categorical response)
#'   dataset <- gen_sample_dataset(
#'     D = c(10,10,10),       # Small 10x10x10 volume
#'     nobs = 100,            # 100 observations
#'     nlevels = 3,           # 3 classes
#'     response_type = "categorical",
#'     data_mode = "image",
#'     blocks = 3             # 3 blocks for cross-validation
#'   )
#'   
#'   # Create region mask with 5 ROIs
#'   region_mask <- NeuroVol(
#'     sample(1:5, size=length(dataset$dataset$mask), replace=TRUE),
#'     space(dataset$dataset$mask)
#'   )
#'   
#'   # Create cross-validation specification
#'   cval <- blocked_cross_validation(dataset$design$block_var)
#'   
#'   # Load SDA classifier (Shrinkage Discriminant Analysis)
#'   model <- load_model("sda_notune")
#'   
#'   # Create MVPA model
#'   mspec <- mvpa_model(
#'     model = model,
#'     dataset = dataset$dataset,
#'     design = dataset$design,
#'     model_type = "classification",
#'     crossval = cval,
#'     return_fits = TRUE    # Return fitted models
#'   )
#'   
#'   # Run regional analysis
#'   results <- run_regional(mspec, region_mask)
#'   
#'   # Access results
#'   head(results$performance)           # Performance metrics
#'   head(results$prediction_table)      # Predictions
#'   first_roi_fit <- results$fits[[1]]  # First ROI's fitted model
#' }
#'
#' @rdname run_regional-methods
#' @export
run_regional <- function(model_spec, region_mask, ...) {
  UseMethod("run_regional")
}

#' crossval_samples
#'
#' Apply a cross-validation scheme to split the data into training and testing sets.
#'
#' @param obj A cross-validation control object.
#' @param data A data frame containing the predictors.
#' @param y A vector containing the response variable.
#' @param ... Extra arguments passed to the specific cross-validation methods.
#'
#' @return A tibble containing training and testing sets for each fold.
#' @export
crossval_samples <- function(obj, data, y,...) { 
  UseMethod("crossval_samples") 
}

#' Generic Pairwise Distance Computation
#'
#' Compute pairwise distances between rows of X using a specified distance object.
#'
#' @param obj A distance object specifying the distance measure.
#' @param X A numeric matrix of data points (rows = samples).
#' @param ... Additional arguments passed to methods.
#'
#' @return A matrix or dist object of pairwise distances.
#' @keywords internal
#' @noRd
pairwise_dist <- function(obj, X,...) {
  UseMethod("pairwise_dist")
}

#' Filter Region of Interest (ROI)
#'
#' Filter an ROI by removing columns with missing values or zero std dev.
#'
#' @param roi A list containing the train and test ROI data.
#' @param ... Additional arguments passed to methods.
#'
#' @return A list with filtered train and test ROI data.
#' @keywords internal
filter_roi <- function(roi, ...) {
  UseMethod("filter_roi", roi$train_roi)
}
</file>

<file path="R/classifiers.R">
#' @keywords internal
#' @noRd
colHuber <- function(x, k=1.5, tol=1e-04) {
  mu <- matrixStats::colMedians(x)
  s <- matrixStats::colMads(x)
  n <- nrow(x)
  sapply(seq_along(mu), function(i) {
    repeat {
      yy <- pmin(pmax(mu[i] - k * s[i], x[, i]), mu[i] + k * s[i])
      mu1 <- sum(yy)/n
      if (abs(mu[i] - mu1) < tol * s[i]) 
        break
      mu[i] <- mu1
    }
    mu[i]
  })
}


#' Pre-defined MVPA Classification Models
#'
#' An environment containing custom classification models for MVPA analysis.
#'
#' @format An environment with the following models:
#' \describe{
#'   \item{corclass}{Correlation-based classifier using template matching with options (pearson, spearman, kendall)}
#'   \item{corsim}{Alias for corclass}
#'   \item{sda_notune}{Shrinkage Discriminant Analysis (SDA) without parameter tuning}
#'   \item{sda_boot}{SDA with bootstrap resampling and feature selection}
#'   \item{glmnet_opt}{Elastic net classifier (glmnet) with optimized alpha/lambda via EPSGO}
#'   \item{sparse_sda}{SDA with sparsity constraints and feature selection}
#'   \item{sda_ranking}{SDA with feature ranking and selection via higher criticism}
#'   \item{mgsda}{Multi-Group Sparse Discriminant Analysis}
#'   \item{lda_thomaz}{Modified LDA classifier for high-dimensional data}
#'   \item{hdrda}{High-Dimensional Regularized Discriminant Analysis}
#' }
#'
#' @details
#' Models are accessed via \code{load_model(name)}. Each implements caret's \code{fit}, \code{predict}, and \code{prob} methods.
#'
#' @examples
#' # Load simple SDA classifier
#' model <- load_model("sda_notune")
#'
#' # Load correlation classifier
#' model <- load_model("corclass")
#'
#' @export
MVPAModels <- new.env()

#' @importFrom MASS huber
#' @importFrom stats median
#' @noRd
corsimFit <- function(x, y, method, robust) {
  estimator <- if (robust) {
    function(vec) {
      h <- try(MASS::huber(vec), silent=TRUE)
      if (inherits(h, "try-error")) median(vec) else h$mu
    }
  } else {
    "mean"
  }
  
  lev <- levels(y)
  if (identical("mean", estimator)) {
    list(conditionMeans=group_means(x, 1, y), levs=lev, method=method, robust=robust)
  } else {
    list(conditionMeans = neuroim2::split_reduce(as.matrix(x), y, estimator), levs=lev, method=method, robust=robust)
  }
}

#' @keywords internal
#' @noRd
predict_corsimFit <- function(modelFit, newData) {
  cres <- cor(t(newData), t(modelFit$conditionMeans), method=modelFit$method)
  res <- max.col(cres)
  modelFit$levs[res]
}

#' @keywords internal
#' @noRd
prob_corsimFit <- function(modelFit, newData) {
  scores <- cor(t(newData), t(modelFit$conditionMeans), method=modelFit$method)
  mc <- scores[cbind(seq_len(nrow(scores)), max.col(scores, ties.method = "first"))]
  probs <- exp(sweep(scores, 1, mc, "-"))
  probs <- zapsmall(probs / rowSums(probs))
  colnames(probs) <- modelFit$levs
  probs
}

# PCA + LDA model
# Store lev after training so predictions can reference classes.
#' @keywords internal
#' @noRd
MVPAModels$pca_lda <- list(
  type = "Classification", 
  library = "MASS", 
  loop = NULL, 
  label="pca_lda",
  parameters=data.frame(parameters="ncomp", class="numeric", labels="ncomp"),
  grid=function(x, y, len = 5) data.frame(ncomp=1:len),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    scmat <- scale(as.matrix(x))
    pres <- svd::propack.svd(scmat, neig=param$ncomp)
    lda.fit <- lda(pres$u[, 1:param$ncomp, drop=FALSE], y)
    attr(lda.fit, "ncomp") <- param$ncomp
    attr(lda.fit, "pcfit") <- pres
    attr(lda.fit, "center") <- attr(scmat, "scaled:center")
    attr(lda.fit, "scale") <- attr(scmat, "scaled:scale")
    attr(lda.fit, "obsLevels") <- lev
    lda.fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    compind <- seq_len(attr(modelFit, "ncomp"))
    pcfit <- attr(modelFit, "pcfit")
    pcx <- scale(newdata, attr(modelFit, "center"), attr(modelFit, "scale")) %*% pcfit$v
    predict(modelFit, pcx[, compind, drop=FALSE])$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    compind <- seq_len(attr(modelFit, "ncomp"))
    pcfit <- attr(modelFit, "pcfit")
    pcx <- scale(newdata, attr(modelFit, "center"), attr(modelFit, "scale")) %*% pcfit$v
    predict(modelFit, pcx[, compind, drop=FALSE])$posterior
  }
)

# corclass and corsim
# Ensure lev is known from fit if needed. corsimFit stores lev internally.
#' @keywords internal
#' @noRd
MVPAModels$corclass <- list(
  type = "Classification", 
  library = "rMVPA", 
  label="corclass",
  loop = NULL, 
  parameters=data.frame(parameters=c("method", "robust"), class=c("character", "logical"),
                        label=c("correlation type: pearson, spearman, or kendall", "mean or huber")),
  grid=function(x, y, len = NULL) {
    if (is.null(len) || len == 1) {
      data.frame(method="pearson", robust=FALSE)
    } else {
      expand.grid(method=c("pearson","spearman","kendall"), robust=c(TRUE,FALSE))
    }
  },
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    corsimFit(x, y, as.character(param$method), param$robust)
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict_corsimFit(modelFit, as.matrix(newdata))
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    prob_corsimFit(modelFit, as.matrix(newdata))
  }
)

#' @keywords internal
#' @noRd
MVPAModels$corsim <- MVPAModels$corclass

# sda_notune
# Add levels storage
#' @keywords internal
#' @noRd
MVPAModels$sda_notune <- list(
  type = "Classification", 
  library = "sda", 
  label="sda_notune",
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    m <- sda::sda(Xtrain=as.matrix(x), L=y, verbose=FALSE, ...)
    m$obsLevels <- lev
    m
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata), verbose=FALSE)$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata), verbose=FALSE)$posterior
  }
)

# sda_boot
# Store lev and ensure predictions label correctly
#' @keywords internal
#' @noRd
MVPAModels$sda_boot <- list(
  type = "Classification", 
  library = "sda", 
  label="sda_boot",
  loop = NULL, 
  parameters=data.frame(parameters=c("reps","frac","lambda_min","lambda_max"),
                        class=c("numeric","numeric","numeric","numeric"),
                        label=c("boot reps","frac features","min lambda","max lambda")),
  grid=function(x, y, len = NULL) data.frame(reps=10, frac=1, lambda_min=.01, lambda_max=.8),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    mfits <- list()
    count <- 1
    failures <- 0
    
    split_y <- split(seq_along(y), y)
    lambda <- seq(param$lambda_min, param$lambda_max, length.out=param$reps)
    
    if (!all(sapply(split_y, function(yy) length(yy) > 0))) {
      stop("Every level in 'y' must have at least one instance for sda_boot.")
    }
    
    assertthat::assert_that(param$frac > 0, msg="sda_boot: 'frac' must be > 0")
    assertthat::assert_that(param$reps > 0, msg="sda_boot: 'reps' must be > 0")
    
    while (count <= param$reps) {
      ysam <- lapply(split_y, function(idx) if (length(idx)==1) idx else sample(idx, length(idx), replace=TRUE))
      row.idx <- sort(unlist(ysam))
      
      ret <- if (param$frac > 0 && param$frac < 1) {
        nkeep <- max(param$frac * ncol(x),1)
        ind <- sample(seq_len(ncol(x)), nkeep)
        fit <- try(sda::sda(Xtrain=x[row.idx,ind,drop=FALSE], L=y[row.idx], lambda=lambda[count], verbose=FALSE, ...),
                   silent=TRUE)
        attr(fit, "keep.ind") <- ind
        fit
      } else {
        fit <- try(sda::sda(Xtrain=x[row.idx,], L=y[row.idx], lambda=lambda[count], verbose=FALSE, ...),
                   silent=TRUE)
        attr(fit, "keep.ind") <- seq_len(ncol(x))
        fit
      }
      
      if (!inherits(ret, "try-error")) {
        mfits[[count]] <- ret
        count <- count + 1
      } else {
        message("sda model fit error, retry: ", failures + 1)
        failures <- failures + 1
        if (failures > 10) return(ret)
      }
    }
    
    out <- list(fits=mfits, lev=lev)
    class(out) <- "sda_boot"
    out
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      predict(fit, as.matrix(newdata)[,ind,drop=FALSE], verbose=FALSE)$posterior
    })
    
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    modelFit$lev[apply(pfinal, 1, which.max)]
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      predict(fit, as.matrix(newdata)[,ind,drop=FALSE], verbose=FALSE)$posterior
    })
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    colnames(pfinal) <- modelFit$lev
    pfinal
  }
)


# lda_thomaz_boot
# Store lev similarly, ensure correct predictions and probs
#' @keywords internal
#' @noRd
MVPAModels$lda_thomaz_boot <- list(
  type = "Classification", 
  library = "sparsediscrim", 
  label="lda_thomaz_boot",
  loop = NULL, 
  parameters=data.frame(parameters=c("reps","frac"), class=c("numeric","numeric"),
                        label=c("bootstrap reps","fraction of features")),
  grid=function(x, y, len = NULL) data.frame(reps=10, frac=1),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    mfits <- list()
    count <- 1
    failures <- 0
    
    split_y <- split(seq_along(y), y)
    if (!all(sapply(split_y, function(yy) length(yy) > 0))) {
      stop("All levels in 'y' must have ≥1 instance for lda_thomaz_boot.")
    }
    assertthat::assert_that(param$frac > 0, msg="lda_thomaz_boot: 'frac' must be > 0")
    
    while (count <= param$reps) {
      ysam <- lapply(split_y, function(idx) if (length(idx)==1) idx else sample(idx, length(idx), replace=TRUE))
      row.idx <- sort(unlist(ysam))
      
      ret <- if (param$frac > 0 && param$frac < 1) {
        nkeep <- max(param$frac * ncol(x),1)
        ind <- sample(seq_len(ncol(x)), nkeep)
        fit <- try(sparsediscrim::lda_thomaz(x=x[row.idx,ind,drop=FALSE], y[row.idx]), silent=TRUE)
        attr(fit, "keep.ind") <- ind
        fit
      } else {
        fit <- try(sparsediscrim::lda_thomaz(x[row.idx,,drop=FALSE], y[row.idx]), silent=TRUE)
        attr(fit, "keep.ind") <- seq_len(ncol(x))
        fit
      }
      
      if (!inherits(ret, "try-error")) {
        mfits[[count]] <- ret
        count <- count + 1
      } else {
        message("lda_thomaz model fit error, retry: ", failures + 1)
        failures <- failures + 1
        if (failures > 10) return(ret)
      }
    }
    
    out <- list(fits=mfits, lev=lev)
    class(out) <- "lda_thomaz_boot"
    out
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      scores <- -t(sparsediscrim:::predict.lda_thomaz(fit, newdata[,ind])$scores)
      mc <- scores[cbind(seq_len(nrow(scores)), max.col(scores, ties.method = "first"))]
      probs <- exp(scores - mc)
      zapsmall(probs/rowSums(probs))
    })
    
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    modelFit$lev[apply(pfinal, 1, which.max)]
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      scores <- -t(sparsediscrim:::predict.lda_thomaz(fit, newdata[,ind])$scores)
      mc <- scores[cbind(seq_len(nrow(scores)), max.col(scores))]
      probs <- exp(scores - mc)
      zapsmall(probs/rowSums(probs))
    })
    
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    # pfinal has no colnames here since lda_thomaz doesn't store them directly.
    # Normally, need levels. If needed: colnames(pfinal) <- modelFit$lev
    pfinal
  }
)

# memoized ranking for sda methods
#' @importFrom memoise memoise
#' @importFrom sda sda.ranking
#' @noRd
memo_rank <- memoise::memoise(function(X, L, fdr) {
  sda::sda.ranking(X,L,fdr=fdr,verbose=FALSE)
})

# glmnet_opt
# Store lev, fix prob calculation
#' @keywords internal
#' @noRd
MVPAModels$glmnet_opt <- list(
  type = "Classification", 
  library = c("c060","glmnet"), 
  label="glmnet_opt",
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    bounds <- t(data.frame(alpha=c(0,1)))
    colnames(bounds)<-c("lower","upper")
    fam <- if (length(lev) > 2) "multinomial" else "binomial"
    
    foldid <- caret::createFolds(y,k=5,list=FALSE)
    fit <- epsgo(Q.func="tune.glmnet.interval",
                 bounds=bounds,
                 parms.coding="none",
                 seed=1234,
                 show="none",
                 fminlower=-100,
                 x=x, y=y, family=fam,
                 type.min="lambda.1se",
                 foldid=foldid,
                 type.measure="mse")
    sfit <- summary(fit, verbose=FALSE)
    modelFit <- glmnet(x,y,family=fam,alpha=sfit$opt.alpha, nlambda=20)
    modelFit$opt_lambda <- sfit$opt.lambda
    modelFit$opt_alpha <- sfit$opt.alpha
    modelFit$obsLevels <- lev
    modelFit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    out <- predict(modelFit, as.matrix(newdata), s=modelFit$opt_lambda, type="class")
    if (is.matrix(out)) out[,1] else out
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    lev <- modelFit$obsLevels
    fam <- if (length(lev) > 2) "multinomial" else "binomial"
    probs <- predict(modelFit, as.matrix(newdata), s=modelFit$opt_lambda, type="response")
    if (fam=="binomial") {
      # Binary
      probs <- as.vector(probs)
      probs <- data.frame(probs_1 = 1 - probs, probs_2 = probs)
      colnames(probs) <- lev
    } else {
      # Multinomial returns a 3D array: (N, classes, 1)
      # Convert to data.frame
      probs <- as.data.frame(probs[,,1,drop=FALSE])
      colnames(probs) <- lev
    }
    probs
  }
)

# sparse_sda
# Store lev if needed, no explicit predict labeling needed except from posterior
#' @keywords internal
#' @noRd
MVPAModels$sparse_sda <- list(
  type = "Classification", 
  library = "sda", 
  label="sparse_sda",
  loop = NULL, 
  parameters=data.frame(parameters=c("frac","lambda"), class=c("numeric","numeric"), 
                        label=c("frac features","lambda")),
  grid=function(x, y, len = NULL) expand.grid(frac=seq(.1,1,length.out=len), lambda=seq(.01,.99,length.out=len)),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)                          
    nkeep <- max(param$frac * ncol(x),1)
    rank <- memo_rank(x, L=y, fdr=FALSE)
    ind <- rank[,"idx"][1:nkeep]
    
    fit <- sda::sda(Xtrain=x[,ind,drop=FALSE], L=y, lambda=param$lambda, verbose=FALSE)
    attr(fit, "keep.ind") <- ind
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]), verbose=FALSE)$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]),verbose=FALSE)$posterior
  }
)


# sda_ranking
# Store lev
#' @keywords internal
#' @noRd
MVPAModels$sda_ranking <- list(
  type = "Classification", 
  library = "sda", 
  label="sda_ranking",
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)                             
    if (ncol(x) > 2) {
      rank <- sda::sda.ranking(Xtrain=x, L=y, fdr=TRUE, verbose=FALSE, ...)
      hcind <- which.max(rank[,"HC"])
      keep.ind <- if (hcind < 2) seq(1, min(ncol(x), 2)) else 1:hcind
      ind <- rank[keep.ind,"idx"]
    } else {
      ind <- seq_len(ncol(x))
    }
    
    fit <- sda::sda(Xtrain=x[,ind,drop=FALSE], L=y, verbose=FALSE)
    attr(fit, "keep.ind") <- ind
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]), verbose=FALSE)$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]),verbose=FALSE)$posterior
  }
)


# mgsda
# Fix MSGDA::classifiyV to MGSDA::classifyV and store lev
# Define modelFit as a list after training
#' @keywords internal
#' @noRd
MVPAModels$mgsda <- list(
  type = "Classification", 
  library = "MGSDA", 
  loop = NULL, 
  parameters=data.frame(parameters="lambda", class="numeric", label="sparsity penalty"),
  
  grid=function(x, y, len = NULL) data.frame(lambda=seq(.001, .99, length.out=len)),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) { 
    ycodes <- as.integer(y)
    V <- MGSDA::dLDA(as.matrix(x), ycodes, lambda=param$lambda, ...)
    modelFit <- list(
      V = V,
      ycodes = ycodes,
      xtrain = as.matrix(x),
      ytrain = y,
      obsLevels = lev
    )
    modelFit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- MGSDA::classifyV(modelFit$xtrain, modelFit$ycodes, as.matrix(newdata), modelFit$V)
    modelFit$obsLevels[preds]
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    # Not implemented in original code. If probabilities are not supported, return NULL or implement if possible.
    # Here we return NULL to avoid errors.
    NULL
  }
)

# lda_thomaz
# Store lev
#' @keywords internal
#' @noRd
MVPAModels$lda_thomaz <- list(
  type = "Classification", 
  library = "sparsediscrim", 
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    fit <- sparsediscrim:::lda_thomaz(as.matrix(x), y, ...)
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    # Returns a list with element $class
    sparsediscrim:::predict.lda_thomaz(modelFit, as.matrix(newdata))$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    p <- sparsediscrim:::predict.lda_thomaz(modelFit, as.matrix(newdata), type="prob")
    # p is posterior probabilities with columns corresponding to classes in modelFit$obsLevels
    if (!is.null(modelFit$obsLevels)) colnames(p) <- modelFit$obsLevels
    p
  }
)

# hdrda
# Store lev
#' @keywords internal
#' @noRd
MVPAModels$hdrda <- list(
  type = "Classification", 
  library = "sparsediscrim", 
  label="hdrda",
  loop = NULL, 
  parameters=data.frame(parameters=c("lambda","gamma"), class=c("numeric","numeric"), 
                        label=c("HRDA pooling","shrinkage parameter")),
  
  grid=function(x, y, len = NULL) expand.grid(lambda=seq(.99, .001, length.out=len), gamma=seq(.001, .99, length.out=len)),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    fit <- sparsediscrim:::hdrda(as.matrix(x), y, lambda=param$lambda, gamma=param$gamma, ...)
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    sparsediscrim:::predict.hdrda(modelFit, as.matrix(newdata))$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    posterior <- sparsediscrim:::predict.hdrda(modelFit, as.matrix(newdata))$posterior
    posterior <- t(apply(posterior,1,function(x) x/sum(x)))
    if (!is.null(modelFit$obsLevels)) colnames(posterior) <- modelFit$obsLevels
    posterior
  }
)


#' @importFrom stats var dnorm
#' @keywords internal
#' @noRd
MVPAModels$naive_bayes <- list(
  type = "Classification",
  library = NULL, # No external library needed for this basic implementation
  label = "naive_bayes",
  loop = NULL,
  parameters = data.frame(parameter="parameter", class="character", label="parameter"), # No tunable parameters
  grid = function(x, y, len = NULL) data.frame(parameter="none"), # Trivial grid

  # FIT function: Train the Naive Bayes model
  fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    if (!is.factor(y)) y <- factor(y) # Ensure y is a factor

    n_train <- nrow(x)
    n_features <- ncol(x)
    classes <- levels(y)
    n_classes <- length(classes)

    # Calculate prior probabilities (log scale)
    log_class_priors <- log(sapply(classes, function(level) mean(y == level)))

    # Check for classes with only one sample
    class_counts <- table(y)
    if (any(class_counts < 2)) {
       problem_classes <- names(class_counts[class_counts < 2])
       stop(paste("Cannot train Naive Bayes: Class(es)",
                  paste(problem_classes, collapse=", "),
                  "have fewer than 2 samples. Variance cannot be estimated."))
    }


    # Calculate class-conditional means and variances for each feature
    mus <- matrix(NA, nrow = n_classes, ncol = n_features)
    vars <- matrix(NA, nrow = n_classes, ncol = n_features)
    rownames(mus) <- rownames(vars) <- classes

    # Use split for efficient calculation by class
    obs_indices_by_class <- split(seq_len(n_train), y)

    for (k in 1:n_classes) {
      class_label <- classes[k]
      idx <- obs_indices_by_class[[class_label]]
      samples_k <- x[idx, , drop = FALSE]

      # Calculate means
      mus[k, ] <- colMeans(samples_k)

      # Calculate variances (MLE estimate, dividing by n, like CosmoMVPA)
      # Using apply with var calculates sample variance (divides by n-1)
      # We adjust to get MLE variance: sample_var * (n-1) / n
      n_samples_in_class <- length(idx)
      vars_k <- apply(samples_k, 2, stats::var) * (n_samples_in_class - 1) / n_samples_in_class

      # Handle zero variance: add a small epsilon to avoid issues with dnorm
      # A very small value relative to typical fMRI signal variance
      zero_var_idx <- which(vars_k <= .Machine$double.eps)
      if (length(zero_var_idx) > 0) {
          # Find overall variance across non-zero var features for this class
          non_zero_vars <- vars_k[vars_k > .Machine$double.eps]
          epsilon <- if(length(non_zero_vars) > 0) min(non_zero_vars) * 1e-6 else 1e-10
          # Ensure epsilon is positive
          epsilon <- max(epsilon, 1e-10)
          vars_k[zero_var_idx] <- epsilon
          warning(sprintf("Naive Bayes: %d features had zero variance for class '%s'. Replaced with small epsilon (%g).",
                          length(zero_var_idx), class_label, epsilon))

      }
       vars[k, ] <- vars_k
    }


    # Store the trained model parameters
    modelFit <- list(
      mus = mus,
      vars = vars,
      log_class_priors = log_class_priors,
      classes = classes,
      obsLevels = classes # Consistent with rMVPA convention
    )

    attr(modelFit, "obsLevels") <- classes
    attr(modelFit, "problemType") <- "Classification"

    return(modelFit)
  },

  # PREDICT function: Predict class labels
  predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    newdata <- as.matrix(newdata)
    log_posteriors <- calculate_log_posteriors(modelFit, newdata)

    # Find the class with the maximum log posterior probability for each sample
    max_idx <- apply(log_posteriors, 1, which.max)
    predicted_classes <- modelFit$classes[max_idx]

    # Return as factor with original levels
    factor(predicted_classes, levels = modelFit$classes)
  },

  # PROB function: Predict class probabilities
  prob = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    newdata <- as.matrix(newdata)
    log_posteriors <- calculate_log_posteriors(modelFit, newdata)

    # Convert log posteriors to probabilities using the log-sum-exp trick for stability
    # For each sample (row), subtract the max log posterior before exponentiating
    probs <- apply(log_posteriors, 1, function(log_p_row) {
       max_log_p <- max(log_p_row)
       log_p_shifted <- log_p_row - max_log_p
       p_shifted <- exp(log_p_shifted)
       p_normalized <- p_shifted / sum(p_shifted)
       # Handle potential NaNs if all log posteriors were -Inf
       p_normalized[is.nan(p_normalized)] <- 1 / length(p_normalized)
       return(p_normalized)
    })

    # The result from apply needs to be transposed
    probs <- t(probs)
    colnames(probs) <- modelFit$classes
    return(probs)
  }
)

# Helper function to calculate log posterior probabilities (used by predict and prob)
# This avoids code duplication
calculate_log_posteriors <- function(modelFit, newdata) {
    mus <- modelFit$mus
    vars <- modelFit$vars
    log_class_priors <- modelFit$log_class_priors
    classes <- modelFit$classes
    n_classes <- length(classes)
    n_test <- nrow(newdata)
    n_features <- ncol(newdata)

    if (n_features != ncol(mus)) {
        stop(sprintf("Feature dimension mismatch: model trained with %d features, newdata has %d",
                     ncol(mus), n_features))
    }

    # Matrix to store log posteriors (samples x classes)
    log_posteriors <- matrix(NA, nrow = n_test, ncol = n_classes)
    colnames(log_posteriors) <- classes

    # Calculate log likelihood for each sample and each class
    for (k in 1:n_classes) {
        mu_k <- mus[k, ]
        var_k <- vars[k, ]

        # Use dnorm with log=TRUE for log PDF calculation
        # Apply to each feature (column) for all test samples
        # Result is a matrix (n_test x n_features)
        # Note: dnorm handles vectors for x, mean, sd efficiently
        log_likelihoods_features <- sapply(1:n_features, function(j) {
             stats::dnorm(newdata[, j], mean = mu_k[j], sd = sqrt(var_k[j]), log = TRUE)
        })

        # Handle -Inf results from dnorm (e.g., if variance was tiny or value far out)
        log_likelihoods_features[!is.finite(log_likelihoods_features)] <- -1e100 # Assign a very small log probability

        # Sum log likelihoods across features (naive independence assumption)
        log_likelihood_sum <- rowSums(log_likelihoods_features)

        # Add log prior probability
        log_posteriors[, k] <- log_likelihood_sum + log_class_priors[k]
    }

     # Handle cases where all posteriors might be -Inf (should be rare)
     all_inf_rows <- apply(log_posteriors, 1, function(row) all(!is.finite(row)))
     if (any(all_inf_rows)) {
         log_posteriors[all_inf_rows,] <- log(1/n_classes) # Assign equal log probability
         warning(sprintf("%d samples had non-finite log posteriors for all classes. Assigned equal probability.", sum(all_inf_rows)))
     }


    return(log_posteriors)
}
</file>

<file path="R/feature_rsa_model.R">
#' Create a Feature-Based RSA Design
#'
#' Creates a design for feature-based Representational Similarity Analysis (RSA).
#' You can either supply a similarity matrix S (and optionally select dimensions)
#' or directly supply a feature matrix F.
#'
#' @param S A symmetric similarity matrix representing the feature space relationships. 
#'          If NULL, you must supply F.
#' @param F A feature space matrix (observations by features). If supplied, this overrides S and k.
#' @param labels Vector of labels corresponding to the rows/columns of S or observations of F.
#' @param k Integer specifying the number of feature dimensions to retain when using S. If 0 (default),
#'          automatically determines dimensions using eigenvalue threshold > 1 (minimum 2 dimensions kept).
#'          This parameter is ignored if F is supplied directly (k becomes ncol(F)).
#' @param max_comps Initial upper limit for the number of components to be derived from the
#'          feature space F by subsequent `feature_rsa_model` methods (PCA, PLS, SCCA).
#'          This value is automatically capped by the final feature dimensionality `k`. Default 10.
#'
#' @return A \code{feature_rsa_design} object (S3 class) containing:
#'   \describe{
#'     \item{S}{The input similarity matrix (if used)}
#'     \item{F}{Feature space projection matrix (k dimensions)}
#'     \item{labels}{Vector of observation labels}
#'     \item{k}{The final number of feature dimensions used}
#'     \item{max_comps}{The upper limit on components (<= k)}
#'   }
#'
#' @details
#' This function defines the feature space representation for the analysis.
#' If F is supplied directly, it is used as-is, and `k` becomes `ncol(F)`.
#' If only S is supplied, an eigen decomposition of S is performed.
#' `k` determines how many eigenvectors form the feature matrix F. If `k=0`,
#' dimensions with eigenvalues > 1 are kept (minimum 2).
#' `max_comps` sets an upper bound for the number of components that model-fitting
#' methods (like PCA, PLS, SCCA in `feature_rsa_model`) can use, and it cannot
#' exceed the final feature dimensionality `k`.
#'
#' @export
feature_rsa_design <- function(S=NULL, F=NULL, labels, k=0, max_comps=10) {
  assertthat::assert_that(!is.null(labels))
  
  if (!is.null(F)) {
    assertthat::assert_that(is.matrix(F))
    assertthat::assert_that(nrow(F) == length(labels))
    k_value <- ncol(F)
    max_comps <- min(max_comps, k_value)
    
    ret <- list(
      S=S, 
      F=F, 
      labels=labels, 
      k=k_value,
      max_comps=max_comps
    )
  } else {
    assertthat::assert_that(!is.null(S))
    assertthat::assert_that(is.matrix(S))
    assertthat::assert_that(nrow(S) == length(labels))
    assertthat::assert_that(isSymmetric(S))
    
    S <- (S + t(S)) / 2
    eres <- eigen(S)
    vals <- eres$values
    vecs <- eres$vectors
    
    if (k == 0) {
      k <- sum(vals > 1)
      k <- max(k, 2)
    } else {
      assertthat::assert_that(k > 0 && k <= nrow(S))
    }
    F <- vecs[, seq_len(k), drop=FALSE]
    max_comps <- min(max_comps, k)
    
    ret <- list(
      S=S, 
      F=F, 
      labels=labels, 
      k=k,
      max_comps=max_comps
    )
  }
  
  class(ret) <- "feature_rsa_design"
  ret
}


#' Create a Feature-Based RSA Model
#'
#' Creates a model for feature-based Representational Similarity Analysis (RSA) that relates neural patterns
#' (X) to a predefined feature space (F).
#'
#' @param dataset An \code{mvpa_dataset} object containing the neural data (\code{X}).
#' @param design A \code{feature_rsa_design} object specifying the feature space (\code{F})
#'   and including the component limit (`max_comps`).
#' @param method Character string specifying the analysis method. One of:
#'   \describe{
#'     \item{scca}{Sparse Canonical Correlation Analysis relating X and F.}
#'     \item{pls}{Partial Least Squares regression predicting X from F.}
#'     \item{pca}{Principal Component Analysis on F, followed by regression predicting X from the PCs.}
#'     \item{glmnet}{Elastic net regression predicting X from F using glmnet with multivariate Gaussian response.}
#'   }
#' @param crossval Optional cross-validation specification.
#' @param cache_pca Logical, if TRUE and method is "pca", cache the PCA decomposition of the
#'   feature matrix F across cross-validation folds involving the same training rows.
#'   Defaults to FALSE.
#' @param alpha Numeric value between 0 and 1, only used when method="glmnet". Controls the elastic net
#'   mixing parameter: 1 for lasso (default), 0 for ridge, values in between for a mixture.
#'   Defaults to 0.5 (equal mix of ridge and lasso).
#' @param cv_glmnet Logical, if TRUE and method="glmnet", use cv.glmnet to automatically select the
#'   optimal lambda value via cross-validation. Defaults to FALSE.
#' @param lambda Optional numeric value or sequence of values, only used when method="glmnet" and
#'   cv_glmnet=FALSE. Specifies the regularization parameter. If NULL (default), a sequence will be 
#'   automatically determined by glmnet.
#' @param nperm Integer, number of permutations to run for statistical testing of model performance
#'   metrics after merging cross-validation folds. Default 0 (no permutation testing).
#' @param permute_by DEPRECATED. Permutation is always done by shuffling rows of the predicted matrix.
#' @param save_distributions Logical, if TRUE and nperm > 0, save the full null distributions
#'   from the permutation test. Defaults to FALSE.
#' @param ... Additional arguments (currently unused).
#'
#' @return A \code{feature_rsa_model} object (S3 class).
#'
#' @details
#' Feature RSA models analyze how well a feature matrix \code{F} (defined in the `design`)
#' relates to neural data \code{X}. The `max_comps` parameter, inherited from the `design` object,
#' sets an upper limit on the number of components used:
#'   - \strong{pca}: Performs PCA on \code{F}. `max_comps` limits the number of principal components
#'     (selected by variance explained) used to predict \code{X}. Actual components used: `min(max_comps, available_PCs)`.
#'   - \strong{pls}: Performs PLS regression predicting \code{X} from \code{F}. `max_comps` sets the
#'     maximum number of PLS components to compute. Actual components used may be fewer based on the PLS algorithm.
#'   - \strong{scca}: Performs SCCA between \code{X} and \code{F}. `max_comps` limits the number of
#'     canonical components retained (selected by correlation strength). Actual components used: `min(max_comps, effective_components)`.
#'   - \strong{glmnet}: Performs elastic net regression predicting \code{X} from \code{F} using the glmnet package
#'     with multivariate Gaussian response family. The regularization (lambda) can be automatically selected via cross-validation
#'     if cv_glmnet=TRUE. The alpha parameter controls the balance between L1 (lasso) and L2 (ridge) regularization.
#'
#' **Performance Metrics** (computed by `evaluate_model` after cross-validation):
#'   - `mean_correlation`: Average correlation between predicted and observed patterns for corresponding trials/conditions (diagonal of the prediction-observation correlation matrix).
#'   - `cor_difference`: The `mean_correlation` minus the average off-diagonal correlation (`mean_correlation` - `off_diag_correlation`). Measures how much better the model predicts the correct trial/condition compared to incorrect ones.
#'   - `mean_rank_percentile`: Average percentile rank of the diagonal correlations. For each condition, ranks how well the model's prediction correlates with the correct observed pattern compared to incorrect patterns. Values range from 0 to 1, with 0.5 expected by chance and 1 indicating perfect discrimination.
#'   - `voxel_correlation`: Correlation between the vectorized predicted and observed data matrices across all trials and voxels.
#'   - `mse`: Mean Squared Error between predicted and observed values.
#'   - `r_squared`: Proportion of variance in the observed data explained by the predicted data.
#'   - `p_*`, `z_*`: If `nperm > 0`, permutation-based p-values and z-scores for the above metrics, assessing significance against a null distribution generated by shuffling predicted trial labels.
#'
#' The number of components actually used (`ncomp`) for the region/searchlight is also included in the performance output.
#'
#' @export
feature_rsa_model <- function(dataset,
                               design,
                               method = c("scca", "pls", "pca", "glmnet"),
                               crossval = NULL,
                               cache_pca = FALSE,
                               alpha = 0.5,
                               cv_glmnet = FALSE,
                               lambda = NULL,
                               nperm = 0,
                               permute_by = c("features", "observations"),
                               save_distributions = FALSE,
                               ...) {
  
  method <- match.arg(method)
  permute_by <- match.arg(permute_by)
  assertthat::assert_that(inherits(dataset, "mvpa_dataset"))
  assertthat::assert_that(inherits(design, "feature_rsa_design"))
  
  # Additional validation for dataset dimensions
  mask_dims <- dim(dataset$mask)[1:3]
  total_voxels <- prod(mask_dims)
  active_voxels <- sum(dataset$mask > 0)
  
  if (total_voxels <= 1) {
    stop("Invalid dataset for feature_rsa_model: Only 1 voxel detected (dimensions ",
         paste(mask_dims, collapse="×"),
         "). Feature RSA analysis requires multiple voxels.")
  }
  
  if (active_voxels <= 1) {
    stop("Invalid dataset for feature_rsa_model: Only ", active_voxels,
         " active voxel(s) in mask. Feature RSA analysis requires multiple active voxels.")
  }
  
  if (is.null(crossval) && !is.null(design$block_var)) {
    crossval <- blocked_cross_validation(design$block_var)
  }
  assertthat::assert_that(!is.null(crossval))
  
  # GLMNet specific validation
  if (method == "glmnet") {
    # Check if glmnet is available
    if (!requireNamespace("glmnet", quietly = TRUE)) {
      stop("Package 'glmnet' is required for glmnet method. Please install it with: install.packages('glmnet')")
    }
    
    # Validate alpha parameter
    assertthat::assert_that(is.numeric(alpha) && alpha >= 0 && alpha <= 1,
                           msg = "alpha must be between 0 and 1 (inclusive)")
    
    # Validate lambda if provided
    if (!is.null(lambda)) {
      assertthat::assert_that(is.numeric(lambda) && all(lambda > 0),
                             msg = "lambda must be positive")
    }
  }
  
  max_comps <- design$max_comps
  
  model_spec <- create_model_spec(
    "feature_rsa_model", 
    dataset = dataset,
    design  = design, 
    method  = method, 
    crossval= crossval, 
    compute_performance = TRUE, 
    return_fits = FALSE
  )
  
  # Single "max_comps" in use
  model_spec$max_comps <- max_comps
  
  # new logic for caching
  model_spec$cache_pca <- cache_pca
  
  # We'll store PCA objects in a named list:
  # each entry is keyed by a "hash" of the training row indices
  model_spec$pca_cache <- list()
  
  # GLMNet parameters
  if (method == "glmnet") {
    model_spec$alpha <- alpha
    model_spec$cv_glmnet <- cv_glmnet
    model_spec$lambda <- lambda
  }
  
  model_spec$nperm <- nperm
  model_spec$permute_by <- permute_by
  model_spec$save_distributions <- save_distributions
  
  model_spec
}


#' @noRd
.hash_row_indices <- function(idx) {
  # Sort them so that permutations of the same set produce the same key
  idx_sorted <- sort(idx)
  # Convert to string
  paste0(idx_sorted, collapse="_")
  # Or use a real hash function, e.g., digest::digest(idx_sorted)
}

#' @noRd
.standardize <- function(X) {
  cm <- colMeans(X)
  csd <- apply(X, 2, sd)
  csd[csd == 0] <- 1
  X_sc <- scale(X, center=cm, scale=csd)
  list(X_sc = X_sc, mean = cm, sd = csd)
}



#' @noRd
.predict_scca <- function(model, F_new) {
  # SCCA prediction predicts X from F
  # Check if training failed to find components (ncomp=0)
  if (is.null(model$ncomp) || model$ncomp < 1) {
      futile.logger::flog.warn(".predict_scca: SCCA training found no components (ncomp=%s). Using mean brain pattern fallback for prediction.", 
                                ifelse(is.null(model$ncomp), "NULL", model$ncomp))
      
      # Fallback: Predict the mean brain pattern from training
      n_test <- nrow(F_new)
      mean_x_train <- model$scca_x_mean
      n_voxels <- length(mean_x_train)
      
      if (n_test <= 0 || n_voxels <= 0) {
         stop("Cannot create fallback prediction: invalid dimensions (n_test=%d, n_voxels=%d)", n_test, n_voxels)
      }
      
      # Create matrix repeating the mean training pattern for each test sample
      X_pred_fallback <- matrix(mean_x_train, nrow = n_test, ncol = n_voxels, byrow = TRUE)
      return(X_pred_fallback)
  }
  
  # --- Proceed with standard SCCA prediction if ncomp >= 1 ---
  
  # First check dimensions and ensure compatibility
  F_new <- as.matrix(F_new)
  n_features_new <- ncol(F_new)
  n_features_expected <- length(model$scca_f_mean)
  tm <- model$trained_model
  ncomp <- model$ncomp
  
  # Strictly enforce dimensional consistency
  if (n_features_new != n_features_expected) {
    stop(sprintf("Feature matrix dimension mismatch for SCCA: expected %d features but got %d. This indicates a data inconsistency between training and testing.",
                n_features_expected, n_features_new))
  }
  
  # Standard case: dimensions match
  # Standardize features using the stored means and standard deviations
  Fsc <- sweep(sweep(F_new, 2, model$scca_f_mean, "-"), 2, model$scca_f_sd, "/")
  
  # Use canonical directions from F to predict X
  fcoef <- t(tm$WY)[, 1:ncomp, drop=FALSE]
  xcoef <- t(tm$WX)[, 1:ncomp, drop=FALSE]
  x_inv <- corpcor::pseudoinverse(xcoef)
  
  # Project features to canonical space
  F_canonical <- Fsc %*% fcoef

  # -- Incorporate canonical correlations for prediction --
  # Get the canonical correlations corresponding to the selected components
  canonical_corrs <- tm$lambda[1:ncomp]
  if (is.null(canonical_corrs) || length(canonical_corrs) != ncomp) {
      stop(sprintf("Prediction error: Could not retrieve the expected %d canonical correlations.", ncomp))
  }
  
  # Scale F canonical variates by the canonical correlations to get predicted U variates
  # U_pred = V_test * Lambda
  U_pred <- sweep(F_canonical, 2, canonical_corrs, "*")
  
  # Map predicted U variates back to standardized X space using pseudoinverse of X weights
  # Xhat_sc = U_pred * Wx^+
  Xhat <- U_pred %*% x_inv
  # --------------------------------------------------------

  # Xhat <- F_canonical %*% x_inv # OLD direct mapping
  Xhat <- sweep(sweep(Xhat, 2, model$scca_x_sd, "*"), 2, model$scca_x_mean, "+")
  return(Xhat)
}


#' @noRd
.predict_pca <- function(model, F_new) {
  # F_new is test features (subset for that fold).
  F_new <- as.matrix(F_new)
  
  # Standardize with the same mean/sd from training
  f_means <- model$pca_f_mean
  f_sds   <- model$pca_f_sd
  
  if (length(f_means) != ncol(F_new)) {
    stop("Mismatch in the number of features for PCA prediction vs training.")
  }
  
  Fsc <- sweep(sweep(F_new, 2, f_means, "-"), 2, f_sds, "/")
  
  # Project onto the fold-trained PCA rotation
  PC_new <- Fsc %*% model$pcarot
  
  # Predict X in standardized space
  X_sc_pred <- cbind(1, PC_new) %*% model$pca_coefs  # dim => [nTest, nVoxelsInRegion (or columns in X)]
  
  # Undo standardization of X
  x_means <- model$pca_x_mean
  x_sds   <- model$pca_x_sd
  
  X_pred <- sweep(sweep(X_sc_pred, 2, x_sds, "*"), 2, x_means, "+")
  X_pred
}


#' @importFrom glmnet cv.glmnet glmnet 
#' @noRd
.predict_glmnet <- function(model, F_new) {
  # F_new is test features (subset for that fold)
  F_new <- as.matrix(F_new)
  
  # Check feature dimensions
  if (ncol(F_new) != length(model$glmnet_f_mean)) {
    stop(sprintf("Feature matrix dimension mismatch for GLMNet: expected %d features but got %d.",
                length(model$glmnet_f_mean), ncol(F_new)))
  }
  
  
  # Standardize features using the stored means and standard deviations
  Fsc <- sweep(sweep(F_new, 2, model$glmnet_f_mean, "-"), 2, model$glmnet_f_sd, "/")
  
  # Make predictions with the trained glmnet model
  # For mgaussian family, predict returns a list with one matrix per response
  preds_mat <- if (model$cv_glmnet) {
    # Use the CV-selected lambda
    drop(predict(model$trained_model, newx = Fsc, s = "lambda.min"))
  } else {
    # Use the single lambda or the first lambda in sequence
    drop(predict(model$trained_model, newx = Fsc, s = model$lambda_used))
  }


  
  # For mgaussian, preds_scaled is a list where each element is a matrix
  # with number of rows = nrow(F_new) and columns = 1 (or nlambda if using multiple lambda values)
  # We need to convert this to a matrix of the same dimensions as X
  
  # Determine the number of response variables (voxels)
  nvox <- ncol(preds_mat)
  
  # Unstandardize: convert from z-scores back to original scale
  X_pred <- sweep(sweep(preds_mat, 2, model$glmnet_x_sd, "*"), 2, model$glmnet_x_mean, "+")
  
  return(X_pred)
}


#' @rdname predict_model-methods
#' @export
predict_model.feature_rsa_model <- function(object, fit, newdata, ...) {
  # Check if the 'fit' object contains an error from the training stage
  if (!is.null(fit$error)) {
     error_msg <- sprintf("predict_model: Cannot predict, training failed with error: %s", fit$error)
     futile.logger::flog.error(error_msg)
     stop(error_msg) # Stop prediction if training failed
  }
  
  # Check if trained_model is missing, even if no explicit error was set
  if (is.null(fit$trained_model) && object$method != "scca") { # SCCA might proceed with ncomp=0
      error_msg <- sprintf("predict_model (%s): 'trained_model' is missing in the fit object provided. Cannot predict.", object$method)
      futile.logger::flog.error(error_msg)
      stop(error_msg)
  }

 
  method <- object$method
  F_new  <- as.matrix(newdata)
  
  # Basic check for newdata dimensions
  if (nrow(F_new) < 1) {
      stop("predict_model: newdata (F_new) has 0 rows.")
  }

  # Wrap the entire prediction logic in tryCatch
  predictions <- tryCatch({
    if (method == "pls") {
      
      # Retrieve necessary components from 'fit'
      pls_model <- fit$trained_model
      f_mean <- fit$pls_f_mean
      f_sd <- fit$pls_f_sd
      x_mean <- fit$pls_x_mean
      x_sd <- fit$pls_x_sd
      ncomp_to_use <- fit$ncomp

      # Validate fit components
      if (is.null(pls_model) || is.null(f_mean) || is.null(f_sd) || is.null(x_mean) || is.null(x_sd) || is.null(ncomp_to_use)) {
        stop("predict_model (PLS): Missing essential components (model, means, sds, ncomp) in the fit object.")
      }
      if (ncomp_to_use < 1) {
         # This case should ideally be handled by training setting ncomp appropriately,
         # but predict.plsr might behave unexpectedly with ncomp=0.
         # Let's return mean prediction similar to SCCA? Or error out?
         # For now, let's error, assuming training should prevent ncomp=0 unless intended.
         stop(sprintf("predict_model (PLS): Number of components to use (ncomp=%d) is less than 1.", ncomp_to_use))
      }

      expected_cols <- length(f_mean)
      actual_cols <- ncol(F_new)
      if (actual_cols != expected_cols) {
        stop(sprintf("predict_model (PLS): Feature column mismatch. Model expects %d columns, newdata has %d.",
                     expected_cols, actual_cols))
      }
      
      # Standardize newdata using training parameters
      sf_test <- scale(F_new, center = f_mean, scale = f_sd)
      # Check for NaNs after scaling (can happen if sd was zero and scale didn't handle it perfectly)
      if (any(is.nan(sf_test))) {
         stop("predict_model (PLS): NaNs generated during standardization of newdata. Check feature variances.")
      }

      # Make predictions using pls::predict
      # preds_raw is expected to be the prediction using ncomp_to_use components,
      # potentially in a 3D array [obs, resp, 1]
      preds_raw <- predict(pls_model, newdata = sf_test, ncomp = ncomp_to_use)
      
      # Drop any singleton dimensions (e.g., the component dimension if size 1)
      preds_sc <- drop(preds_raw)

      # Ensure preds_sc is a matrix even if only one observation/response
      if (!is.matrix(preds_sc)) {
         preds_sc <- matrix(preds_sc, nrow=nrow(F_new), ncol=length(x_mean))
      }

      # Un-standardize: X = X_sc * sd + mean
      preds <- sweep(sweep(preds_sc, 2, x_sd, "*"), 2, x_mean, "+")
      return(preds)
      
    } else if (method == "scca") {
      # SCCA prediction logic is in .predict_scca
      # Pass the entire 'fit' object which contains model and standardization params
      return(.predict_scca(fit, F_new)) 
      
    } else if (method == "pca") {
      # PCA prediction logic is in .predict_pca
      # Pass the entire 'fit' object
      return(.predict_pca(fit, F_new))
      
    } else if (method == "glmnet") {
      # GLMNet prediction logic is in .predict_glmnet
      # Pass the entire 'fit' object
      return(.predict_glmnet(fit, F_new))
      
    } else {
      stop(paste("Unknown method in predict_model.feature_rsa_model:", method))
    }
  }, error = function(e) {
      # Catch any error from the specific prediction methods or checks
      error_msg <- sprintf("predict_model (%s): Prediction failed - %s", method, e$message)
      futile.logger::flog.error(error_msg)
      # Re-throw the error so format_result can catch it
      stop(error_msg)
  })
  
  # Final check on prediction output
  if (is.null(predictions) || !is.matrix(predictions)) {
     error_msg <- sprintf("predict_model (%s): Prediction result is NULL or not a matrix. Check internal prediction logic.", method)
     futile.logger::flog.error(error_msg)
     stop(error_msg)
  }
   if (nrow(predictions) != nrow(F_new)) {
     error_msg <- sprintf("predict_model (%s): Prediction result has %d rows, but expected %d (matching newdata).", 
                         method, nrow(predictions), nrow(F_new))
     futile.logger::flog.error(error_msg)
     stop(error_msg)
  }
  
  return(predictions)
}


#' @noRd
#' @keywords internal
#' Helper that performs permutation testing for Feature RSA
#' 
#' @param observed Matrix of observed data
#' @param predicted Matrix of predicted data
#' @param nperm Number of permutations
#' @param save_distributions Logical, whether to save all permutation distributions
#' @param mean_cor Scalar: the observed mean correlation
#' @param cor_difference Scalar: the observed correlation difference (mean_cor - off_diag_cor)
#' @param mean_rank_percentile Scalar: the observed mean percentile rank of diagonal correlations
#' @param voxel_cor Scalar: the observed voxel correlation
#' @param mse Scalar: the observed MSE
#' @param r_squared Scalar: the observed R^2
#' @param cor_temporal_means Scalar: the observed correlation of temporal means
#' @param mean_voxelwise_temporal_cor Scalar: the observed mean voxelwise temporal correlation
#' @param cors Vector of diagonal correlations
#' @return A list with p-values, z-scores, and optionally a list of permutations
.perm_test_feature_rsa <- function(observed,
                                   predicted,
                                   nperm,
                                   save_distributions,
                                   mean_cor,
                                   cor_difference,
                                   mean_rank_percentile,
                                   voxel_cor,
                                   mse,
                                   r_squared,
                                   cor_temporal_means,
                                   mean_voxelwise_temporal_cor,
                                   cors)
{
  message("Performing permutation tests with ", nperm, " permutations... (feature_rsa_model)")
  
  n_rows <- nrow(predicted)
  n_cols <- ncol(predicted)
  rss <- sum((observed - predicted)^2)
  tss <- sum((observed - mean(observed))^2)

  # Counters for how many permutations are "better" than the true model
  count_better_mean_corr  <- 0
  count_better_cor_diff <- 0
  count_better_rank_perc <- 0
  count_better_voxel_corr <- 0
  count_better_mse        <- 0
  count_better_r_squared  <- 0
  count_better_cor_temp_means <- 0 # New
  count_better_mean_vox_temp_cor <- 0 # New

  # For mean & SD (to calculate z-scores)
  sum_mean_corr    <- 0; sum_sq_mean_corr <- 0
  sum_cor_diff    <- 0; sum_sq_cor_diff <- 0
  sum_rank_perc    <- 0; sum_sq_rank_perc <- 0
  sum_voxel_corr    <- 0; sum_sq_voxel_corr <- 0
  sum_mse    <- 0; sum_sq_mse <- 0
  sum_r_squared    <- 0; sum_sq_r_squared <- 0
  sum_cor_temp_means <- 0; sum_sq_cor_temp_means <- 0 # New
  sum_mean_vox_temp_cor <- 0; sum_sq_mean_vox_temp_cor <- 0 # New

  # Optionally store entire distributions
  if (save_distributions) {
    perm_mean_corr     <- numeric(nperm)
    perm_cor_diff      <- numeric(nperm)
    perm_rank_perc     <- numeric(nperm)
    perm_voxel_corr    <- numeric(nperm)
    perm_mse           <- numeric(nperm)
    perm_r_squared     <- numeric(nperm)
    perm_correlations  <- matrix(NA, nrow=nperm, ncol=length(cors))
    perm_cor_temp_means <- numeric(nperm) # New
    perm_mean_vox_temp_cor <- numeric(nperm) # New
  }
  
  # Pre-calculate observed spatial means for efficiency in loop
  mean_obs_across_space <- rowMeans(observed, na.rm = TRUE)
  
  for (i in seq_len(nperm)) {
    # Permute row order of predicted
    perm_idx <- sample(n_rows)
    perm_pred <- predicted[perm_idx, , drop=FALSE]

    # --- Compute metrics for this permutation ---
    # Standard metrics
    perm_cormat <- tryCatch(cor(perm_pred, observed), error=function(e) matrix(NA, nrow=n_rows, ncol=n_rows))
    perm_cors <- diag(perm_cormat)
    pmc  <- mean(perm_cors, na.rm=TRUE)
    
    n <- nrow(perm_cormat)
    perm_off_diag <- NA_real_
    if (n > 1 && sum(!is.na(perm_cormat)) > sum(!is.na(perm_cors))) {
        perm_off_diag <- (sum(perm_cormat, na.rm=TRUE) - sum(perm_cors, na.rm=TRUE)) / (n*n - n)
    }
    pcd <- pmc - perm_off_diag  # Correlation difference
    
    perm_ranks <- rep(NA_real_, n)
    if (n > 1) {
        for (j in 1:n) {
            condition_cors <- perm_cormat[j, ]
            n_valid_cors <- sum(!is.na(condition_cors))
            if (n_valid_cors > 1) {
                perm_ranks[j] <- (sum(condition_cors <= condition_cors[j], na.rm = TRUE) - 1) / (n_valid_cors - 1)
            }
        }
    }
    prp <- mean(perm_ranks, na.rm=TRUE)  # Mean rank percentile
    
    pvc  <- tryCatch(cor(as.vector(perm_pred), as.vector(observed)), error=function(e) NA_real_)
    pmse <- mean((perm_pred - observed)^2, na.rm=TRUE)
    prsq <- NA_real_
    if (tss > 0) {
      rss_perm <- sum((observed - perm_pred)^2, na.rm=TRUE)
      prsq <- 1 - rss_perm / tss
    }

    # Correlation of Temporal Means for permutation
    pctm <- NA_real_
    if (nrow(perm_pred) > 1) {
       mean_perm_pred_across_space <- rowMeans(perm_pred, na.rm = TRUE)
       pctm <- tryCatch(cor(mean_perm_pred_across_space, mean_obs_across_space), error=function(e) NA_real_)
    }

    # Mean Voxelwise Temporal Correlation for permutation
    pmvtc <- NA_real_
    if (nrow(perm_pred) > 1 && n_cols > 0) {
       perm_voxel_cors <- numeric(n_cols)
       for (k in 1:n_cols) {
           perm_voxel_cors[k] <- tryCatch(cor(observed[, k], perm_pred[, k]), error=function(e) NA)
       }
       pmvtc <- mean(perm_voxel_cors, na.rm = TRUE)
    }

    # --- Update counters --- 
    if (!is.na(pmc) && !is.na(mean_cor) && pmc >= mean_cor) count_better_mean_corr <- count_better_mean_corr + 1
    if (!is.na(pcd) && !is.na(cor_difference) && pcd >= cor_difference) count_better_cor_diff <- count_better_cor_diff + 1
    if (!is.na(prp) && !is.na(mean_rank_percentile) && prp >= mean_rank_percentile) count_better_rank_perc <- count_better_rank_perc + 1
    if (!is.na(pvc) && !is.na(voxel_cor) && pvc >= voxel_cor) count_better_voxel_corr <- count_better_voxel_corr + 1
    if (!is.na(pmse) && !is.na(mse) && pmse <= mse) count_better_mse <- count_better_mse + 1 # Lower is better for MSE
    if (!is.na(prsq) && !is.na(r_squared) && prsq >= r_squared) count_better_r_squared <- count_better_r_squared + 1
    if (!is.na(pctm) && !is.na(cor_temporal_means) && pctm >= cor_temporal_means) count_better_cor_temp_means <- count_better_cor_temp_means + 1 # New
    if (!is.na(pmvtc) && !is.na(mean_voxelwise_temporal_cor) && pmvtc >= mean_voxelwise_temporal_cor) count_better_mean_vox_temp_cor <- count_better_mean_vox_temp_cor + 1 # New

    # --- Sums for z-scores --- 
    if (!is.na(pmc)) { sum_mean_corr    <- sum_mean_corr + pmc; sum_sq_mean_corr <- sum_sq_mean_corr + pmc^2 }
    if (!is.na(pcd)) { sum_cor_diff    <- sum_cor_diff + pcd; sum_sq_cor_diff <- sum_sq_cor_diff + pcd^2 }
    if (!is.na(prp)) { sum_rank_perc    <- sum_rank_perc + prp; sum_sq_rank_perc <- sum_sq_rank_perc + prp^2 }
    if (!is.na(pvc)) { sum_voxel_corr    <- sum_voxel_corr + pvc; sum_sq_voxel_corr <- sum_sq_voxel_corr + pvc^2 }
    if (!is.na(pmse)) { sum_mse    <- sum_mse + pmse; sum_sq_mse <- sum_sq_mse + pmse^2 }
    if (!is.na(prsq)) { sum_r_squared    <- sum_r_squared + prsq; sum_sq_r_squared <- sum_sq_r_squared + prsq^2 }
    if (!is.na(pctm)) { sum_cor_temp_means <- sum_cor_temp_means + pctm; sum_sq_cor_temp_means <- sum_sq_cor_temp_means + pctm^2 } # New
    if (!is.na(pmvtc)) { sum_mean_vox_temp_cor <- sum_mean_vox_temp_cor + pmvtc; sum_sq_mean_vox_temp_cor <- sum_sq_mean_vox_temp_cor + pmvtc^2 } # New

    # Possibly store the full permutation distribution
    if (save_distributions) {
      perm_mean_corr[i]    <- pmc
      perm_cor_diff[i]     <- pcd
      perm_rank_perc[i]    <- prp
      perm_voxel_corr[i]   <- pvc
      perm_mse[i]          <- pmse
      perm_r_squared[i]    <- prsq
      perm_correlations[i,] <- perm_cors
      perm_cor_temp_means[i] <- pctm # New
      perm_mean_vox_temp_cor[i] <- pmvtc # New
    }
  }

  # --- Compute p-values --- 
  # Calculate effective N for each metric (number of permutations where metric was computable)
  # Needs stored distributions if save_distributions=TRUE
  n_eff_list <- list()
  if (save_distributions) {
     n_eff_list$mean_corr <- sum(!is.na(perm_mean_corr))
     n_eff_list$cor_diff <- sum(!is.na(perm_cor_diff))
     n_eff_list$rank_perc <- sum(!is.na(perm_rank_perc))
     n_eff_list$voxel_corr <- sum(!is.na(perm_voxel_corr))
     n_eff_list$mse <- sum(!is.na(perm_mse))
     n_eff_list$r_squared <- sum(!is.na(perm_r_squared))
     n_eff_list$cor_temp_means <- sum(!is.na(perm_cor_temp_means))
     n_eff_list$mean_vox_temp_cor <- sum(!is.na(perm_mean_vox_temp_cor))
  } else {
     # If not saving distributions, estimate N_eff based on initial valid check (less precise)
     n_eff_list <- lapply(list(mean_cor, cor_difference, mean_rank_percentile, voxel_cor, mse, r_squared, cor_temporal_means, mean_voxelwise_temporal_cor), function(x) if(is.na(x)) 0 else nperm)
     names(n_eff_list) <- c("mean_corr", "cor_diff", "rank_perc", "voxel_corr", "mse", "r_squared", "cor_temp_means", "mean_vox_temp_cor")
  }
  
  # Helper for p-value calculation
  safe_p <- function(count_better, n_eff) {
       if (n_eff > 0) (count_better + 1) / (n_eff + 1) else NA_real_
  }

  p_mean_corr  <- safe_p(count_better_mean_corr, n_eff_list$mean_corr)
  p_cor_diff   <- safe_p(count_better_cor_diff, n_eff_list$cor_diff)
  p_rank_perc  <- safe_p(count_better_rank_perc, n_eff_list$rank_perc)
  p_voxel_corr <- safe_p(count_better_voxel_corr, n_eff_list$voxel_corr)
  p_mse        <- safe_p(count_better_mse, n_eff_list$mse)
  p_r_squared  <- safe_p(count_better_r_squared, n_eff_list$r_squared)
  p_cor_temp_means <- safe_p(count_better_cor_temp_means, n_eff_list$cor_temp_means)
  p_mean_vox_temp_cor <- safe_p(count_better_mean_vox_temp_cor, n_eff_list$mean_vox_temp_cor)

  # --- Compute means and SDs of permutation distributions --- 
  safe_mean_sd <- function(sum_val, sum_sq_val, n_eff) {
      if (n_eff > 0) {
          mean_perm = sum_val / n_eff
          var_perm = max(0, (sum_sq_val / n_eff) - mean_perm^2)
          sd_perm = sqrt(var_perm)
      } else {
          mean_perm = NA_real_
          sd_perm = NA_real_
      }
      list(mean = mean_perm, sd = sd_perm)
  }
  
  stats_mean_corr <- safe_mean_sd(sum_mean_corr, sum_sq_mean_corr, n_eff_list$mean_corr)
  stats_cor_diff <- safe_mean_sd(sum_cor_diff, sum_sq_cor_diff, n_eff_list$cor_diff)
  stats_rank_perc <- safe_mean_sd(sum_rank_perc, sum_sq_rank_perc, n_eff_list$rank_perc)
  stats_voxel_corr <- safe_mean_sd(sum_voxel_corr, sum_sq_voxel_corr, n_eff_list$voxel_corr)
  stats_mse <- safe_mean_sd(sum_mse, sum_sq_mse, n_eff_list$mse)
  stats_r_squared <- safe_mean_sd(sum_r_squared, sum_sq_r_squared, n_eff_list$r_squared)
  stats_cor_temp_means <- safe_mean_sd(sum_cor_temp_means, sum_sq_cor_temp_means, n_eff_list$cor_temp_means) # New
  stats_mean_vox_temp_cor <- safe_mean_sd(sum_mean_vox_temp_cor, sum_sq_mean_vox_temp_cor, n_eff_list$mean_vox_temp_cor) # New
  
  mean_perm_mean_corr <- stats_mean_corr$mean; sd_perm_mean_corr <- stats_mean_corr$sd
  mean_perm_cor_diff <- stats_cor_diff$mean; sd_perm_cor_diff <- stats_cor_diff$sd
  mean_perm_rank_perc <- stats_rank_perc$mean; sd_perm_rank_perc <- stats_rank_perc$sd
  mean_perm_voxel_corr <- stats_voxel_corr$mean; sd_perm_voxel_corr <- stats_voxel_corr$sd
  mean_perm_mse <- stats_mse$mean; sd_perm_mse <- stats_mse$sd
  mean_perm_r_squared <- stats_r_squared$mean; sd_perm_r_squared <- stats_r_squared$sd
  mean_perm_cor_temp_means <- stats_cor_temp_means$mean; sd_perm_cor_temp_means <- stats_cor_temp_means$sd # New
  mean_perm_mean_vox_temp_cor <- stats_mean_vox_temp_cor$mean; sd_perm_mean_vox_temp_cor <- stats_mean_vox_temp_cor$sd # New
  
  # --- z-scores --- 
  eps <- .Machine$double.eps
  safe_z <- function(observed_val, mean_perm, sd_perm, lower_is_better=FALSE) {
      if (is.na(observed_val) || is.na(mean_perm) || is.na(sd_perm)) return(NA_real_)
      # Ensure sd is not effectively zero
      sd_use <- max(sd_perm, eps)
      if (lower_is_better) {
         (mean_perm - observed_val) / sd_use # e.g., for MSE
      } else {
         (observed_val - mean_perm) / sd_use # For correlations, R^2 etc.
      }
  }
  
  z_mean_corr  <- safe_z(mean_cor, mean_perm_mean_corr, sd_perm_mean_corr)
  z_cor_diff   <- safe_z(cor_difference, mean_perm_cor_diff, sd_perm_cor_diff)
  z_rank_perc  <- safe_z(mean_rank_percentile, mean_perm_rank_perc, sd_perm_rank_perc)
  z_voxel_corr <- safe_z(voxel_cor, mean_perm_voxel_corr, sd_perm_voxel_corr)
  z_mse        <- safe_z(mse, mean_perm_mse, sd_perm_mse, lower_is_better=TRUE) 
  z_r_squared  <- safe_z(r_squared, mean_perm_r_squared, sd_perm_r_squared)
  z_cor_temp_means <- safe_z(cor_temporal_means, mean_perm_cor_temp_means, sd_perm_cor_temp_means) # New
  z_mean_vox_temp_cor <- safe_z(mean_voxelwise_temporal_cor, mean_perm_mean_vox_temp_cor, sd_perm_mean_vox_temp_cor) # New

  out <- list(
    p_values = c(mean_correlation = p_mean_corr,
                 cor_difference = p_cor_diff,
                 mean_rank_percentile = p_rank_perc,
                 voxel_correlation= p_voxel_corr,
                 mse = p_mse,
                 r_squared = p_r_squared,
                 cor_temporal_means = p_cor_temp_means,
                 mean_voxelwise_temporal_cor = p_mean_vox_temp_cor),
    z_scores = c(mean_correlation = z_mean_corr,
                 cor_difference = z_cor_diff,
                 mean_rank_percentile = z_rank_perc,
                 voxel_correlation= z_voxel_corr,
                 mse = z_mse,
                 r_squared = z_r_squared,
                 cor_temporal_means = z_cor_temp_means,
                 mean_voxelwise_temporal_cor = z_mean_vox_temp_cor)
  )

  if (save_distributions) {
    out$permutation_distributions <- list(
      mean_correlation  = perm_mean_corr,
      cor_difference    = perm_cor_diff,
      mean_rank_percentile = perm_rank_perc,
      voxel_correlation = perm_voxel_corr,
      mse               = perm_mse,
      r_squared         = perm_r_squared,
      correlations      = perm_correlations,
      cor_temporal_means = perm_cor_temp_means,
      mean_voxelwise_temporal_cor = perm_mean_vox_temp_cor
    )
  }
  
  out
}



#' Evaluate model performance for feature RSA
#'
#' Computes correlation-based metrics (diag correlation, mean correlation, voxel correlation),
#' MSE, R^2, and optionally performs permutation tests (via a helper function).
#'
#' @param object The feature RSA model
#' @param predicted Matrix of predicted values (from feature space F to voxel space X)
#' @param observed Matrix of observed values (actual voxel space X)
#' @param nperm Number of permutations for statistical testing (default: 0, no permutation)
#' @param save_distributions Logical indicating whether to save full permutation distributions
#' @param ... Additional arguments
#'
#' @return A list containing performance metrics and optional permutation results
#' @export
evaluate_model.feature_rsa_model <- function(object,
                                             predicted,
                                             observed,
                                             nperm = 0,
                                             save_distributions = FALSE,
                                             ...) 
{
  observed  <- as.matrix(observed)
  predicted <- as.matrix(predicted)

  
  
  # Check for constant predictions (zero variance) which cause issues
  if (any(apply(predicted, 2, stats::sd) == 0) || any(apply(observed, 2, stats::sd) == 0)) {
    warning("evaluate_model: Predictions or observed data have zero variance in some columns. Correlation metrics may be NA.")
  }
  
  if (ncol(observed) != ncol(predicted)) {
    stop(sprintf("Mismatch in columns: predicted has %d, observed has %d.", 
                 ncol(predicted), ncol(observed)))
  }
  
  # Base RSA metrics
  cormat     <- cor(predicted, observed)
  cors       <- diag(cormat)
  mean_cor   <- mean(cors, na.rm = TRUE) # Add na.rm = TRUE for robustness
  
  # Calculate mean of off-diagonal correlations
  n <- nrow(cormat)
  off_diag_cors <- (sum(cormat, na.rm = TRUE) - sum(cors, na.rm = TRUE)) / (n*n - n) # Add na.rm
  
  # New metric: mean diagonal correlation minus mean off-diagonal correlation
  # This measures how much better the model predicts the correct condition
  # compared to incorrect conditions.
  cor_difference <- mean_cor - off_diag_cors
  
  # Calculate rank percentile for each condition
  ranks <- numeric(n)
  for (i in 1:n) {
    # Get correlations for the ith predicted pattern with all observed patterns
    condition_cors <- cormat[i, ]
    # Compute percentile rank of diagonal correlation among all other correlations
    # Higher correlation = better rank; adjust to 0-1 scale excluding self-comparison
    ranks[i] <- (sum(condition_cors <= condition_cors[i], na.rm = TRUE) - 1) / (sum(!is.na(condition_cors)) - 1) # Handle NAs
  }
  mean_rank_percentile <- mean(ranks, na.rm = TRUE) # Add na.rm
  voxel_cor  <- cor(as.vector(predicted), as.vector(observed))
  mse        <- mean((predicted - observed)^2, na.rm=TRUE) # Add na.rm
  rss        <- sum((observed - predicted)^2, na.rm=TRUE)  # Add na.rm
  tss        <- sum((observed - mean(observed, na.rm=TRUE))^2, na.rm=TRUE) # Add na.rm
  r_squared  <- if (tss == 0) NA else 1 - (rss / tss) # Handle zero total sum of squares
  
  # Log warnings if key metrics are NA/NaN
  if (!is.finite(mean_cor)) futile.logger::flog.warn("Mean correlation is NA/NaN.")
  if (!is.finite(cor_difference)) futile.logger::flog.warn("Correlation difference is NA/NaN.")
  if (!is.finite(mean_rank_percentile)) futile.logger::flog.warn("Mean rank percentile is NA/NaN.")
  if (!is.finite(voxel_cor)) futile.logger::flog.warn("Voxel correlation is NA/NaN.")
  if (!is.finite(r_squared)) futile.logger::flog.warn("R-squared is NA/NaN.")
  
  # --- Calculate Correlation of Temporal Means (Spatial Averages) ---
  cor_temporal_means <- NA_real_
  if (nrow(observed) > 1 && nrow(predicted) > 1) { # Need >1 observation
      mean_obs_across_space <- tryCatch(rowMeans(observed, na.rm = TRUE), error=function(e) NULL)
      mean_pred_across_space <- tryCatch(rowMeans(predicted, na.rm = TRUE), error=function(e) NULL)
      if (!is.null(mean_obs_across_space) && !is.null(mean_pred_across_space)) {
          cor_temporal_means <- tryCatch(cor(mean_obs_across_space, mean_pred_across_space), error=function(e) NA_real_)
      } else {
         warning("evaluate_model: Could not compute rowMeans for cor_temporal_means.")
      }
  } else {
       warning("evaluate_model: Cannot calculate cor_temporal_means with < 2 observations.")
  }
  if (!is.finite(cor_temporal_means)) cor_temporal_means <- NA_real_ # Ensure NA if calculation failed
  # ------------------------------------------------------------------

  # --- Calculate Mean Voxelwise Temporal Correlation ---
  mean_voxelwise_temporal_cor <- NA_real_
  if (nrow(observed) > 1 && nrow(predicted) > 1 && ncol(observed) > 0) { # Need >1 observation and >0 voxels
      num_voxels <- ncol(observed)
      voxel_cors <- numeric(num_voxels)
      for (i in 1:num_voxels) {
          voxel_cors[i] <- tryCatch(cor(observed[, i], predicted[, i]), error = function(e) NA)
      }
      mean_voxelwise_temporal_cor <- mean(voxel_cors, na.rm = TRUE)
  } else {
       warning("evaluate_model: Cannot calculate mean_voxelwise_temporal_cor with < 2 observations or 0 voxels.")
  }
   if (!is.finite(mean_voxelwise_temporal_cor)) mean_voxelwise_temporal_cor <- NA_real_ # Ensure NA if calculation failed
  # ------------------------------------------------------


  perm_results <- NULL
  # Placeholder for incremental correlation - calculation requires comparing
  # results from models trained on feature subsets, which is not possible
  # within the evaluation of a single model run.
  # incremental_correlation <- NA_real_  # <<< REMOVE THIS COMPLETELY

  if (nperm > 0) {
    perm_results <- .perm_test_feature_rsa(
      observed = observed,
      predicted = predicted,
      nperm = nperm,
      save_distributions = save_distributions,
      mean_cor = mean_cor,
      cor_difference = cor_difference,
      mean_rank_percentile = mean_rank_percentile,
      voxel_cor = voxel_cor,
      mse = mse,
      r_squared = r_squared,
      # incremental_correlation = incremental_correlation, # Pass the calculated value # <<< REMOVE THIS
      cor_temporal_means = cor_temporal_means, # Pass new metric 1
      mean_voxelwise_temporal_cor = mean_voxelwise_temporal_cor, # Pass new metric 2
      cors = cors
    )
  }
  
  list(
    correlations        = cors,
    mean_correlation    = mean_cor,
    off_diag_correlation= off_diag_cors,
    cor_difference      = cor_difference,
    mean_rank_percentile = mean_rank_percentile,
    voxel_correlation   = voxel_cor,
    mse                 = mse,
    r_squared           = r_squared,
    cor_temporal_means = cor_temporal_means, # Add new metric 1
    mean_voxelwise_temporal_cor = mean_voxelwise_temporal_cor, # Add new metric 2
    permutation_results = perm_results
  )
}


#' @export
train_model.feature_rsa_model <- function(obj, X, y, indices, ...) {
  
  # X: brain data (samples x voxels)
  # y: should be the Feature Matrix F (samples x features)
  Fsub <- y
  
  result <- list(method=obj$method, design=obj$design)
  
  # Check for minimum data size
  if (nrow(X) < 2 || ncol(X) < 1 || nrow(Fsub) < 2 || ncol(Fsub) < 1) {
    error_msg <- sprintf("Insufficient data for training (X dims: %d x %d, F dims: %d x %d). Requires at least 2 samples and 1 voxel/feature.", 
                         nrow(X), ncol(X), nrow(Fsub), ncol(Fsub))
    futile.logger::flog.error(error_msg)
    result$error <- error_msg
    return(result)
  }
  
  # ---- PLS Train ----
  if (obj$method == "pls") {
    pls_res <- tryCatch({
      # Check for near-zero variance *before* standardization attempt
      var_X <- apply(X, 2, var, na.rm = TRUE)
      var_F <- apply(Fsub, 2, var, na.rm = TRUE)
      if (any(var_X < 1e-10) || any(var_F < 1e-10)) {
        stop("Near zero variance detected in X or F before standardization.")
      }
      
      sx <- .standardize(X)
      sf <- .standardize(Fsub)
      
      # Check variance *after* standardization (shouldn't happen if .standardize handles sd=0, but double-check)
      if (any(sx$sd < 1e-10) || any(sf$sd < 1e-10)) {
         stop("Near zero variance detected after standardization.")
      }
      
      max_k_possible <- min(nrow(sf$X_sc) - 1, ncol(sf$X_sc))
      k <- min(obj$max_comps, max_k_possible)
      
      if (k < 1) {
        stop(sprintf("Calculated number of PLS components (%d) is less than 1 (max_comps: %d, max_possible: %d).", 
                     k, obj$max_comps, max_k_possible))
      }
      
      # Fit PLS model
      pls::plsr(sx$X_sc ~ sf$X_sc, ncomp = k, scale = FALSE, validation = "none")
      
    }, error = function(e) {
      error_msg <- sprintf("train_model (PLS): Error during training - %s", e$message)
      futile.logger::flog.error(error_msg)
      # Return an error indicator object
      list(error = error_msg) 
    })

    # Check if tryCatch returned an error object
    if (!is.null(pls_res$error)) {
       result$error <- pls_res$error
       return(result)
    }

    # Store necessary results ONLY if successful
    result$trained_model <- pls_res
    result$pls_x_mean    <- sx$mean # Need sx/sf from the try block scope
    result$pls_x_sd      <- sx$sd
    result$pls_f_mean    <- sf$mean
    result$pls_f_sd      <- sf$sd
    result$ncomp         <- k       # Need k from the try block scope

  } else if (obj$method == "scca") {
    # ---- SCCA Train ----
    scca_res <- tryCatch({
       sx <- .standardize(X)
       sf <- .standardize(Fsub)
       if (any(sx$sd < 1e-8) || any(sf$sd < 1e-8)) {
          stop("Zero variance detected after standardization.")
       }
       # Store standardization details temporarily within this scope
       list(res=whitening::scca(sx$X_sc, sf$X_sc, scale=FALSE), sx=sx, sf=sf)
    }, error = function(e) {
      error_msg <- sprintf("train_model (SCCA): SCCA execution error - %s", e$message)
      futile.logger::flog.error(error_msg)
      list(error = error_msg)
    })

    # Check if tryCatch returned an error object
    if (!is.null(scca_res$error)) {
      result$error <- scca_res$error
      return(result)
    }
    
    # Extract results if successful
    scca_fit <- scca_res$res
    sx <- scca_res$sx
    sf <- scca_res$sf
    
    effective_ncomp <- if (!is.null(scca_fit$lambda)) sum(abs(scca_fit$lambda) > 1e-6) else 0
    ncomp <- min(effective_ncomp, obj$max_comps)
    
    # Store standardization info regardless (needed for potential fallback in predict)
    result$scca_x_mean <- sx$mean
    result$scca_x_sd   <- sx$sd
    result$scca_f_mean <- sf$mean
    result$scca_f_sd   <- sf$sd
    result$ncomp       <- ncomp 
        
    if (ncomp < 1) {
      futile.logger::flog.info("train_model (SCCA): No effective canonical components found (effective: %d, max_comps: %d). Prediction will use mean fallback.", 
                               effective_ncomp, obj$max_comps)
      # Still store the (potentially empty) scca_fit
      result$trained_model <- scca_fit
    } else {
      result$trained_model <- scca_fit
    }
    
  } else if (obj$method == "pca") {
    #browser()
    #
    # -- PCA with possible caching --
    #
    pca_result <- tryCatch({
        key <- .hash_row_indices(indices)
        cached_pca <- NULL
        if (isTRUE(obj$cache_pca) && !is.null(obj$pca_cache[[key]])) {
          cached_pca <- obj$pca_cache[[key]]
          sx <- .standardize(X) # Still standardize X for this ROI
          pca_res <- cached_pca$pca_res
          sf_mean <- cached_pca$f_mean
          sf_sd   <- cached_pca$f_sd
        } else {
          sx <- .standardize(X)
          sf <- .standardize(Fsub)
          if (any(sf$sd < 1e-10)) { # Check variance before prcomp
             stop("Zero variance detected in features (F) before PCA.")
          }
          pca_res <- prcomp(sf$X_sc, scale.=FALSE)
          sf_mean <- sf$mean
          sf_sd   <- sf$sd
          # Store if caching is enabled
          if (isTRUE(obj$cache_pca)) {
            obj$pca_cache[[key]] <- list(
              pca_res = pca_res, f_mean = sf_mean, f_sd = sf_sd
            )
          }
        }
        
        available_k <- ncol(pca_res$x)
        k <- min(obj$max_comps, available_k)
        if (k < 1) {
          stop(sprintf("No principal components available (k=%d) after PCA (max_comps: %d, available: %d).", 
                      k, obj$max_comps, available_k))
        }
        PC_train_subset <- pca_res$x[, seq_len(k), drop=FALSE]
        
        df_pcs <- as.data.frame(PC_train_subset)
        if (nrow(df_pcs) <= k) {
          stop(sprintf("Insufficient data for PCA regression (samples: %d, components: %d). Need more samples than components.", 
                       nrow(df_pcs), k))
        }
        
        fit <- lm(sx$X_sc ~ ., data=df_pcs)
        
        # Return all necessary components from successful run
        list(
          pcarot     = pca_res$rotation[, seq_len(k), drop=FALSE],
          pca_f_mean = sf_mean,
          pca_f_sd   = sf_sd,
          pca_coefs  = coef(fit),
          pca_x_mean = sx$mean,
          pca_x_sd   = sx$sd,
          ncomp      = k,
          trained_model = fit # Add the lm fit object here
        )
        
    }, error = function(e) {
        error_msg <- sprintf("train_model (PCA): Error during training - %s", e$message)
        futile.logger::flog.error(error_msg)
        list(error = error_msg)
    })

    # Check if tryCatch returned an error object
    if (!is.null(pca_result$error)) {
        result$error <- pca_result$error
        return(result)
    }
    
    # Assign results if successful
    result <- c(result, pca_result) # Merge the list of results
    
  } else if (obj$method == "glmnet") {
    #
    # ---- GLMNet Train ----
    #
    glm_result <- tryCatch({
        # Standardize X and F
        sx <- .standardize(X)
        sf <- .standardize(Fsub)
        
        if (any(sx$sd < 1e-10) || any(sf$sd < 1e-10)) { # Check variance
             stop("Zero variance detected in X or F after standardization.")
        }

        if (nrow(sx$X_sc) < 2 || nrow(sf$X_sc) < 2) {
          stop(sprintf("Insufficient observations for GLMNet (X: %d, F: %d). Requires >= 2.", nrow(X), nrow(Fsub)))
        }
        
        lambda_to_use <- obj$lambda
        cv_results <- NULL # Placeholder for CV output
        cv_error <- NULL # Placeholder for CV specific error
        
        # Determine if CV should run
        run_cv <- isTRUE(obj$cv_glmnet)
        
        if (run_cv) {
          n_obs <- nrow(sf$X_sc)
          if (n_obs < 3) { # cv.glmnet default nfolds=10 requires >=3
              futile.logger::flog.warn("train_model (GLMNet CV): Too few observations (%d) for reliable CV. Skipping CV.", n_obs)
              run_cv <- FALSE
          } else {
             foldid <- tryCatch({
                 # Use default k-fold (typically 10), let cv.glmnet handle if n_obs < nfolds
                 # Using internal cv.glmnet fold generation might be more robust
                 NULL 
             }, error = function(e) {
                 futile.logger::flog.warn("train_model (GLMNet CV): Error creating fold IDs - %s. Skipping CV.", e$message)
                 run_cv <<- FALSE # Modify run_cv in the outer scope
                 NULL
             })
             
             if (run_cv) { # Check again if foldid creation failed
                cv_fit <- tryCatch({
                    glmnet::cv.glmnet(
                      x = sf$X_sc, 
                      y = sx$X_sc,
                      family = "mgaussian",
                      alpha = obj$alpha,
                      lambda = obj$lambda, # Pass user lambda if specified
                      foldid = foldid,    # Pass NULL to let cv.glmnet create folds
                      standardize = FALSE,
                      intercept = TRUE
                    )
                }, error = function(e) {
                    cv_error <<- sprintf("cv.glmnet failed: %s", e$message) # Assign to outer scope
                    futile.logger::flog.warn("train_model (GLMNet CV): %s. Fitting with standard glmnet instead.", cv_error)
                    run_cv <<- FALSE # Modify run_cv in the outer scope
                    NULL # Return NULL to indicate CV failure
                })
                
                if (run_cv && !is.null(cv_fit)) { # If CV succeeded
                    lambda_to_use <- cv_fit$lambda.min
                    cv_results <- cv_fit # Store CV results
                }
             }
          }
        }
        
        # Fit standard glmnet (either as fallback or primary)
        final_fit <- glmnet::glmnet(
          x = sf$X_sc, 
          y = sx$X_sc,
          family = "mgaussian",
          alpha = obj$alpha,
          lambda = lambda_to_use, # Use CV lambda if available, otherwise obj$lambda
          standardize = FALSE,
          intercept = TRUE
        )
        
        # Determine lambda used for prediction
        lambda_used_for_pred <- if (run_cv && !is.null(cv_results)) {
           lambda_to_use # lambda.min from successful CV
        } else if (!is.null(final_fit$lambda)) {
           final_fit$lambda[1] # First lambda if multiple were fit (e.g., obj$lambda=NULL)
        } else {
           NA # Should not happen if fit succeeded
        }

        # Calculate ncomp proxy
        ncomp_proxy <- NA
        if (!is.null(final_fit) && !is.null(lambda_used_for_pred) && is.finite(lambda_used_for_pred)) {
           coefs <- tryCatch(glmnet::coef.glmnet(final_fit, s = lambda_used_for_pred), error=function(e) NULL)
           if (!is.null(coefs) && is.list(coefs)) { # mgaussian returns a list
              nonzero_count <- sapply(coefs, function(cm) sum(as.matrix(cm[-1,]) != 0)) # Exclude intercept
              ncomp_proxy <- round(mean(nonzero_count))
           } else {
              futile.logger::flog.warn("train_model (GLMNet): Could not extract coefficients to calculate ncomp proxy.")
           }
        } else {
           futile.logger::flog.warn("train_model (GLMNet): Could not determine lambda used or fit failed; cannot calculate ncomp proxy.")
        }

        # Return results
        list(
          trained_model = final_fit,
          glmnet_x_mean = sx$mean,
          glmnet_x_sd   = sx$sd,
          glmnet_f_mean = sf$mean,
          glmnet_f_sd   = sf$sd,
          cv_glmnet     = (run_cv && !is.null(cv_results)), # True only if CV ran *and* succeeded
          cv_results    = cv_results, # Store CV object if it succeeded
          cv_error      = cv_error,   # Store CV error message if it occurred
          lambda_used   = lambda_used_for_pred,
          ncomp         = ncomp_proxy
        )
        
    }, error = function(e) {
        # Catch errors from standardization or the final glmnet fit
        error_msg <- sprintf("train_model (GLMNet): Error during training - %s", e$message)
        futile.logger::flog.error(error_msg)
        list(error = error_msg)
    })
    
    # Check if tryCatch returned an error object
    if (!is.null(glm_result$error)) {
        result$error <- glm_result$error
        return(result)
    }
    
    # Log CV error if it occurred but didn't stop the process
    if (!is.null(glm_result$cv_error)) {
       # This was already logged as warning, but good to have in final result list too?
       # Maybe add it to the result list itself
       result$cv_warning <- glm_result$cv_error
    }
    
    # Assign results if successful
    result <- c(result, glm_result)

  } else {
    # This case should ideally not be reached if method is matched earlier
    error_msg <- paste("Unknown method in train_model.feature_rsa_model:", obj$method)
    futile.logger::flog.error(error_msg)
    result$error <- error_msg
    return(result)
  }

  
  # Check for NULL trained_model just in case
  if (is.null(result$trained_model)) {
     error_msg <- sprintf("train_model (%s): Training finished but 'trained_model' is NULL. This indicates an unexpected issue.", obj$method)
     futile.logger::flog.error(error_msg)
     result$error <- error_msg
     # Ensure ncomp is NA if model is NULL
     if (!"ncomp" %in% names(result)) result$ncomp <- NA 
  }
  
  # Ensure ncomp exists in the result list, set to NA if missing
  if (!"ncomp" %in% names(result)) {
      futile.logger::flog.warn("train_model (%s): 'ncomp' was not set during training. Setting to NA.", obj$method)
      result$ncomp <- NA_real_
  }
  
  return(result) # Return the final result list
}


#' @export
y_train.feature_rsa_model <- function(obj) {
  obj$design$F  # Features are used as predictors (y in training function)
}

#' @export
y_train.feature_rsa_design <- function(obj) {
  obj$F  # Features are used as predictors
}

#' @export
format_result.feature_rsa_model <- function(obj, result, error_message=NULL, context, ...) {
  
  if (!is.null(error_message)) {
    return(tibble::tibble(
      observed      = list(NULL), 
      predicted     = list(NULL),
      result        = list(NULL),
      performance   = list(NULL),
      error         = TRUE, 
      error_message = error_message
    ))
  }
  
  Xobs  <- as.data.frame(context$test)
  Ftest <- as.matrix(context$ytest)
  
  Xpred <- tryCatch({
    predict_model.feature_rsa_model(obj, result, Ftest)
  }, error=function(e) {
       # Log the specific error
       futile.logger::flog.warn("format_result: Prediction failed - %s", e$message)
       # Return a list indicating error and the message
       list(error = TRUE, message = paste("Prediction failed:", e$message))
    })
  
  # Check if the prediction step returned an error list
  if (is.list(Xpred) && !is.null(Xpred$error) && Xpred$error) {
    return(tibble::tibble(
      observed      = list(NULL), 
      predicted     = list(NULL),
      result        = list(NULL),
      performance   = list(NULL),
      error         = TRUE, 
      error_message = Xpred$message # Use the captured error message
    ))
  }
  
  # Check if Xpred is NULL for any other unexpected reason (shouldn't happen ideally)
  if (is.null(Xpred)) {
     return(tibble::tibble(
      observed      = list(NULL), 
      predicted     = list(NULL),
      result        = list(NULL),
      performance   = list(NULL),
      error         = TRUE, 
      error_message = "Prediction returned NULL unexpectedly."
    ))
  }
  
  # Evaluate WITHOUT permutations at the fold level
  perf <- evaluate_model.feature_rsa_model(
    object = obj,
    predicted = Xpred,
    observed  = Xobs,
    nperm = 0  # no permutation here
  )
  
  # Get ncomp from the first fold's result (assuming it's consistent)
  ncomp_used <- result$ncomp
  
  # Summarize
  perf_mat <- matrix(
    c(perf$mean_correlation,
      perf$cor_difference,
      perf$mean_rank_percentile,
      perf$voxel_correlation,
      perf$mse,
      perf$r_squared,
      perf$cor_temporal_means, # Add new metric 1
      perf$mean_voxelwise_temporal_cor, # Add new metric 2
      ncomp_used),
    nrow = 1,
    ncol = 9,
    dimnames = list(NULL, c("mean_correlation", "cor_difference", "mean_rank_percentile", "voxel_correlation", "mse", "r_squared", "cor_temporal_means", "mean_voxelwise_temporal_cor", "ncomp"))
  )
  
  tibble::tibble(
    observed    = list(Xobs),
    predicted   = list(Xpred),
    result      = list(result),
    performance = list(perf_mat),
    error       = FALSE,
    error_message = "~"
  )
}

#' @export
merge_results.feature_rsa_model <- function(obj, result_set, indices, id, ...) {
 
  if (any(result_set$error)) {
    emessage <- result_set$error_message[ which(result_set$error)[1] ]
    return(
      tibble::tibble(
        result       = list(NULL),
        indices      = list(indices),
        performance  = list(NULL),
        id           = id,
        error        = TRUE,
        error_message= emessage
      )
    )
  }
  
  observed_list  <- result_set$observed
  predicted_list <- result_set$predicted

  
  
  # Get the list of results from each fold (contains ncomp)
  fold_results_list <- result_set$result
  
  combined_observed  <- do.call(rbind, observed_list)
  combined_predicted <- do.call(rbind, predicted_list)
  
  # Extract ncomp from the first fold's result 
  # (result_set$result is a list, first element is result from fold 1)
  # Add robustness in case ncomp is NULL or NA
  ncomp_val <- fold_results_list[[1]]$ncomp
  ncomp_used <- if (!is.null(ncomp_val) && is.finite(ncomp_val)) {
    ncomp_val
  } else {
    NA # Use NA if ncomp wasn't properly recorded
  }
  
  # Now we do permutations (if nperm>0 in the model spec)
  perf <- evaluate_model.feature_rsa_model(
    object    = obj,
    predicted = combined_predicted,
    observed  = combined_observed,
    nperm     = obj$nperm,
    save_distributions = obj$save_distributions
  )
  
  # Collate results
  base_metrics <- c(
    perf$mean_correlation,
    perf$cor_difference,
    perf$mean_rank_percentile,
    perf$voxel_correlation,
    perf$mse,
    perf$r_squared,
    # perf$incremental_correlation, # REMOVED
    perf$cor_temporal_means, # Add new metric 1
    perf$mean_voxelwise_temporal_cor, # Add new metric 2
    ncomp_used
  )
  base_names <- c(
    "mean_correlation", "cor_difference", "mean_rank_percentile", 
    "voxel_correlation", "mse", "r_squared", 
    # "incremental_correlation", # REMOVED
    "cor_temporal_means", "mean_voxelwise_temporal_cor", # Add names here
    "ncomp"
  )

  if (is.null(perf$permutation_results)) {
      perf_values <- base_metrics
      perf_names <- base_names
  } else {
      perm_p_values <- perf$permutation_results$p_values
      perm_z_scores <- perf$permutation_results$z_scores
      
      # Ensure order matches p_values/z_scores structure in .perm_test_feature_rsa
      # Dynamically get names to be robust
      p_names <- paste0("p_", names(perm_p_values))
      z_names <- paste0("z_", names(perm_z_scores))

      perf_values <- c(base_metrics, perm_p_values, perm_z_scores)
      perf_names <- c(base_names, p_names, z_names)
  }
  
  perf_mat <- matrix(
      perf_values,
      nrow = 1,
      ncol = length(perf_values),
      dimnames = list(NULL, perf_names)
  )
  
  # Remove any potential columns that are all NA (handles case where incremental_corr placeholders might be NA)
  # Though they are explicitly set to NA_real_, this adds robustness
  perf_mat <- perf_mat[, colSums(is.na(perf_mat)) < nrow(perf_mat), drop = FALSE]

  tibble::tibble(
    result      = list(NULL),
    indices     = list(indices),
    performance = list(perf_mat),
    id          = id,
    error       = FALSE,
    error_message = "~"
  )
}

#' Summary Method for Feature RSA Model
#'
#' @param object The feature RSA model
#' @param ... Additional args
#' @export
summary.feature_rsa_model <- function(object, ...) {
  print(object)
  if (!is.null(object$trained_model)) {
    cat("\nModel Performance:\n")
    print(object$performance)
  }
}



#' Print Method for Feature RSA Design
#'
#' @param x A feature_rsa_design object.
#' @param ... Additional arguments (ignored).
#' @export
print.feature_rsa_design <- function(x, ...) {
  # Create a border line for styling
  border <- crayon::bold(crayon::cyan(strrep("=", 50)))
  
  # Header
  cat(border, "\n")
  cat(crayon::bold(crayon::cyan("          Feature RSA Design          \n")))
  cat(border, "\n\n")
  
  # Extract key details
  n_obs <- nrow(x$F)
  n_feat <- ncol(x$F)
  
  # Print number of observations and feature dimensions
  cat(crayon::bold(crayon::green("Number of Observations: ")), n_obs, "\n")
  cat(crayon::bold(crayon::green("Feature Dimensions:     ")), n_feat, "\n")
  
  # Display the single max_comps limit stored in the design
  # This is the upper limit for components derived from the feature space (F)
  # for *any* method used in feature_rsa_model.
  cat(crayon::bold(crayon::blue("Max Components Limit:   ")), 
      if(!is.null(x$max_comps)) x$max_comps else "Not explicitly set (using default)", "\n")
  
  # Indicate whether a similarity matrix was provided
  if (!is.null(x$S)) {
    cat(crayon::bold(crayon::magenta("Similarity Matrix:      ")), "Provided\n")
  } else {
    cat(crayon::bold(crayon::magenta("Similarity Matrix:      ")), 
        "Not provided (using feature matrix F directly)\n")
  }
  
  # Print first few labels
  n_labels <- length(x$labels)
  n_to_print <- min(5, n_labels)
  label_str <- paste(x$labels[1:n_to_print], collapse = ", ")
  if (n_labels > n_to_print) {
    label_str <- paste0(label_str, ", ...")
  }
  cat(crayon::bold(crayon::yellow("Labels (first few):   ")), label_str, "\n")
  
  # Footer
  cat("\n", border, "\n")
}

#' Print Method for Feature RSA Model
#'
#' @param x A feature_rsa_model object.
#' @param ... Additional arguments (ignored).
#' @export
print.feature_rsa_model <- function(x, ...) {
  # Create a border line for styling
  border <- crayon::bold(crayon::cyan(strrep("=", 50)))
  
  # Header
  cat(border, "\n")
  cat(crayon::bold(crayon::cyan("          Feature RSA Model           \n")))
  cat(border, "\n\n")
  
  # Display the method used (e.g., scca, pls, or pca)
  cat(crayon::bold(crayon::green("Method: ")), x$method, "\n")
  
  # Check if the design component is present to extract dimensions
  if (!is.null(x$design)) {
    n_obs <- nrow(x$design$F)
    n_feat <- ncol(x$design$F)
  } else {
    n_obs <- "Unknown"
    n_feat <- "Unknown"
  }
  
  cat(crayon::bold(crayon::green("Number of Observations: ")), n_obs, "\n")
  cat(crayon::bold(crayon::green("Feature Dimensions:     ")), n_feat, "\n")
  
  # Display component limit for the current method
  if (x$method == "pca") {
    comp_limit <- if(!is.null(x$max_pca_comps)) {
      x$max_pca_comps
    } else if (!is.null(x$design$max_pca_comps)) {
      x$design$max_pca_comps
    } else {
      "Default"
    }
    cat(crayon::bold(crayon::blue("PCA max components:     ")), comp_limit, "\n")
    
    # Indicate if PCA has been precomputed for efficiency
    if (!is.null(x$precomputed_pca)) {
      cat(crayon::bold(crayon::green("PCA Optimization:       ")), 
          "PCA precomputed for efficiency\n")
    }
  } else if (x$method == "scca") {
    comp_limit <- if(!is.null(x$max_scca_comps)) {
      x$max_scca_comps
    } else if (!is.null(x$design$max_scca_comps)) {
      x$design$max_scca_comps
    } else {
      "Default"
    }
    cat(crayon::bold(crayon::blue("SCCA max components:    ")), comp_limit, "\n")
  } else if (x$method == "pls") {
    comp_limit <- if(!is.null(x$max_pls_comps)) {
      x$max_pls_comps
    } else if (!is.null(x$design$max_pls_comps)) {
      x$design$max_pls_comps
    } else {
      "Default"
    }
    cat(crayon::bold(crayon::blue("PLS max components:     ")), comp_limit, "\n")
  } else if (x$method == "glmnet") {
    cat(crayon::bold(crayon::blue("Elastic Net alpha:      ")), x$alpha, "\n")
    cat(crayon::bold(crayon::blue("Cross-validate lambda:  ")), 
        ifelse(isTRUE(x$cv_glmnet), "Yes", "No"), "\n")
    
    if (!is.null(x$lambda)) {
      lambda_str <- if (length(x$lambda) > 3) {
        paste0(paste(x$lambda[1:3], collapse=", "), ", ...")
      } else {
        paste(x$lambda, collapse=", ")
      }
      cat(crayon::bold(crayon::blue("Lambda values:         ")), lambda_str, "\n")
    }
  }
  
  # Indicate training status
  if (!is.null(x$trained_model)) {
    cat(crayon::bold(crayon::magenta("Status: ")), "Trained model available\n")
  } else {
    cat(crayon::bold(crayon::magenta("Status: ")), "Model not yet trained\n")
  }
  
  # Display cross-validation status
  if (!is.null(x$crossval)) {
    cat(crayon::bold(crayon::yellow("Cross-Validation: ")), "Configured\n")
  } else {
    cat(crayon::bold(crayon::yellow("Cross-Validation: ")), "Not configured\n")
  }
  
  # Footer
  cat("\n", border, "\n")
}
</file>

<file path="DESCRIPTION">
Package: rMVPA
Type: Package
Title: Multivoxel Pattern Analysis in R
Version: 0.1.2
Date: 2019-03-08
Authors@R: person("Bradley", "Buchsbaum",, "brad.buchsbaum@gmail.com", c("aut", "cre"))
Maintainer: Bradley R. Buchsbaum <brad.buchsbaum@gmail.com>
Description: An R package to facilitate multivoxel pattern analysis of
    neuroimaging data in R.
License: GPL (>= 2)
Imports:
    assertthat,
    methods,
    dplyr,
    tibble,
    caret,
    neuroim2,
    itertools,
    parallel,
    foreach,
    sda,
    doParallel,
    MASS,
    futile.logger,
    stringr,
    Matrix,
    Metrics,
    memoise,
    purrr,
    furrr,
    ModelMetrics,
    matrixStats,
    modelr,
    Rfit,
    neurosurf,
    io,
    ffmanova,
    sparsediscrim,
    corpcor
Collate:
    'allgeneric.R'
    'custom.R'
    'balance.R'
    'classifiers.R'
    'common.R'
    'crossval.R'
    'dataset.R'
    'design.R'
    'feature_selection.R'
    'globals.R'
    'manova_model.R'
    'mvpa_iterate.R'
    'mvpa_model.R'
    'model_fit.R'
    'mvpa_result.R'
    'performance.R'
    'rMVPA.R'
    'regional.R'
    'resample.R'
    'rsa_model.R'
    'roisplit.R'
    'searchlight.R'
    'vector_rsa_model.R'
    'distcalc.R'
    'utils.R'
    'feature_rsa_model.R'
    'new_analysis_overview.R'
RoxygenNote: 7.3.2.9000
Suggests:
    testthat,
    covr,
    knitr,
    rmarkdown,
    roxygen2,
    glmnet,
    e1071,
    randomForest,
    iterators,
    spls,
    corpcor
Remotes:
    bbuchsbaum/neuroim2,
    bbuchsbaum/neurosurf
VignetteBuilder: knitr
Encoding: UTF-8
URL: http://bbuchsbaum.github.io/rMVPA/
</file>

<file path="R/regional.R">
#' @keywords internal
get_unique_regions.NeuroVol <- function(region_mask, ...) {
  sort(unique(region_mask[region_mask > 0]))
}

#' @keywords internal
get_unique_regions.NeuroSurface <- function(region_mask, ...) {
  sort(unique(region_mask@data[region_mask@data > 0]))
}

#' Combine Regional Results
#'
#' This function combines regional results from a list into a single data frame.
#'
#' @param results A list of regional results.
#' @return A data frame with combined regional results.
#' @details
#' The function is used to combine regional results from a list into a single data frame.
#' It handles both factor and non-factor observed values and creates a combined data frame with the corresponding columns.
#' @keywords internal
#' @noRd
combine_regional_results = function(results) {
  roinum=NULL
  .rownum=NULL
  
  # Check if the observed values are factors (for categorical data)
  if (is.factor(results$result[[1]]$observed)) {
    results %>% dplyr::rowwise() %>% dplyr::do( {
      
      # Create a tibble containing observed, predicted, and additional information
      tib1 <- tibble::tibble(
        .rownum=seq_along(.$result$observed),
        roinum=rep(.$id, length(.$result$observed)),
        observed=.$result$observed,
        pobserved=sapply(seq_along(.$result$observed), function(i) .$result$probs[i, .$result$observed[i]]),
        predicted=.$result$predicted,
        correct=as.character(.$result$observed) == as.character(.$result$predicted)
      )
      
      # Create a tibble with the probabilities for each class
      tib2 <- tibble::as_tibble(.$result$probs, .name_repair=.name_repair)
      names(tib2) <- paste0("prob_", names(tib2))
      
      # Combine tib1 and tib2
      cbind(tib1, tib2)
    })
  } else {
    # For non-factor observed values (for continuous data)
    results %>% dplyr::rowwise() %>% dplyr::do(
      tibble::tibble(
        .rownum=seq_along(.$result$observed),
        roinum=rep(.$id, length(.$result$observed)),
        observed=.$result$observed,
        predicted=.$result$predicted)
    )
  }
}

#' Combine prediction tables
#'
#' Combines multiple prediction tables (e.g., from different models or regions) into a single table.
#' Supports weighted combination and collapsing regions.
#'
#' @param predtabs A list of prediction tables (data frames) to be combined.
#' @param wts A vector of weights, with the same length as \code{predtabs}. Default is equal weights.
#' @param collapse_regions A logical value; if TRUE, regions are collapsed in the final prediction table.
#'
#' @return A combined prediction table (data frame).
#' @import dplyr
#' @importFrom purrr map_df
#' @examples
#' # Create example prediction tables
#' observed = factor(sample(letters[1:2], 10, replace = TRUE))
#' predtab1 <- data.frame(.rownum = 1:10,
#'                        roinum = rep(1, 10),
#'                        observed = observed,
#'                        prob_A = runif(10),
#'                        prob_B = runif(10))
#' predtab2 <- data.frame(.rownum = 1:10,
#'                        roinum = rep(2, 10),
#'                        observed = observed,
#'                        prob_A = runif(10),
#'                        prob_B = runif(10))
#'
#' # Combine the tables
#' combined_table <- combine_prediction_tables(list(predtab1, predtab2))
#' @export
combine_prediction_tables <- function(predtabs, wts=rep(1,length(predtabs)), collapse_regions=FALSE) {
  assert_that(length(predtabs) == length(wts))
  assert_that(sum(wts) > 0)
  assert_that(all(purrr::map_lgl(predtabs, is.data.frame)))
  
  wts <- wts/sum(wts)
  
  .weight <- NULL
  .rownum <- NULL
  roinum <-  NULL
  observed <- NULL
  predicted <- NULL
  
  if (is.character(predtabs[[1]]$observed) || is.factor(predtabs[[1]]$observed)) {
    ## applies constant weight to each table and concatenates
    ptab <- map(seq_along(predtabs), function(i) predtabs[[i]] %>% mutate(.tableid=i, .weight=wts[i])) %>% 
      map_df(bind_rows) %>% as_tibble(.name_repair=.name_repair)
    
    probs <- if (collapse_regions) {
      
      ptab %>% dplyr::group_by(.rownum,observed) %>% summarise_at(vars(starts_with("prob_")), 
                                                                  list(~stats::weighted.mean(., w = .weight)))
    } else {
     
      ## groups over rownames, condition, and roinum, then compute weighted means of probabilities
      ptab %>% dplyr::group_by(.rownum,observed,roinum) %>% summarise_at(vars(starts_with("prob_")), 
                                                                         list(~stats::weighted.mean(., w = .weight)))
    }
    
    p <- probs %>% ungroup() %>% dplyr::select(dplyr::starts_with("prob_"))
    pmat <- as.matrix(p)
      
    pobserved <- pmat[cbind(seq(1,nrow(probs)),as.integer(probs$observed))]
    mc <- max.col(pmat)
    preds <- levels(probs$observed)[mc]
    
    
    prediction_table <- tibble(
      .rownum=probs$.rownum,
      roinum=if (collapse_regions) 1 else probs$roinum,
      observed=probs$observed,
      pobserved=pobserved,
      predicted=preds,
      correct = predicted == probs$observed,
    ) %>% bind_cols(p)
  } else if (is.numeric(predtabs[[1]]$observed)) {
    stop("combining continuous predictions not implemented")
  }
}


#' Merge regional MVPA results
#'
#' Merge multiple regional MVPA results into a single result.
#'
#' @param x A \code{regional_mvpa_result} object.
#' @param ... Additional \code{regional_mvpa_result} objects to be merged.
#'
#' @return A merged \code{regional_mvpa_result} object.
#' @export
merge_results.regional_mvpa_result <- function(x, ...) {
  rlist <- list(x,...)
  combine_prediction_tables(rlist)
}


#' Create a \code{regional_mvpa_result} instance
#'
#' Constructs a regional MVPA result object that stores the results of MVPA analysis in a specific region.
#'
#' @param model_spec A model specification object.
#' @param performance_table A data frame with performance measures.
#' @param prediction_table A data frame with prediction results.
#' @param vol_results A list of voxel-level results.
#' @param fits Optional model fits.
#'
#' @return A \code{regional_mvpa_result} object.
#' @examples
#' # Create example inputs
#' model_spec <- list(dataset = "Example dataset")
#' performance_table <- data.frame(accuracy = c(0.8, 0.85))
#' prediction_table <- data.frame(observed = factor(rep(letters[1:2], 5)),
#'                                 predicted = factor(rep(letters[1:2], 5)))
#' vol_results <- list(vol1 = "Example vol_result 1", vol2 = "Example vol_result 2")
#' fits <- list(fit1 = "Example fit 1", fit2 = "Example fit 2")
#'
#' # Construct a regional_mvpa_result
#' regional_result <- regional_mvpa_result(model_spec, performance_table,
#'                                         prediction_table, vol_results, fits = fits)
#' @export
regional_mvpa_result <- function(model_spec, performance_table, prediction_table, vol_results, fits=fits) {
  ret <- list(model_spec=model_spec, 
              performance_table=performance_table,
              prediction_table=prediction_table,
              vol_results=vol_results,
              fits=fits)
  
  class(ret) <- c("regional_mvpa_result", "list")
  ret
  
}

#' Prepare regional data for MVPA analysis
#'
#' This function processes the input data and prepares the regions for MVPA analysis by extracting
#' voxel indices for each region of interest (ROI) specified in the region_mask.
#'
#' @param model_spec A model specification object.
#' @param region_mask A mask representing different regions in the brain image.
#'
#' @return A list containing information about the regions for further processing:
#'   * allrois: A vector of unique ROI labels.
#'   * region_vec: A vector representation of the region_mask.
#'   * region_set: A sorted vector of unique ROI labels in the region_mask.
#'   * vox_iter: A list containing voxel indices for each ROI.
#'   * lens: A vector containing the number of voxels in each ROI.
#'   * keep: A logical vector indicating if an ROI should be kept for analysis (those with more than one voxel).
#'
#' @examples
#' # Create example inputs
#' model_spec <- list(dataset = "Example dataset")
#' region_mask <- matrix(c(rep(0, 5), rep(1, 5), rep(2, 5), rep(3, 5)), nrow = 5)
#'
#' # Prepare regional data
#' regional_data <- prep_regional(model_spec, region_mask)
#' @export
prep_regional <- function(model_spec, region_mask) {
  allrois <- get_unique_regions(region_mask)
  ##allrois <- sort(unique(region_mask[region_mask>0]))
  region_vec <- as.vector(region_mask)
  region_set <- sort(as.integer(unique(region_vec[region_vec > 0])))
  if (length(region_set) < 1) {
    stop("run_regional: invalid ROI mask, number of ROIs = 0")
  }

  vox_iter <- lapply(region_set, function(rnum) which(region_vec == rnum & model_spec$dataset$mask > 0))
  lens <- sapply(vox_iter, length)
  keep <- lens > 1
  
  if (all(!keep)) {
    futile.logger::flog.error("run_regional: no ROIs have more than one voxel.")
    stop()
  }
  
  if (any(lens < 2)) {
    futile.logger::flog.warn(paste("some ROIs have less than two voxels, removing them from list: ", paste(region_set[lens < 2], collapse=",")))
    vox_iter <- vox_iter[keep]
    region_set <- region_set[keep]
  }
  
  list(allrois=allrois, region_vec=region_vec, region_set=region_set,
       vox_iter=vox_iter, lens=lens, keep=keep)
  
}

#' Compile performance results and generate volumetric results
#'
#' This function compiles the performance results from the regional MVPA analysis
#' and generates volumetric results based on the region_mask.
#'
#' @param results A list containing the results from regional MVPA analysis.
#' @param region_mask A mask representing different regions in the brain image.
#'
#' @return A list containing:
#'   * vols: A list of volumetric results for each performance metric.
#'   * perf_mat: A data frame containing the compiled performance results with an added 'roinum' column.
#'
#' @importFrom neuroim2 map_values
#' @keywords internal
#' @noRd
comp_perf <- function(results, region_mask) {
 
  roinum <- NULL
  ## compile performance results
  perf_mat <- tryCatch({
    do.call(rbind, results$performance)
  }, error = function(e) {
    message("Warning: Error creating performance matrix: ", e$message)
    return(NULL)
  })
  
  # Ensure we keep original names, make unique if duplicates exist
  perf_mat <- as_tibble(perf_mat, .name_repair = "unique")
  
  # Check if perf_mat is NULL or has 0 columns
  if (is.null(perf_mat) || !is.data.frame(perf_mat) || ncol(perf_mat) == 0) {
    message("Warning: Performance matrix is empty or invalid. Returning empty results.")
    return(list(vols=list(), perf_mat=tibble::tibble(roinum=unlist(results$id))))
  }
  
  ## generate volumetric results
  ## TODO fails when region_mask is an logical vol
  vols <- lapply(1:ncol(perf_mat), function(i) map_values(region_mask,
                                                         cbind(as.integer(results$id),
                                                               perf_mat[[i]])))
  names(vols) <- names(perf_mat)
  
  perfmat <- tibble::as_tibble(perf_mat,.name_repair=.name_repair) %>% dplyr::mutate(roinum = unlist(results$id)) %>% dplyr::select(roinum, dplyr::everything())
  list(vols=vols, perf_mat=perfmat)
}

#' @rdname run_regional-methods
#' @param compute_performance Logical indicating whether to compute performance metrics (defaults to \code{model_spec$compute_performance}).
#' @param return_predictions Logical indicating whether to combine a full prediction table (defaults to \code{model_spec$return_predictions}).
#' @param return_fits Logical indicating whether to return the fitted models (defaults to \code{model_spec$return_fits}).
#' @details This function serves as the base implementation for regional analyses, orchestrating data preparation, iteration over regions, performance computation, and result aggregation. Specific `run_regional` methods for different model classes may call this function or provide specialized behavior.
#' @export
run_regional_base <- function(model_spec,
                              region_mask,
                              coalesce_design_vars = FALSE,
                              processor = NULL,
                              verbose = FALSE,
                              compute_performance = model_spec$compute_performance,
                              return_predictions = model_spec$return_predictions,
                              return_fits = model_spec$return_fits,
                              ...) {

  
 
  # 1) Prepare regions
  prepped <- prep_regional(model_spec, region_mask)
  
  # 2) Iterate over regions
  results <- mvpa_iterate(
    model_spec,
    prepped$vox_iter,
    ids = prepped$region_set,
    processor = processor,
    verbose = verbose,
    ...
  )
  
  # 3) Performance computation
  perf <- if (isTRUE(compute_performance)) {
    comp_perf(results, region_mask)
  } else {
    list(vols = list(), perf_mat = tibble::tibble())
  }
  
  # 4) Predictions
  prediction_table <- NULL
  if (isTRUE(return_predictions)) {
    prediction_table <- combine_regional_results(results)
    
    if (coalesce_design_vars && !is.null(prediction_table)) {
      prediction_table <- coalesce_join(
        prediction_table,
        test_design(model_spec$design),
        by = ".rownum"
      )
    }
  }
  
  # 5) Fits
  fits <- NULL
  if (isTRUE(return_fits)) {
    fits <- lapply(results$result, "[[", "predictor")
  }
  
  # 6) Construct and return final result
  regional_mvpa_result(
    model_spec        = model_spec,
    performance_table = perf$perf_mat,
    prediction_table  = prediction_table,
    vol_results       = perf$vols,
    fits             = fits
  )
}


#' Default Method for run_regional
#'
#' @rdname run_regional-methods
#' @details This is the fallback method called when no specialized `run_regional` method is found for the class of `model_spec`. It typically calls `run_regional_base`.
#' @export
run_regional.default <- function(model_spec, region_mask, ...) {
  run_regional_base(model_spec, region_mask, ...)
}


#' Regional MVPA for `mvpa_model` Objects
#'
#' @rdname run_regional-methods
#' @details This method provides the standard regional analysis pipeline for objects of class `mvpa_model` by calling `run_regional_base`.
#' @export
run_regional.mvpa_model <- function(model_spec, region_mask,
                                    coalesce_design_vars = FALSE,
                                    processor = NULL,
                                    verbose = FALSE,
                                    ...) {
  
  run_regional_base(
    model_spec,
    region_mask,
    coalesce_design_vars = coalesce_design_vars,
    processor = processor,
    verbose = verbose,
    ...
  )
}


#' Regional MVPA for `rsa_model` Objects
#'
#' @rdname run_regional-methods
#' @param return_fits Whether to return each region's fitted model (default \code{FALSE}).
#' @param compute_performance \code{logical} indicating whether to compute performance metrics (default \code{TRUE}).
#' @details For `rsa_model` objects, `return_predictions` defaults to `FALSE` as standard RSA typically doesn't produce a prediction table in the same way as classification/regression models.
#' @export
run_regional.rsa_model <- function(model_spec, region_mask,
                                   return_fits = FALSE,
                                   compute_performance = TRUE,
                                   coalesce_design_vars = FALSE,
                                   ...) {
  
  run_regional_base(
    model_spec,
    region_mask,
    coalesce_design_vars  = coalesce_design_vars,
    compute_performance   = compute_performance,
    return_fits           = return_fits,
    return_predictions    = FALSE,  # Override default for RSA
    ...
  )
}


#' Regional MVPA for `feature_rsa_model` Objects
#'
#' @rdname run_regional-methods
#' @param coalesce_design_vars If \code{TRUE}, merges design variables into the prediction table.
#' @param processor An optional region processor function.
#' @details This method handles regional analysis for `feature_rsa_model` objects, typically calling `run_regional_base`.
#' @export
# run_regional.feature_rsa_model <- function(model_spec, region_mask,
#                                            coalesce_design_vars = FALSE,
#                                            processor = NULL,
#                                            verbose = FALSE,
#                                            ...) {
# 
#   
#   run_regional_base(
#     model_spec,
#     region_mask,
#     coalesce_design_vars = coalesce_design_vars,
#     processor = processor,
#     verbose = verbose,
#     ...
#   )
# }

#' Regional MVPA for `vector_rsa_model` Objects
#'
#' @rdname run_regional-methods
#' @param return_fits Logical indicating whether to return the fitted models (default \code{FALSE}).
#' @param compute_performance Logical indicating whether to compute performance metrics (default \code{TRUE}).
#' @details For `vector_rsa_model` objects, `return_predictions` defaults to `FALSE` in `run_regional_base`.
#' If `model_spec$return_predictions` is TRUE, this method will assemble an `observation_scores_table`.
#' @importFrom dplyr bind_rows rename mutate row_number left_join
#' @importFrom tidyr unnest
#' @export
run_regional.vector_rsa_model <- function(model_spec, region_mask,
                                         return_fits = FALSE,
                                         compute_performance = TRUE,
                                         coalesce_design_vars = FALSE, # Usually FALSE for RSA
                                         processor = NULL,
                                         verbose = FALSE,
                                         ...) {
  
  # 1) Prepare regions (using base helper)
  prepped <- prep_regional(model_spec, region_mask)
  
  # 2) Iterate over regions using mvpa_iterate
  # The result from merge_results.vector_rsa_model will contain:
  # - performance: list column with the summary performance matrix
  # - result: list column containing list(rsa_scores=scores_vector) or NULL
  iteration_results <- mvpa_iterate(
    model_spec,
    prepped$vox_iter,
    ids = prepped$region_set,
    processor = processor, # Use default processor unless specified
    verbose = verbose,
    ...
  )
  
  # 3) Performance computation (using base helper)
  # This extracts the 'performance' column from iteration_results
  perf <- if (isTRUE(compute_performance)) {
    comp_perf(iteration_results, region_mask)
  } else {
    list(vols = list(), perf_mat = tibble::tibble())
  }
  
  # 4) Assemble observation scores (if requested)
  prediction_table <- NULL
  if (isTRUE(model_spec$return_predictions) && "result" %in% names(iteration_results)) {
    # Filter out NULL results (where return_predictions was FALSE or errors occurred)
    valid_results <- iteration_results[!sapply(iteration_results$result, is.null), ]
    
    if (nrow(valid_results) > 0) {
      # Create a tibble: roinum | rsa_scores_list
      scores_data <- tibble::tibble(
          roinum = valid_results$id, 
          scores_list = lapply(valid_results$result, function(res) res$rsa_scores)
      )
      
      # Unnest to get a long table: roinum | observation_index | rsa_score
      prediction_table <- scores_data %>%
           mutate(observation_index = map(scores_list, seq_along)) %>% # Add observation index within ROI
           tidyr::unnest(cols = c(scores_list, observation_index)) %>% 
           dplyr::rename(rsa_score = scores_list) # Rename the scores column
           
       # Optionally merge design variables (might need adjustment based on score indices)
       if (coalesce_design_vars) {
            # We need a way to map observation_index back to the original design .rownum
            # This assumes scores are in the same order as the original y_train 
            # (which `second_order_similarity` preserves)
            # Need the original design dataframe 
            orig_design <- model_spec$design$design_table # Assuming it's stored here? Check mvpa_design
            if (!is.null(orig_design)) {
                # Add .rownum based on the original sequence
                # This relies on the assumption that the number of scores matches nrow(orig_design)
                num_obs_in_design <- nrow(orig_design)
                prediction_table <- prediction_table %>%
                   # Need to handle potential mismatch if scores length != num_obs_in_design
                   # For now, assume they match and add .rownum directly
                   dplyr::mutate(.rownum = observation_index) %>%
                   # Perform the join
                   coalesce_join(orig_design, by = ".rownum")
            } else {
                 warning("coalesce_design_vars=TRUE but original design table not found in model_spec$design$design_table")
            }
       }
           
    } else {
         warning("return_predictions=TRUE, but no observation scores were returned from processing.")
    }
  }
  
  # 5) Fits (using base logic - check if applicable for vector_rsa)
  # train_model returns scores, not a fit object, so fits will likely be NULL
  fits <- NULL
  if (isTRUE(return_fits)) {
      # The `result` column now holds scores, not fits. This needs reconsideration.
      # fits <- lapply(iteration_results$result, "[[<some_fit_element>") # This won't work
      warning("`return_fits=TRUE` requested for vector_rsa_model, but this model type does not currently return standard fit objects.")
  }
  
  # 6) Construct and return final result (using base constructor)
  regional_mvpa_result(
    model_spec        = model_spec,
    performance_table = perf$perf_mat,
    prediction_table  = prediction_table, # Add the assembled scores table
    vol_results       = perf$vols,
    fits             = fits
  )
}
</file>

<file path="R/searchlight.R">
#' Wrap output results
#'
#' This function wraps the output results of the performance matrix into a list
#' of SparseNeuroVec objects for each column in the performance matrix.
#'
#' @keywords internal
#' @param perf_mat A performance matrix containing classifier results.
#' @param dataset A dataset object containing the dataset information.
#' @param ids An optional vector of voxel IDs.
#' @return A named list of SparseNeuroVec objects representing the wrapped output results.
wrap_out <- function(perf_mat, dataset, ids=NULL) {
  out <- lapply(1:ncol(perf_mat), function(i) create_searchlight_performance(dataset, perf_mat[,i], ids))
  names(out) <- colnames(perf_mat)
  
  # Add class and metadata
  structure(
    list(
      results = out,
      n_voxels = length(dataset$mask),
      active_voxels = sum(dataset$mask > 0),
      metrics = colnames(perf_mat)
    ),
    class = c("searchlight_result", "list")
  )
}

#' @export
#' @method print searchlight_result
print.searchlight_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  metric_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ Searchlight Analysis Results ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Coverage"), "\n")
  cat(info_style("│  ├─ Total Voxels: "), number_style(format(x$n_voxels, big.mark=",")), "\n")
  cat(info_style("│  └─ Active Voxels: "), number_style(format(x$active_voxels, big.mark=",")), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  for (metric in x$metrics) {
    results <- x$results[[metric]]
    if (inherits(results, "searchlight_performance")) {
      cat(info_style("   ├─ "), metric_style(metric), "\n")
      cat(info_style("   │  ├─ Mean: "), number_style(sprintf("%.4f", results$summary_stats$mean)), "\n")
      cat(info_style("   │  ├─ SD: "), number_style(sprintf("%.4f", results$summary_stats$sd)), "\n")
      cat(info_style("   │  ├─ Min: "), number_style(sprintf("%.4f", results$summary_stats$min)), "\n")
      cat(info_style("   │  └─ Max: "), number_style(sprintf("%.4f", results$summary_stats$max)), "\n")
    }
  }
  
  if (!is.null(x$pobserved)) {
    cat(section_style("\n└─ Observed Probabilities"), "\n")
    # Add probability summary if needed
  }
  
  cat("\n")
}

#' Combine standard classifier results
#'
#' This function combines the standard classifier results from a good results data frame
#' by binding the performance rows together and optionally computes the observed probabilities.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification
#' @param good_results A data frame containing the successful classifier results
#' @param bad_results A data frame containing the unsuccessful classifier results
#' @return A list containing the combined performance matrix and other information
combine_standard <- function(model_spec, good_results, bad_results) {
  # Add improved error handling with diagnostics
  if (nrow(good_results) == 0) {
    futile.logger::flog.error("No successful results to combine. Examining errors from bad results:")
    
    # Analyze bad results to provide better diagnostics
    if (nrow(bad_results) > 0) {
      # Group error messages and count occurrences
      error_summary <- table(bad_results$error_message)
      futile.logger::flog.error("Error summary from %d failed ROIs:", nrow(bad_results))
      
      for (i in seq_along(error_summary)) {
        error_msg <- names(error_summary)[i]
        count <- error_summary[i]
        futile.logger::flog.error("  - %s: %d occurrences", error_msg, count)
      }
      
      # Show a sample of the first few errors
      sample_size <- min(5, nrow(bad_results))
      futile.logger::flog.error("Sample of first %d errors:", sample_size)
      for (i in 1:sample_size) {
        futile.logger::flog.error("  ROI %s: %s", 
                                 bad_results$id[i], 
                                 bad_results$error_message[i])
      }
    } else {
      futile.logger::flog.error("No error information available.")
    }
    
    stop("No valid results for standard searchlight: all ROIs failed to process")
  }
  
  result <- NULL
  
  # Proceed with combining results
  tryCatch({
    ind <- unlist(good_results$id)
    perf_mat <- good_results %>% dplyr::select(performance) %>% (function(x) do.call(rbind, x[[1]]))
    
    has_results <- any(unlist(purrr::map(good_results$result, function(x) !is.null(x))))
    ret <- wrap_out(perf_mat, model_spec$dataset, ind)
    
    if (has_results) {
      pobserved <- good_results %>% 
        dplyr::select(result) %>% 
        pull(result) %>% 
        purrr::map(~ prob_observed(.)) %>% 
        bind_cols()
      
      # Create appropriate type of output based on dataset type
      if (inherits(model_spec$dataset, "mvpa_surface_dataset")) {
        # For surface data
        pobserved <- neurosurf::NeuroSurfaceVector(
          geometry = geometry(model_spec$dataset$train_data),
          indices = seq_len(nrow(pobserved)),  # Or appropriate indices
          mat = as.matrix(pobserved)
        )
      } else {
        # For volume data
        pobserved <- SparseNeuroVec(
          as.matrix(pobserved), 
          space(model_spec$dataset$mask), 
          mask=as.logical(model_spec$dataset$mask)
        )
      }
      
      ret$pobserved <- pobserved
    }
    
    return(ret)
  }, error = function(e) {
    futile.logger::flog.error("Error combining results: %s", e$message)
    futile.logger::flog.debug("Error details: %s", e$call)
    stop(paste("Failed to combine searchlight results:", e$message))
  })
}

#' Combine RSA standard classifier results
#'
#' This function combines the RSA standard classifier results from a good results data frame
#' by binding the performance rows together.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification.
#' @param good_results A data frame containing the successful classifier results.
#' @param bad_results A data frame containing the unsuccessful classifier results.
#' @return A list containing the combined performance matrix along with other information from the dataset.
combine_rsa_standard <- function(model_spec, good_results, bad_results) {
  # Enhanced error handling with detailed diagnostics
  if (nrow(good_results) == 0) {
    futile.logger::flog.error("No successful results for RSA searchlight. Examining errors from bad results:")
    
    # Analyze bad results to provide better diagnostics
    if (nrow(bad_results) > 0) {
      # Group error messages and count occurrences
      error_summary <- table(bad_results$error_message)
      futile.logger::flog.error("Error summary from %d failed ROIs:", nrow(bad_results))
      
      for (i in seq_along(error_summary)) {
        error_msg <- names(error_summary)[i]
        count <- error_summary[i]
        futile.logger::flog.error("  - %s: %d occurrences", error_msg, count)
      }
      
      # Show a sample of the first few errors
      sample_size <- min(5, nrow(bad_results))
      futile.logger::flog.error("Sample of first %d errors:", sample_size)
      for (i in 1:sample_size) {
        futile.logger::flog.error("  ROI %s: %s", 
                                 bad_results$id[i], 
                                 bad_results$error_message[i])
      }
      
      # Check if there are any common issues
      if (any(grepl("unable to find an inherited method for function 'values'", bad_results$error_message))) {
        futile.logger::flog.error("  - Many errors involve NULL ROIs. This may be caused by insufficient voxels in searchlight regions.")
      }
      if (any(grepl("insufficient data dimensions", bad_results$error_message))) {
        futile.logger::flog.error("  - Many errors involve insufficient data dimensions. Your feature data may be too high-dimensional for small searchlight regions.")
      }
    } else {
      futile.logger::flog.error("No error information available.")
    }
    
    stop("No valid results for RSA searchlight: all ROIs failed to process")
  }
  
  # Check if performance data is available
  if (length(good_results$performance) == 0 || all(sapply(good_results$performance, is.null))) {
    futile.logger::flog.error("Performance metrics missing in all results")
    stop("No valid performance metrics for RSA searchlight")
  }
  
  ind <- unlist(good_results$id)
  
  # Extract the performance matrix using safe error handling
  tryCatch({
    perf_mat <- good_results %>% dplyr::select(performance) %>% (function(x) do.call(rbind, x[[1]]))
    ret <- wrap_out(perf_mat, model_spec$dataset, ind)
    return(ret)
  }, error = function(e) {
    futile.logger::flog.error("Error combining RSA results: %s", e$message)
    
    # Try to provide more specific diagnostic information
    if (grepl("requires numeric/complex matrix/vector arguments", e$message)) {
      futile.logger::flog.error("This error often occurs when performance metrics are incompatible across ROIs. Check your performance calculation.")
    } else if (grepl("subscript out of bounds", e$message)) {
      futile.logger::flog.error("This may be caused by inconsistent dimensions in your results. Check that all ROIs return the same metrics.")
    }
    
    stop(paste("Failed to combine RSA results:", e$message))
  })
}

#' Combine Vector RSA standard classifier results
#'
#' This function combines the Vector RSA standard classifier results from a good results data frame
#' by binding the performance rows together.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification.
#' @param good_results A data frame containing the successful classifier results.
#' @param bad_results A data frame containing the unsuccessful classifier results.
#' @return A list containing the combined performance matrix along with other information from the dataset.
combine_vector_rsa_standard <- function(model_spec, good_results, bad_results) {
  ind <- unlist(good_results$id)
  perf_mat <- good_results %>% dplyr::select(performance) %>% (function(x) do.call(rbind, x[[1]]))
  score_mat <- data.frame(sim=rowMeans(perf_mat))
  ret <- wrap_out(score_mat, model_spec$dataset, ind)
  ret
}

#' Combine randomized classifier results
#'
#' This function combines the randomized classifier results from a good results data frame
#' and normalizes the performance matrix by the number of instances for each voxel index.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification.
#' @param good_results A data frame containing the successful classifier results.
#' @param bad_results A data frame containing the unsuccessful classifier results.
#' @return A list containing the combined and normalized performance matrix along with other information from the dataset.
combine_randomized <- function(model_spec, good_results, bad_results=NULL) {
  # Check if we have results
  if (nrow(good_results) == 0 || length(good_results$performance) == 0) {
    futile.logger::flog.error("No valid results for randomized searchlight")
    stop("No valid results for randomized searchlight")
  }
  
  all_ind <- sort(unlist(good_results$indices))
  ind_count <- table(all_ind)
  ind_set <- unique(all_ind)
  
  # Get the number of performance metrics
  ncols <- length(good_results$performance[[1]])
  
  # Create sparse matrix to hold results
  perf_mat <- Matrix::sparseMatrix(i=rep(ind_set, ncols), j=rep(1:ncols, each=length(ind_set)), 
                                  x=rep(0, length(ind_set)*ncols), 
                                  dims=c(length(model_spec$dataset$mask), ncols))
  
  # Process each result
  for (i in 1:nrow(good_results)) {
    ind <- good_results$indices[[i]]
    if (!is.null(ind) && !is.null(good_results$performance[[i]])) {
      # Use tryCatch to handle errors gracefully
      tryCatch({
        m <- kronecker(matrix(good_results$performance[[i]], 1, ncols), rep(1, length(ind)))
        perf_mat[ind,] <- perf_mat[ind,] + m
      }, error = function(e) {
        futile.logger::flog.warn("Error processing result %d: %s", i, e$message)
      })
    }
  }

  # Normalize by the count of overlapping searchlights
  perf_mat[ind_set,] <- sweep(perf_mat[ind_set,,drop=FALSE], 1, as.integer(ind_count), FUN="/")
  
  # Set column names from the performance metrics
  colnames(perf_mat) <- names(good_results$performance[[1]])
  
  # Wrap and return results
  ret <- wrap_out(perf_mat, model_spec$dataset)
  ret
}

#' Pool classifier results
#'
#' This function pools classifier results collected over a set of overlapping indices.
#'
#' @keywords internal
#' @param ... A variable list of data frames containing classifier results to be pooled.
#' @return A list of merged classifier results.
#' @noRd
pool_results <- function(...) {
  reslist <- list(...)
  check <- sapply(reslist, function(res) inherits(res, "data.frame")) 
  assertthat::assert_that(all(check), msg="pool_results: all arguments must be of type 'data.frame'")
  good_results <- do.call(rbind, reslist)
 
  ## the sorted vector of all voxel indices
  all_ind <- sort(unlist(good_results$indices))
  ## how many instances of each voxel?
  ind_count <- table(all_ind)
  ind_set <- unique(all_ind)
  
  ## map every result to the set of indices in that set
  indmap <- do.call(rbind, lapply(1:nrow(good_results), function(i) {
    ind <- good_results$indices[[i]]
    cbind(i, ind)
  }))
  
  
  respsets <- split(indmap[,1], indmap[,2])
  
  merged_results <- purrr::map(respsets, do_merge_results, good_results=good_results)
}



#' Merge searchlight results
#'
#' This function merges searchlight results, combining the first result with the rest of the results.
#'
#' @keywords internal
#' @param r1 A list of indices representing the searchlight results to be merged.
#' @param good_results A data frame containing the valid searchlight results.
#' @return A combined searchlight result object.
do_merge_results <- function(r1, good_results) {
  if (length(r1) > 1) {
    first <- r1[1]
    rest <- r1[2:length(r1)]
    z1 <- good_results$result[[first]]
    z2 <- good_results$result[rest]
    ff <- purrr::partial(merge_results, x=z1)
    do.call(ff, z2)
  } else {
    good_results$result[[r1[1]]]
  }
}

#' Combine randomized searchlight results by pooling
#'
#' This function combines randomized searchlight results by pooling the good results.
#'
#' @keywords internal
#' @param model_spec An object specifying the model used in the searchlight analysis.
#' @param good_results A data frame containing the valid searchlight results.
#' @param bad_results A data frame containing the invalid searchlight results.
#' @return An object containing the combined searchlight results.
pool_randomized <- function(model_spec, good_results, bad_results) {
  if (nrow(good_results) == 0) {
    stop("searchlight: no searchlight samples produced valid results")
  }
  
  
  merged_results <- pool_results(good_results)
  pobserved <- merged_results %>% purrr::map( ~ prob_observed(.)) %>% bind_cols()
  ind_set <- sort(unique(unlist(good_results$indices)))

  all_ids <- which(model_spec$dataset$mask > 0)
  ## if we did not get a result for all voxel ids returned results...
  mask <- if (length(ind_set) != length(all_ids)) {
    mask <- model_spec$dataset$mask
    keep <- all_ids %in% ind_set
    mask[all_ids[!keep]] <- 0
    mask
  } else {
    model_spec$dataset$mask
  }
  
  
  pobserved <- SparseNeuroVec(as.matrix(pobserved), neuroim2::space(mask), mask=as.logical(mask))
  
  #perf_list <- furrr::future_map(merged_results, function(res) compute_performance(model_spec, res))
  perf_list <- purrr::map(merged_results, function(res) compute_performance(model_spec, res))
  
  ncols <- length(perf_list[[1]])
  pmat <- do.call(rbind, perf_list)
  
  perf_mat <- Matrix::sparseMatrix(i=rep(ind_set, ncols), j=rep(1:ncols, each=length(ind_set)), 
                                   x=as.vector(pmat), dims=c(length(model_spec$dataset$mask), ncols))
  
  
  colnames(perf_mat) <- names(perf_list[[1]])
  ret <- wrap_out(perf_mat, model_spec$dataset, ids=NULL) 
  ret$pobserved <- pobserved
  ret
}

#' Perform randomized searchlight analysis
#'
#' This function performs randomized searchlight analysis using a specified model, radius, and number of iterations.
#' It can be customized with different MVPA functions, combiners, and permutation options.
#'
#' @keywords internal
#' @param model_spec An object specifying the model to be used in the searchlight analysis.
#' @param radius The radius of the searchlight sphere.
#' @param niter The number of iterations for randomized searchlight.
#' @param mvpa_fun The MVPA function to be used in the searchlight analysis (default is \code{mvpa_iterate}).
#' @param combiner The function to be used to combine results (default is \code{pool_randomized}).
#' @param ... Additional arguments to be passed to the MVPA function.
#'
#' @importFrom futile.logger flog.error flog.info
#' @importFrom dplyr filter bind_rows
#' @importFrom furrr future_map
do_randomized <- function(model_spec, radius, niter, 
                         mvpa_fun=mvpa_iterate, 
                         combiner=pool_randomized, 
                         ...) {
  error=NULL
  total_models <- 0
  total_errors <- 0
  
  futile.logger::flog.info("🔄 Starting randomized searchlight analysis:")
  futile.logger::flog.info("├─ Radius: %s", crayon::blue(radius))
  futile.logger::flog.info("└─ Iterations: %s", crayon::blue(niter))
  
  ret <- purrr::map(seq(1,niter), function(i) {
    futile.logger::flog.info("\n📊 Iteration %s/%s", crayon::blue(i), crayon::blue(niter))
    slight <- get_searchlight(model_spec$dataset, "randomized", radius)
    
    ## hacky
  
    cind <- if (is.integer(slight[[1]])) {
      ## SurfaceSearchlight...
      purrr::map_int(slight, ~ attr(., "center.index"))
    } else {
      purrr::map_int(slight, ~ .@parent_index)
    }
    
    result <- mvpa_fun(model_spec, slight, cind, ...)
    
    # Count successful and failed models
    n_success <- sum(!result$error, na.rm=TRUE)
    n_errors <- sum(result$error, na.rm=TRUE)
    total_models <<- total_models + n_success
    total_errors <<- total_errors + n_errors
    
    if (n_errors > 0) {
      futile.logger::flog.debug("└─ %s ROIs failed in this iteration", n_errors)
    }
    
    result
  })
  
  results <- dplyr::bind_rows(ret)
  good_results <- results %>% dplyr::filter(error == FALSE)
  bad_results <- results %>% dplyr::filter(error == TRUE)
  
  # Final summary with improved formatting
  futile.logger::flog.info("\n✨ Searchlight Analysis Complete")
  futile.logger::flog.info("├─ Total Models Fit: %s", crayon::green(total_models))
  if (total_errors > 0) {
    futile.logger::flog.info("└─ Failed ROIs: %s (%s%%)", 
                            crayon::yellow(total_errors),
                            crayon::yellow(sprintf("%.1f", total_errors/(total_models + total_errors)*100)))
  } else {
    futile.logger::flog.info("└─ All ROIs processed successfully!")
  }
  
  if (nrow(good_results) == 0) {
    futile.logger::flog.error("❌ No valid results for randomized searchlight")
    stop("No valid results produced")
  }
  
  combiner(model_spec, good_results)
}



#' Perform standard searchlight analysis
#'
#' This function performs standard searchlight analysis using a specified model and radius.
#' It can be customized with different MVPA functions, combiners, and permutation options.
#'
#' @keywords internal
#' @param model_spec An object specifying the model to be used in the searchlight analysis.
#' @param radius The radius of the searchlight sphere.
#' @param mvpa_fun The MVPA function to be used in the searchlight analysis (default is \code{mvpa_iterate}).
#' @param combiner The function to be used to combine results (default is \code{combine_standard}).
#' @param ... Additional arguments to be passed to the MVPA function.
do_standard <- function(model_spec, radius, mvpa_fun=mvpa_iterate, combiner=combine_standard, ...) {
  error=NULL
  flog.info("creating standard searchlight")
  slight <- get_searchlight(model_spec$dataset, "standard", radius)
  
   
  cind <- which(model_spec$dataset$mask > 0)
  flog.info("running standard searchlight iterator")
  ret <- mvpa_fun(model_spec, slight, cind, ...)
  good_results <- ret %>% dplyr::filter(!error)
  bad_results <- ret %>% dplyr::filter(error == TRUE)
  
  if (nrow(bad_results) > 0) {
    flog.info(bad_results$error_message)
  }
  
  if (nrow(good_results) == 0) {
    ## TODO print out some debug information
    flog.error("no valid results for standard searchlight, exiting.")
  }
  
  combiner(model_spec, good_results, bad_results)
}



#' A "base" function for searchlight analysis
#'
#' This function implements the generic logic for running a searchlight:
#' \enumerate{
#'   \item Checks \code{radius} and \code{method}.
#'   \item For "standard" searchlight, calls \code{do_standard(...)}.
#'   \item For "randomized", calls \code{do_randomized(...)} with \code{niter} times.
#'   \item Handles the \code{combiner} function or string ("pool", "average").
#' }
#'
#' It does not assume any specific model type, but expects that \code{model_spec}
#' is compatible with \code{do_standard(...)} or \code{do_randomized(...)} in your code.
#'
#' @param model_spec A model specification object (e.g., \code{mvpa_model}, \code{vector_rsa_model}, etc.).
#' @param radius Numeric searchlight radius (1 to 100).
#' @param method Character: "standard" or "randomized".
#' @param niter Number of iterations if \code{method="randomized"}.
#' @param combiner Either a function that combines partial results or a string
#'        ("pool", "average") that selects a built-in combiner.
#' @param ... Additional arguments passed on to \code{do_standard} or \code{do_randomized}.
#'
#' @return The result object from \code{do_standard} or \code{do_randomized} (often a \code{searchlight_result} or similar).
#'
#' @export
run_searchlight_base <- function(model_spec,
                                 radius = 8,
                                 method = c("randomized", "standard"),
                                 niter = 4,
                                 combiner = "average",
                                 ...) {
  # 1) Check radius
  if (radius < 1 || radius > 100) {
    stop(paste("radius", radius, "outside allowable range (1-100)"))
  }
  
  # 2) Match method
  method <- match.arg(method)
  
  # If method is randomized, check niter
  if (method == "randomized") {
    assert_that(niter >= 1, msg = "Number of iterations for randomized searchlight must be >= 1")
  }
  
  # 3) Decide combiner if it's "pool" or "average"
  #    (In your code, you might have do_standard/do_randomized handle this logic directly—this is just an example.)
  chosen_combiner <- combiner
  if (!is.function(combiner)) {
    if (combiner == "pool") {
      chosen_combiner <- pool_randomized        # or combine_standard, depending on your code
    } else if (combiner == "average") {
      chosen_combiner <- combine_randomized     # or combine_standard, depending on your code
    } else {
      stop("'combiner' must be 'average', 'pool', or a custom function.")
    }
  }
  
  print(paste("combiner is", str(chosen_combiner)))
  
  # 4) Dispatch to do_standard or do_randomized
  res <- if (method == "standard") {
    flog.info("Running standard searchlight with radius = %s", radius)
    do_standard(model_spec, radius, combiner = chosen_combiner, ...)
  } else {  # method == "randomized"
    flog.info("Running randomized searchlight with radius = %s and niter = %s", radius, niter)
    do_randomized(model_spec, radius, niter = niter, combiner = chosen_combiner, ...)
  }
  
  res
}

#' Default method for run_searchlight
#'
#' By default, if an object's class does not implement a specific 
#' \code{run_searchlight.<class>} method, this fallback will call
#' \code{run_searchlight_base}.
#'
#' @param model_spec The generic model specification object.
#' @inheritParams run_searchlight_base
#'
#' @export
run_searchlight.default <- function(model_spec, radius = 8, method = c("randomized","standard"),
                                    niter = 4, combiner = "average", ...) {
  run_searchlight_base(
    model_spec    = model_spec,
    radius        = radius,
    method        = method,
    niter         = niter,
    combiner      = combiner,
    ...
  )
}

#' run_searchlight method for vector_rsa_model
#'
#' This sets a custom \code{mvpa_fun} (e.g., \code{vector_rsa_iterate}) or 
#' different combiners for standard vs. randomized, etc.
#'
#' @param model_spec A \code{vector_rsa_model} object.
#' @inheritParams run_searchlight_base
#' @export
run_searchlight.vector_rsa <- function(model_spec,
                                       radius = 8,
                                       method = c("randomized","standard"),
                                       niter = 4,
                                       ...) {
  method <- match.arg(method)
  
  if (method == "standard") {
    flog.info("Running standard vector RSA searchlight (radius = %s)", radius)
    do_standard(model_spec, radius, mvpa_fun = vector_rsa_iterate, combiner = combine_vector_rsa_standard, ...)
  } else {
    flog.info("Running randomized vector RSA searchlight (radius = %s, niter = %s)", radius, niter)
    do_randomized(model_spec, radius, niter = niter, mvpa_fun = vector_rsa_iterate, combiner = combine_randomized, ...)
  }
}


#' Create a searchlight performance object
#'
#' @keywords internal
#' @param dataset The dataset object
#' @param perf_vec Performance vector for a single metric
#' @param ids Optional vector of voxel IDs
#' @return A searchlight_performance object
create_searchlight_performance <- function(dataset, perf_vec, ids=NULL) {
  # First use the S3 wrap_output method to create the NeuroVol
  ret <- wrap_output(dataset, perf_vec, ids)
  
  # Get non-zero and non-NA values for statistics
  vals <- perf_vec[perf_vec != 0 & !is.na(perf_vec)]
  
  # Then wrap it in our searchlight_performance structure
  structure(
    list(
      data = ret,
      metric_name = names(perf_vec)[1],
      n_nonzero = sum(perf_vec != 0, na.rm = TRUE),
      summary_stats = list(
        mean = if(length(vals) > 0) mean(vals, na.rm = TRUE) else NA,
        sd = if(length(vals) > 0) sd(vals, na.rm = TRUE) else NA,
        min = if(length(vals) > 0) min(vals, na.rm = TRUE) else NA,
        max = if(length(vals) > 0) max(vals, na.rm = TRUE) else NA
      ),
      indices = ids  # Store the indices for reference
    ),
    class = c("searchlight_performance", "list")
  )
}

#' @export
#' @method print searchlight_performance
print.searchlight_performance <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  metric_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ Searchlight Performance: "), 
      metric_style(x$metric_name), header_style(" ▀▀█"), "\n\n")
  
  # Data information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Non-zero Values: "), 
      if(is.null(x$n_nonzero)) crayon::red("NULL") else number_style(format(x$n_nonzero, big.mark=",")), 
      "\n")
  
  # Statistics
  cat(section_style("└─ Statistics"), "\n")
  
  # Helper function to format stats with better NULL handling
  format_stat <- function(val) {
    if (is.null(val) || (length(val) == 0)) {
      crayon::red("No data")
    } else if (is.na(val)) {
      crayon::red("No valid data")
    } else {
      number_style(sprintf("%.4f", val))
    }
  }
  
  # Safely extract stats with NULL checking
  stats <- x$summary_stats
  if (is.null(stats)) {
    stats <- list(mean=NULL, sd=NULL, min=NULL, max=NULL)
  }
  
  cat(info_style("   ├─ Mean: "), format_stat(stats$mean), "\n")
  cat(info_style("   ├─ SD: "), format_stat(stats$sd), "\n")
  cat(info_style("   ├─ Min: "), format_stat(stats$min), "\n")
  cat(info_style("   └─ Max: "), format_stat(stats$max), "\n\n")
}
</file>

<file path="R/mvpa_iterate.R">
#' @noRd
#' @keywords internal
setup_mvpa_logger <- function() {
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty logging. Please install it.")
  }
  
  # Use the standard layout but with colored messages
  futile.logger::flog.layout(futile.logger::layout.simple)
  
  # Set default threshold to INFO to hide DEBUG messages
  # This prevents common/expected errors from being displayed
  futile.logger::flog.threshold(futile.logger::INFO)
}

#' @keywords internal
try_warning  <- function(expr) {
  warn <- err <- NULL
  value <- withCallingHandlers(
    tryCatch(expr, error=function(e) {
      err <<- e
      NULL
    }), warning=function(w) {
      warn <<- paste0(warn, str_trim(as.character(w)))
      invokeRestart("muffleWarning")
    })
  list(value=value, warning=warn, error=err)
}



#' @noRd
#' @keywords internal
generate_crossval_samples <- function(mspec, roi) {
  crossval_samples(mspec$crossval, tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair="minimal"), y_train(mspec))
}

#' @noRd
#' @keywords internal
handle_model_training_error <- function(result, id, ytest) {
  futile.logger::flog.warn("⚠ Model %s fitting error: %s", 
                          crayon::blue(id), 
                          crayon::red(attr(result, "condition")$message))
  emessage <- if (is.null(attr(result, "condition")$message)) "" else attr(result, "condition")$message
  tibble::tibble(class=list(NULL), probs=list(NULL), y_true=list(ytest), 
                 fit=list(NULL), error=TRUE, error_message=emessage)
}

create_result_tibble <- function(cres, ind, mspec, id, result, compute_performance) {
  if (compute_performance) {
    tibble::tibble(result=list(cres), indices=list(ind), 
                   performance=list(compute_performance(mspec, cres)), id=id, 
                   error=FALSE, error_message="~", 
                   warning=!is.null(result$warning), 
                   warning_message=if (is.null(result$warning)) "~" else result$warning)
  } else {
    tibble::tibble(result=list(cres), indices=list(ind), performance=list(NULL), id=id, 
                   error=FALSE, error_message="~", 
                   warning=!is.null(result$warning), 
                   warning_message=if (is.null(result$warning)) "~" else result$warning)
  }
}



#' External Cross-Validation
#'
#' This function performs external cross-validation on the provided ROI and model specification.
#' It returns a tibble with performance metrics, fitted model (optional), and any warnings or errors.
#'
#' @param roi A list containing train_roi and test_roi elements.
#' @param mspec A model specification object.
#' @param id A unique identifier for the model.
#'
#' @return A tibble with performance metrics, fitted model (optional), and any warnings or errors.
#' @noRd
#' @keywords internal
external_crossval <- function(mspec, roi, id) {
  # Prepare the training data
  xtrain <- tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair="minimal")


  ytrain <- y_train(mspec)
 
  # Get the testing labels
  ytest <- y_test(mspec)

  # Get the ROI indices
  ind <- neuroim2::indices(roi$train_roi)

  #browser()
  # Train the model and handle any errors
  result <- try(train_model(mspec, xtrain, ytrain, indices=ind,
                            param=mspec$tune_grid,
                            tune_reps=mspec$tune_reps))
  
 

  if (inherits(result, "try-error")) {
    # Log a warning if there's an error during model training
    flog.warn("error fitting model %s : %s", id, attr(result, "condition")$message)
    # Store error messages and return a tibble with the error information
    emessage <- if (is.null(attr(result, "condition")$message)) "" else attr(result, "condition")$message
    tibble::tibble(class=list(NULL), probs=list(NULL), y_true=list(ytest),
                   fit=list(NULL), error=TRUE, error_message=emessage)
  } else {
    # Make predictions using the trained model
    pred <- predict(result, tibble::as_tibble(neuroim2::values(roi$test_roi), .name_repair="minimal"), NULL)
    # Convert predictions to a list
    plist <- lapply(pred, list)
    plist$y_true <- list(ytest)
    plist$test_ind <- list(as.integer(seq_along(ytest)))

    # Create a tibble with the predictions
    ret <- tibble::as_tibble(plist, .name_repair = .name_repair)

    # Wrap the results and return the fitted model if required
    cres <- if (mspec$return_fit) {
      wrap_result(ret, mspec$design, result$fit)
    } else {
      wrap_result(ret, mspec$design)
    }

    # Compute performance and return a tibble with the results and any warnings
    if (mspec$compute_performance) {
      tibble::tibble(result=list(cres), indices=list(ind),
                     performance=list(compute_performance(mspec, cres)), id=id,
                     error=FALSE, error_message="~",
                     warning=!is.null(result$warning),
                     warning_message=if (is.null(result$warning)) "~" else result$warning)
    } else {
      tibble::tibble(result=list(cres), indices=list(ind), performance=list(NULL), id=id,
                     error=FALSE, error_message="~",
                     warning=!is.null(result$warning),
                     warning_message=if (is.null(result$warning)) "~" else result$warning)
    }

  }
}


#' Perform Internal Cross-Validation for MVPA Models
#'
#' This function performs internal cross-validation on a region of interest (ROI) using a specified 
#' MVPA model. It handles the training, prediction, and result aggregation for each cross-validation fold.
#'
#' @param mspec An MVPA model specification object containing:
#'   \describe{
#'     \item{crossval}{Cross-validation specification}
#'     \item{compute_performance}{Logical indicating whether to compute performance metrics}
#'     \item{return_fit}{Logical indicating whether to return fitted models}
#'   }
#' @param roi A list containing at least:
#'   \describe{
#'     \item{train_roi}{Training data as a NeuroVec or NeuroSurfaceVector object}
#'   }
#' @param id Identifier for the current analysis
#'
#' @return A tibble containing:
#'   \describe{
#'     \item{result}{List of prediction results for each fold}
#'     \item{indices}{ROI indices used in the analysis}
#'     \item{performance}{Performance metrics if compute_performance is TRUE}
#'     \item{id}{Analysis identifier}
#'     \item{error}{Logical indicating if an error occurred}
#'     \item{error_message}{Error message if applicable}
#'     \item{warning}{Logical indicating if a warning occurred}
#'     \item{warning_message}{Warning message if applicable}
#'   }
#'
#' @details
#' The function performs the following steps:
#' 1. Generates cross-validation samples using the specified scheme
#' 2. For each fold:
#'    - Checks for minimum feature requirements
#'    - Trains the model on the training set
#'    - Makes predictions on the test set
#'    - Formats and stores results
#' 3. Merges results across all folds
#'
#' @note
#' This is an internal function used by mvpa_iterate and should not be called directly.
#' It assumes that input validation has already been performed.
#'
#' @keywords internal
#' @noRd
internal_crossval <- function(mspec, roi, id) {
  
  # Generate cross-validation samples
  # Note: This step could potentially be moved outside the function
  samples <- crossval_samples(mspec$crossval, tibble::as_tibble(neuroim2::values(roi$train_roi), 
                                                                .name_repair=.name_repair), y_train(mspec))

  # Get ROI indices
  ind <- neuroim2::indices(roi$train_roi)

 

  # Iterate through the samples and fit the model
  ret <- samples %>% pmap(function(ytrain, ytest, train, test, .id) {
    # Check if the number of features is less than 2
    if (ncol(train) < 2) {
      # Return an error message
      return(
        format_result(mspec, NULL, error_message="error: less than 2 features", context=list(roi=roi, ytrain=ytrain, ytest=ytest, train=train, test=test, .id=.id))
      )
    }

   
   
    # Train the model
    result <- try(train_model(mspec, 
                              tibble::as_tibble(train, .name_repair=.name_repair), 
                              ytrain,
                              indices=ind))
   

    # Check if there was an error during model fitting
    if (inherits(result, "try-error")) {
      flog.warn("error fitting model %s : %s", id, attr(result, "condition")$message)
      # Store error messages
      emessage <- if (is.null(attr(result, "condition")$message)) "" else attr(result, "condition")$message
      format_result(mspec, result=NULL, error_message=emessage, context=list(roi=roi, ytrain=ytrain, ytest=ytest, train=train, test=test, .id=.id))
    } else {
      # Predict on test data
      format_result(mspec, result, error_message=NULL, context=list(roi=roi, ytrain=ytrain, ytest=ytest, train=train, test=test, .id=.id))
    }
  }) %>% purrr::discard(is.null) %>% dplyr::bind_rows()
  

  merge_results(mspec, ret, indices=ind, id=id)
}

    


#' @keywords internal
#' @noRd
extract_roi <- function(sample, data) {
  r <- as_roi(sample,data)
  v <- neuroim2::values(r$train_roi)
  
  # Use silent=TRUE to prevent error messages from being displayed on the console
  r <- try(filter_roi(r), silent=TRUE)
  
  if (inherits(r, "try-error") || ncol(v) < 2) {
    # Only log at debug level so these expected errors don't alarm users
    if (inherits(r, "try-error")) {
      futile.logger::flog.debug("Skipping ROI: insufficient valid columns")
    } else if (ncol(v) < 2) {
      futile.logger::flog.debug("Skipping ROI: less than 2 columns")
    }
    NULL
  } else {
    r
  }
}
  
#' Iterate MVPA Analysis Over Multiple ROIs
#' 
#' @description
#' Performs multivariate pattern analysis (MVPA) across multiple regions of interest (ROIs) 
#' using batch processing and parallel computation.
#'
#' @param mod_spec An MVPA model specification object containing the dataset to analyze,
#'        compute_performance (logical indicating whether to compute performance metrics),
#'        and return_predictions (logical indicating whether to return predictions).
#' @param vox_list A list of voxel indices or coordinates defining each ROI to analyze.
#' @param ids Vector of identifiers for each ROI analysis. Defaults to 1:length(vox_list).
#' @param batch_size Integer specifying number of ROIs to process per batch.
#'        Defaults to 10\% of total ROIs.
#' @param verbose Logical indicating whether to print progress messages. Defaults to TRUE.
#' @param processor Optional custom processing function. If NULL, uses default processor.
#'        Must accept parameters (obj, roi, rnum) and return a tibble.
#'
#' @details
#' The function processes ROIs in batches to manage memory usage. For each batch:
#' \enumerate{
#'   \item Extracts ROI data from the dataset.
#'   \item Filters out ROIs with fewer than 2 voxels.
#'   \item Processes each ROI using either the default or custom processor.
#'   \item Combines results across all batches.
#' }
#'
#' @return A tibble containing results for each ROI with columns:
#' \describe{
#'   \item{result}{List column of analysis results (NULL if return_predictions=FALSE).}
#'   \item{indices}{List column of ROI indices used.}
#'   \item{performance}{List column of performance metrics (if computed).}
#'   \item{id}{ROI identifier.}
#'   \item{error}{Logical indicating if an error occurred.}
#'   \item{error_message}{Error message if applicable.}
#'   \item{warning}{Logical indicating if a warning occurred.}
#'   \item{warning_message}{Warning message if applicable.}
#' }
#'
#' @importFrom furrr future_pmap
#' @importFrom purrr map
#' @export
mvpa_iterate <- function(mod_spec, vox_list, ids = 1:length(vox_list), 
                         batch_size = as.integer(.1 * length(ids)),
                         verbose = TRUE,
                         processor = NULL) {
  setup_mvpa_logger()
  
  if (length(vox_list) == 0) {
    futile.logger::flog.warn("⚠ Empty voxel list provided. No analysis to perform.")
    return(tibble::tibble())
  }
  
  futile.logger::flog.debug("Starting mvpa_iterate with %d voxels", length(vox_list))
  
  tryCatch({
    assert_that(length(ids) == length(vox_list), 
                msg = paste("length(ids) = ", length(ids), "::", "length(vox_list) =", length(vox_list)))
    
    batch_size <- max(1, batch_size)
    nbatches <- ceiling(length(ids) / batch_size)
    batch_group <- sort(rep(1:nbatches, length.out = length(ids)))
    batch_ids <- split(1:length(ids), batch_group)
    rnums <- split(ids, batch_group)
    
    dset <- mod_spec$dataset
    tot <- length(ids)
    
    results <- vector("list", length(batch_ids))
    skipped_rois <- 0
    processed_rois <- 0
    
    for (i in seq_along(batch_ids)) {
      tryCatch({
        if (verbose) {
          futile.logger::flog.info("⚡ Processing batch %s/%s", 
                                  crayon::blue(i), 
                                  crayon::blue(nbatches))
        }
        
        vlist <- vox_list[batch_ids[[i]]]
        size <- sapply(vlist, function(v) length(v))
        
        futile.logger::flog.debug("Processing batch %d with %d voxels", i, length(vlist))
        
        sf <- get_samples(mod_spec$dataset, vox_list[batch_ids[[i]]]) %>% 
          mutate(.id=batch_ids[[i]], rnum=rnums[[i]], size=size) %>% 
          filter(size>=2)
        
        futile.logger::flog.debug("Sample frame has %d rows after filtering", nrow(sf))
        
        if (nrow(sf) > 0) {
          sf <- sf %>% 
            rowwise() %>% 
            mutate(roi=list(extract_roi(sample,dset))) %>% 
            select(-sample)
          
          # Strip the dataset from mod_spec before passing to parallel workers
          mod_spec_stripped <- strip_dataset(mod_spec)
          
          # Pass the stripped version
          results[[i]] <- run_future(mod_spec_stripped, sf, processor, verbose)
          processed_rois <- processed_rois + nrow(sf)
          
          futile.logger::flog.debug("Batch %d produced %d results", i, nrow(results[[i]]))
        } else {
          skipped_rois <- skipped_rois + length(batch_ids[[i]])
          futile.logger::flog.warn("%s Batch %s: All ROIs filtered out (size < 2 voxels)", 
                                  crayon::yellow("⚠"),
                                  crayon::blue(i))
          results[[i]] <- tibble::tibble(
            result = list(NULL),
            indices = list(NULL),
            performance = list(NULL),
            id = rnums[[i]],
            error = TRUE,
            error_message = "ROI filtered out (size < 2 voxels)",
            warning = TRUE,
            warning_message = "ROI filtered out (size < 2 voxels)"
          )
        }
        
      }, error = function(e) {
        futile.logger::flog.error("Batch %d failed: %s", i, e$message)
        NULL
      })
    }
    
    # Final summary log
    futile.logger::flog.info("\n✨ MVPA Iteration Complete\n├─ Total ROIs: %s\n├─ Processed: %s\n└─ Skipped: %s",
                            crayon::blue(tot),
                            crayon::blue(processed_rois),
                            crayon::yellow(skipped_rois))
    
    # Combine all results
    final_results <- dplyr::bind_rows(results)
    final_results
  }, error = function(e) {
    futile.logger::flog.error("mvpa_iterate failed: %s", e$message)
    return(tibble::tibble())
  })
}

#' @noRd
run_future.default <- function(obj, frame, processor=NULL, verbose=FALSE, ...) {
  gc()
  total_items <- nrow(frame)
  processed_items <- 0
  error_count <- 0
  
  do_fun <- if (is.null(processor)) {
    function(obj, roi, rnum) {
      process_roi(obj, roi, rnum)
    }
  } else {
    processor
  }
  
  results <- frame %>% furrr::future_pmap(function(.id, rnum, roi, size) {
    # Update progress based on actual items processed
    processed_items <<- processed_items + 1
    
    if (verbose && (processed_items %% 100 == 0)) {
      progress_percent <- as.integer(processed_items/total_items * 100)
      futile.logger::flog.info("↻ Batch Progress: %s%% (%d/%d ROIs)", 
                              crayon::blue(progress_percent),
                              processed_items,
                              total_items)
    }
    
    tryCatch({
      if (is.null(roi)) {
        # ROI failed validation, but this is an expected case in searchlights
        error_count <<- error_count + 1
        futile.logger::flog.debug("ROI %d: Skipped (failed validation)", rnum)
        return(tibble::tibble(
          result = list(NULL),
          indices = list(NULL),
          performance = list(NULL),
          id = rnum,
          error = TRUE,
          error_message = "ROI failed validation (insufficient valid columns)",
          warning = TRUE,
          warning_message = "ROI failed validation (insufficient valid columns)"
        ))
      }
      
      result <- do_fun(obj, roi, rnum)
      
      if (!obj$return_predictions) {
        result <- result %>% mutate(result = list(NULL))
      }
      
      result
    }, error = function(e) {
      error_count <<- error_count + 1
      # Use debug level to avoid alarming users with expected errors
      futile.logger::flog.debug("ROI %d: Processing error (%s)", rnum, e$message)
      tibble::tibble(
        result = list(NULL),
        indices = list(NULL),
        performance = list(NULL),
        id = rnum,
        error = TRUE,
        error_message = paste("Error processing ROI:", e$message),
        warning = TRUE,
        warning_message = paste("Error processing ROI:", e$message)
      )
    })
  }, .options=furrr::furrr_options(seed=TRUE))
  
  # Log summary of errors if any occurred
  if (error_count > 0 && verbose) {
    futile.logger::flog.info("%s %d ROIs skipped or had processing errors (expected in searchlight analysis)", 
                            crayon::yellow("⚠"),
                            error_count)
  }
  
  results %>% dplyr::bind_rows()
}
</file>

</files>
