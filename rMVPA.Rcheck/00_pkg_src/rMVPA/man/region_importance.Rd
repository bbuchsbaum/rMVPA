% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/allgeneric.R, R/importance.R
\name{region_importance}
\alias{region_importance}
\alias{region_importance.mvpa_model}
\title{Region Importance via Random Subset Comparison}
\usage{
region_importance(model_spec, ...)

\method{region_importance}{mvpa_model}(
  model_spec,
  n_iter = 200,
  subset_fraction = 0.5,
  metric = NULL,
  ...
)
}
\arguments{
\item{model_spec}{An \code{mvpa_model} specification.}

\item{...}{Additional arguments (currently unused).}

\item{n_iter}{Number of random subset iterations (default 200).}

\item{subset_fraction}{Fraction of features sampled per iteration (default 0.5).}

\item{metric}{Character name of performance metric to use (e.g. "Accuracy").
NULL uses the first metric returned by \code{compute_performance}.}
}
\value{
A \code{region_importance_result} object.

A \code{region_importance_result} object.
}
\description{
Assess each feature's (region/parcel/voxel) contribution to model performance
by comparing cross-validated accuracy when the feature is included vs. excluded
across many random feature subsets.
}
\details{
\strong{Interpretation.}
\code{region_importance} measures each feature's \emph{marginal predictive
contribution}: how much cross-validated performance improves, on average,
when the feature is included versus excluded across random feature subsets.
This is an approximation of Shapley values and is purely a \strong{decoding}
(backward) measure -- it never examines model weights.

\strong{Haufe et al. (2014) considerations.}
Because this method works through out-of-sample performance deltas rather
than weight interpretation, it sidesteps the core failure mode described by
Haufe et al. (2014), where backward-model weights are misinterpreted as
activation patterns.

However, \emph{suppressor variables} -- features that improve classification
by cancelling correlated noise rather than carrying signal -- will receive
positive importance, since they genuinely improve generalization.
This is correct from a decoding perspective ("which features help classify?")
but potentially misleading from a neuroscience perspective ("where does the
signal originate?").

\strong{Complementary methods.}
For forward-model (activation-pattern) interpretation of linear models,
use \code{\link{haufe_importance}} directly or rely on the
\code{importance_vector} returned by \code{\link{run_global}}, which uses
\code{\link{model_importance}} internally.  For non-linear models such as
random forests, \code{region_importance} is the recommended approach.
}
