% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/allgeneric.R
\name{run_searchlight}
\alias{run_searchlight}
\title{Run Searchlight Analysis}
\usage{
run_searchlight(
  model_spec,
  radius,
  method = c("standard", "randomized", "resampled"),
  niter = 4,
  backend = c("default", "shard", "auto"),
  ...
)
}
\arguments{
\item{model_spec}{A \code{mvpa_model} instance containing the model specifications}

\item{radius}{The searchlight radius in millimeters}

\item{method}{The type of searchlight, either 'randomized' or 'standard'}

\item{niter}{The number of searchlight iterations (used only for 'randomized' method)}

\item{backend}{Execution backend: \code{"default"} (standard pipeline),
\code{"shard"} (shared-memory backend), or \code{"auto"} (try shard and
fall back to default).}

\item{...}{Extra arguments passed to specific searchlight methods. Currently supported:
\itemize{
  \item \code{batch_size}: Integer specifying how many searchlights (ROIs) are grouped
        into a single batch for processing by \code{\link{mvpa_iterate}}. Each batch is
        launched sequentially, while ROIs within a batch are processed in parallel (using
        the active \pkg{future}/\pkg{furrr} plan). The default is 10\% of the total number
        of searchlights. Smaller values lower peak memory usage and can improve load
        balancing, at the cost of more scheduling overhead; larger values reduce overhead
        but require more memory per worker. As a rule of thumb, start with the default,
        decrease \code{batch_size} if you hit memory limits, and increase it if you have
        many ROIs, ample RAM, and see low CPU utilization.
}}
}
\value{
A named list of \code{NeuroVol} objects containing performance metrics (e.g., AUC) at each voxel location
}
\description{
Execute a searchlight analysis using multivariate pattern analysis.
}
\examples{
\donttest{
  # Generate sample dataset with categorical response
  dataset <- gen_sample_dataset(
    D = c(8,8,8),           # 8x8x8 volume
    nobs = 100,             # 100 observations
    response_type = "categorical",
    data_mode = "image",
    blocks = 3,             # 3 blocks for cross-validation
    nlevels = 2             # binary classification
  )
  
  # Create cross-validation specification using blocks
  cval <- blocked_cross_validation(dataset$design$block_var)
  
  # Load the SDA classifier (Shrinkage Discriminant Analysis)
  model <- load_model("sda_notune")
  
  # Create MVPA model
  mspec <- mvpa_model(
    model = model,
    dataset = dataset$dataset,
    design = dataset$design,
    model_type = "classification",
    crossval = cval
  )
  
  # Run searchlight analysis
  results <- run_searchlight(
    mspec,
    radius = 8,            # 8mm radius
    method = "standard"    # Use standard searchlight
  )
  
  # Run with custom batch size for memory management
  # results <- run_searchlight(
  #   mspec,
  #   radius = 8,
  #   method = "standard",
  #   batch_size = 500      # Process 500 searchlights per batch
  # )
  
  # Results contain performance metrics
  # Access them with results$performance
}

}
