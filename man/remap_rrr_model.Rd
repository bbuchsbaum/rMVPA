% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/remap_rrr_model.R
\name{remap_rrr_model}
\alias{remap_rrr_model}
\title{REMAP-RRR: Residual low-rank, domain-adaptive cross-decoding}
\usage{
remap_rrr_model(
  dataset,
  design,
  base_classifier = "corclass",
  link_by = NULL,
  center = TRUE,
  shrink_whiten = TRUE,
  rank = "auto",
  max_rank = 20,
  forward_adapt = TRUE,
  ridge_rrr_lambda = NULL,
  leave_one_key_out = TRUE,
  min_pairs = 5,
  save_fold_singulars = FALSE,
  return_adapter = TRUE,
  lambda_grid = c(0, 0.25, 0.5, 0.75, 1),
  return_diag = FALSE,
  ...
)
}
\arguments{
\item{dataset}{mvpa_dataset with `train_data`, optional `test_data`, and `mask`.}

\item{design}{mvpa_design with `y_train`, optional `y_test`, and design tables.}

\item{base_classifier}{Name in the rMVPA model registry (kept for compatibility; ignored by REMAP which uses correlation to templates internally).}

\item{link_by}{Optional column present in both `train_design` and `test_design` used to pair
source and target trials (e.g., "ImageID"). If `NULL`, pairs by class labels (`y`).}

\item{center}{logical; center prototypes before whitening (default TRUE).}

\item{shrink_whiten}{logical; use `corpcor::cov.shrink` for joint whitening (default TRUE).}

\item{rank}{"auto" for `rrpack::cv.rrr` rank selection, integer for fixed rank, or `0`
for identity/no adaptation fallback.}

\item{max_rank}{Upper bound on rank search (default 20).}

\item{forward_adapt}{Logical; kept for compatibility, currently ignored.}

\item{ridge_rrr_lambda}{Optional numeric lambda for `rrpack::rrs.fit` (ridge RRR) on the residuals.}

\item{leave_one_key_out}{logical; if TRUE, use leave-one-key-out (LOKO) over items for adaptor learning (default TRUE).}

\item{min_pairs}{Minimum number of paired prototypes required for adaptor fitting (default 5).}

\item{save_fold_singulars}{logical; if TRUE, save singular values from each fold's adaptor (default FALSE).}

\item{return_adapter}{logical; if TRUE, store diagnostics (e.g., per-voxel R2, per-fold stats) in `result$predictor`.}

\item{lambda_grid}{Numeric vector of candidate \eqn{\lambda} values for shrinkage toward naïve; defaults to `c(0, .25, .5, .75, 1)`.}

\item{return_diag}{Logical; reserved for future detailed diagnostics (currently unused).}

\item{...}{Additional arguments passed to `create_model_spec`.}
}
\value{
A model spec of class `remap_rrr_model` compatible with `run_regional()` / `run_searchlight()`.
}
\description{
Domain-adaptive cross-decoding that models memory as "perception + low-rank correction"
in a jointly whitened space. Given paired item/class prototypes from a source
domain (e.g., perception) and a target domain (e.g., memory), REMAP learns a
reduced-rank residual map \eqn{\Delta} by regressing the residuals \eqn{R_w = Y_w - X_w}
on the whitened source prototypes \eqn{X_w}. Predicted target templates are then
\deqn{\hat Y_w = X_w + \lambda X_w \Delta,}
where \eqn{\lambda \in [0,1]} is chosen on the training items (prototype self-retrieval).
Classification of held-out target trials is performed by correlating their whitened
patterns with \eqn{\hat Y_w}. When the data do not support a reliable residual, the
method automatically shrinks to the naïve cross-decoder (\eqn{\lambda = 0}).
}
\section{Key ideas}{

\itemize{
  \item Joint whitening (shared covariance) puts perception and memory in the same coordinates.
  \item Residual RRR learns a low-rank correction \eqn{\Delta} on \eqn{R_w = Y_w - X_w}.
  \item Automatic shrinkage to naïve via \eqn{\lambda} selection on training items.
  \item Prediction/classification by correlation to \eqn{\hat Y_w} in the whitened space.
  \item Fallback to naïve cross-decoding when `rank = 0` or `rrpack` is unavailable.
}
}

\section{LOKO vs. single-fit adapters}{

When `leave_one_key_out = TRUE`, REMAP performs a leave-one-key-out (LOKO)
loop over items to estimate adaptor diagnostics (e.g. per-voxel R2 and
improvement over naïve) in an item-wise cross-validated fashion. This is
computationally heavy but yields less optimistic adaptor-level summaries.
Test-set classification performance (accuracy/AUC on `dataset$test_data`)
is always evaluated on trials that were not used to fit the adaptor, so
standard train→test performance remains out-of-sample regardless of the
LOKO setting. For large searchlight analyses where adaptor diagnostics are
secondary, it is often practical to set `leave_one_key_out = FALSE` and use
a fixed rank to obtain much faster single-fit adapters per ROI.
}

