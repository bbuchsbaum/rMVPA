---
title: Searchlight Analysis
author: Bradley Buchsbaum
date: '`r Sys.Date()`'
output: rmarkdown::html_vignette
vignette: '%\VignetteIndexEntry{Searchlight Analysis} %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}'
params:
  family: red
css: albers.css
resource_files:
- albers.css
- albers.js
includes:
  in_header: |-
    <script src="albers.js"></script>
    <script>document.addEventListener('DOMContentLoaded',()=>document.body.classList.add('palette-red'));</script>

---


## Searchlight Analysis
```{r, echo=FALSE, message=FALSE}
suppressPackageStartupMessages(library(neuroim2))
##suppressPackageStartupMessages(library(devtools))
library(rMVPA)

# Safe histogram helper: prefer ggplot2; otherwise print a compact summary
safe_plot_hist <- function(x, title = "Histogram", bins = 30) {
  x <- as.numeric(x)
  x <- x[is.finite(x)]
  if (!length(x)) {
    message("No finite AUC values to plot.")
    return(invisible(NULL))
  }
  if (requireNamespace("ggplot2", quietly = TRUE)) {
    df <- data.frame(x = x)
    p <- ggplot2::ggplot(df, ggplot2::aes(x = x)) +
      ggplot2::geom_histogram(bins = bins, fill = "grey80", color = "white") +
      ggplot2::labs(title = title, x = "AUC", y = "Count") +
      ggplot2::theme_minimal()
    print(p)
  } else {
    smry <- sprintf("AUC summary: n=%d, mean=%.3f, sd=%.3f, min=%.3f, max=%.3f", 
                    length(x), mean(x), stats::sd(x), min(x), max(x))
    message(smry)
  }
}

# Light build mode for pkgdown to avoid long-running chunks
light_build <- identical(Sys.getenv("IN_PKGDOWN"), "true")
```

### Simulated data

We first create a small volumetric dataset for demonstration. The call below generates a 4D image with 6×6×6 spatial voxels and 80 observations along the time dimension. Observations are grouped into 4 blocks (20 trials each), and the response factor `y` has two levels. The return value contains both the data (`mvpa_dataset`) and the design (`mvpa_design`).


```{r}

dataset <- gen_sample_dataset(D=c(6,6,6), nobs = 80, blocks=4, nlevels=2)
print(dataset)
```

### Cross‑validation

Because trials are organized into runs (blocks), we use a blocked cross‑validation scheme: train on `k−1` blocks and test on the held‑out block. This leave‑one‑group‑out strategy respects temporal correlations within runs and yields a more realistic estimate of generalization.

```{r}
block <- dataset$design$block_var
crossval <- blocked_cross_validation(block)
crossval
```

### Model specification

We use the shrinkage discriminant analysis model (`sda_notune`), which estimates its shrinkage parameter from the training folds. See the [sda package](https://cran.r-project.org/web/packages/sda/index.html) for details.

```{r}
sda_model <- load_model("sda_notune") 
model <- mvpa_model(model=sda_model, dataset=dataset$dataset, design=dataset$design, crossval=crossval)
model
```

### Standard searchlight

`run_searchlight()` returns image volumes with performance metrics at each sphere center. For two‑class problems we report cross‑validated accuracy and AUC (centered at 0 by subtracting 0.5). The `radius` is in millimeters; `method = "standard"` evaluates one model per sphere centered on each voxel.

```{r, eval=!light_build}
result <- run_searchlight(model, radius=4, method="standard")
result
```

```{r, echo=FALSE, eval=!light_build}
if (!is.null(result$results$AUC)) {
  auc_vals <- as.numeric(neuroim2::values(result$results$AUC))
  auc_vals <- auc_vals[is.finite(auc_vals)]
  if (length(auc_vals) > 0) safe_plot_hist(auc_vals, title = "Searchlight AUC (centered)")
}
```

## Randomized searchlight

The randomized variant samples non‑overlapping spheres and propagates each sphere’s score to all voxels it covers. Repeating this for `niter` exhaustive passes yields, at each voxel, the average performance across the spheres that included it. This can improve spatial localization and often reduces computation, since the number of models scales with `nvoxels / radius × niter` rather than the total number of voxels.

```{r, eval=!light_build}
result <- run_searchlight(model, radius=4, method="randomized", niter=8)
result
```

```{r, echo=FALSE, eval=!light_build}
if (!is.null(result$results$AUC)) {
  auc_vals <- as.numeric(neuroim2::values(result$results$AUC))
  auc_vals <- auc_vals[is.finite(auc_vals)]
  if (length(auc_vals) > 0) safe_plot_hist(auc_vals, title = "Randomized Searchlight AUC (centered)")
}
```

## Using different classifiers

Any classifier in the model registry can be used in a searchlight. You can also register your own with `register_mvpa_model()`. For example, to run a linear SVM:

```{r, eval=FALSE}
svm_model <- load_model("svmLinear") 
model <- mvpa_model(model=svm_model, dataset=dataset$dataset, design=dataset$design, crossval=crossval)
result_svm <- run_searchlight(model, radius=4, method="randomized", niter=2)
```


For random forests, you can either fix `mtry` or tune it over a small grid. Tuning quickly becomes expensive in whole‑brain analyses, so `sda_notune` is often a good default when compute is limited.

```{r, eval=!light_build}
if (requireNamespace("randomForest", quietly = TRUE)) {
  rf_model <- load_model("rf")
  model <- mvpa_model(model = rf_model, dataset = dataset$dataset, design = dataset$design,
                      crossval = crossval, tune_grid = data.frame(mtry = 2))
  result_rf <- run_searchlight(model, radius = 4, method = "randomized", niter = 2)
  result_rf
} else {
  message("Package 'randomForest' not installed; skipping RF example.")
}
```

```{r, message=FALSE, warning=FALSE, eval=!light_build}
if (requireNamespace("randomForest", quietly = TRUE)) {
  grid <- data.frame(mtry = c(2, 4, 6, 8))
  model2 <- mvpa_model(model = rf_model, dataset = dataset$dataset, design = dataset$design,
                       crossval = crossval, tune_grid = grid, tune_reps = 2)
  result_rf_tuned <- run_searchlight(model2, radius = 6, method = "randomized", niter = 1)
result_rf_tuned
} else {
  message("Package 'randomForest' not installed; skipping tuned RF example.")
}
```

## Domain-adaptive cross-decoding (REMAP‑RRR)

REMAP‑RRR learns a low‑rank affine map from a source domain (e.g., perception) to a target domain (e.g., memory) and trains a classifier in the target space. Provide an external test set in your dataset/design (same pattern as localizer→WM), and optionally a `link_by` column shared by train/test designs to pair items across domains.

```{r, eval=!light_build}
# Synthetic source→target setup
toy <- gen_sample_dataset(D = c(6,6,6), nobs = 120, nlevels = 4, blocks = 3, external_test = TRUE)
regionMask <- neuroim2::NeuroVol(sample(1:5, size = length(toy$dataset$mask), replace = TRUE),
                                 neuroim2::space(toy$dataset$mask))

mspec <- remap_rrr_model(
  dataset = toy$dataset,
  design  = toy$design,
  base_classifier   = "sda_notune",
  link_by           = NULL,     # defaults to class-wise pairing
  rank              = 0,        # identity fallback (no rrpack required)
  leave_one_key_out = TRUE
)
remap_res <- run_regional(mspec, regionMask)
names(remap_res$vol_results)  # includes adapter_rank and adapter_mean_r2
```

Set `rank = "auto"` or a positive integer to enable reduced‑rank mapping via `rrpack`.

<!-- ## External Cross-Validation Example -->

<!-- Instead of using internal cross-validation, as we used in the above examples, sometimes one might have a held-out test data that can be used as a validation set. Here we show how to  -->

<!-- ```{r} -->
<!-- # Create training data -->
<!-- train_design <- data.frame(y = rep(letters[1:4], 5), x1 = rnorm(20), x2 = rnorm(20), block = rep(1:4, each = 5)) -->

<!-- # Create test data -->
<!-- test_design <- data.frame(y = sample(letters[1:4], 10, replace=TRUE)) -->

<!-- # Create an MVPA design object with external cross-validation -->
<!-- design <- mvpa_design(train_design, ~ y, block_var = ~ block, test_design = test_design, y_test = ~ y) -->

<!-- # Display the MVPA design object -->
<!-- print(design) -->

<!-- # Create a new dataset for external cross-validation -->
<!-- dataset_ext <- list(dataset = dataset$dataset, design = design) -->

<!-- # Construct an `mvpa_model` object with a Shrinkage Discriminant Analysis classifier (`sda_notune`) -->
<!-- sda_model_ext <- load_model("sda_notune") -->
<!-- model_ext <- mvpa_model(model = sda_model_ext, dataset = dataset_ext$dataset, design = dataset_ext$design) -->

<!-- # Run a randomized searchlight analysis using external cross-validation -->
<!-- result_ext <- run_searchlight(model_ext, radius = 4, method = "randomized", niter = 8) -->

<!-- # Display the results -->
<!-- print(result_ext) -->

<!-- # Plot a histogram of AUC values -->
<!-- hist(result_ext$AUC) -->
<!-- ``` -->
