Codebase Analysis for: .

Directory Structure:
└── .
    ├── utils.R
    ├── resample.R
    ├── vector_rsa_model.R
    ├── performance.R
    ├── regional.R
    ├── manova_model.R
    ├── design.R
    ├── mvpa_result.R
    ├── model_fit.R
    ├── allgeneric.R
    ├── globals.R
    ├── classifiers.R
    ├── mvpa_model.R
    ├── rMVPA.R
    ├── common.R
    ├── searchlight.R
    ├── dataset.R
    ├── feature_selection.R
    ├── rsa_model.R
    ├── crossval.R
    ├── mvpa_iterate.R
    ├── feature_rsa_model.R
    ├── roisplit.R
    └── distcalc.R

Summary:
Total files analyzed: 24
Total directories analyzed: 0
Estimated output size: 284.39 KB
Actual analyzed size: 281.07 KB
Total tokens: 75900
Actual text content size: 280.02 KB

File Contents:

==================================================
File: ./utils.R
==================================================

#' Compute Group Means of a Matrix
#'
#' This function calculates the average vector for each level of a grouping variable in a given matrix.
#'
#' @param X A matrix for which group means should be calculated.
#' @param margin An integer specifying the margin to average over. Use 1 for averaging over rows, and 2 for averaging over columns.
#' @param group A grouping variable, either a factor or an integer vector, that defines the groups to calculate the means for.
#' @return A matrix with the same number of rows or columns (depending on the margin) as the input matrix X, and the number of columns or rows corresponding to the number of unique groups in the grouping variable.
#' @examples
#' # Create a random matrix
#' data <- matrix(rnorm(100 * 100), 100, 100)
#'
#' # Define a grouping variable
#' groups <- factor(rep(1:5, each = 20))
#'
#' # Calculate group means for each row
#' row_means <- group_means(data, margin = 1, group = groups)
#'
#' # Calculate group means for each column
#' col_means <- group_means(data, margin = 2, group = groups)
#' @export
group_means <- function(X, margin, group) {
  if (margin == 1) {
    xsum <- rowsum(X, group)
    sweep(xsum, 1, table(group), "/") 
  } else if (margin == 2) {
    xsum <- rowsum(t(X), group)
    t(sweep(xsum, 1, table(group), "/"))
  } else {
    stop("'margin' must be 1 or 2")
  }
}

#' @noRd
spearman_cor <- function(x, y=NULL, use="everything") {
  cor(x,y,use, method="spearman")
}

#' @noRd
kendall_cor <- function(x, y=NULL, use="everything") {
  cor(x,y,use, method="kendall")
}

#' @noRd
zeroVarianceColumns <- function(M) {
  which(apply(M, 2, sd, na.rm=TRUE) == 0)
}

#' @keywords internal
#' @noRd
zeroVarianceColumns2 <- function(M) {
  apply(M, 2, sd, na.rm=TRUE) == 0
}

#' @keywords internal
#' @noRd
na_cols <- function(M) {
  apply(M, 2, function(x) any(is.na(x)))
}

#' @keywords internal
#' @noRd
nonzeroVarianceColumns <- function(M) {
  which(apply(M, 2, sd, na.rm=TRUE) > 0)
}

#' @keywords internal
#' @noRd
nonzeroVarianceColumns2 <- function(M) {
  ret <- apply(M, 2, sd, na.rm=TRUE) > 0
  ret[is.na(ret)] <- FALSE
  ret
}

#' @noRd
removeZeroVarianceColumns <- function(M) {
  hasVariance <- which(apply(M, 2, sd, na.rm=TRUE) != 0)
  if (length(hasVariance) > 0) {
    M[, hasVariance, drop=FALSE]
  } else {
    M
  }
}

## dfferent version
## https://alistaire.rbind.io/blog/coalescing-joins/
#' Coalesce Join Two Data Frames
#'
#' This function performs a specified type of join on two data frames and then coalesces the joined columns based on their common column names.
#'
#' @param x A data frame to be joined.
#' @param y A second data frame to be joined.
#' @param by A character vector of variables to join by. If NULL (the default), the function will use the common column names in 'x' and 'y'.
#' @param suffix A character vector of length 2, specifying the suffixes to be used for making unique the common column names in 'x' and 'y'. The default is c(".x", ".y").
#' @param join A join function to be used for joining the data frames. The default is dplyr::full_join.
#' @param ... Additional arguments passed on to the join function.
#' @return A data frame resulting from the specified join operation and coalesced columns.
#' @keywords internal
coalesce_join2 <- function(x, y, 
                          by = NULL, suffix = c(".x", ".y"), 
                          join = dplyr::full_join, ...) {
  joined <- join(x, y, by = by, suffix = suffix, ...)
  # names of desired output
  cols <- union(names(x), names(y))
  
  to_coalesce <- names(joined)[!names(joined) %in% cols]
  suffix_used <- suffix[ifelse(endsWith(to_coalesce, suffix[1]), 1, 2)]
  # remove suffixes and deduplicate
  to_coalesce <- unique(substr(
    to_coalesce, 
    1, 
    nchar(to_coalesce) - nchar(suffix_used)
  ))
  
  coalesced <- purrr::map_dfc(to_coalesce, ~dplyr::coalesce(
    joined[[paste0(.x, suffix[1])]], 
    joined[[paste0(.x, suffix[2])]]
  ))
  names(coalesced) <- to_coalesce
  
  dplyr::bind_cols(joined, coalesced)[cols]
}


#' @keywords internal
coalesce_join <- function(x, y, 
                          by = NULL, suffix = c(".x", ".y"), 
                          join = dplyr::full_join, ...) {
  joined <- join(x, y, by = by, suffix = suffix, ...)
  # names of desired output
  cols <- union(names(x), names(y))
  
  to_coalesce <- names(joined)[!names(joined) %in% cols]
  
  if (length(to_coalesce) == 0) {
    ## nothing to coalesce...
    return(joined)
  }
  
  suffix_used <- suffix[ifelse(endsWith(to_coalesce, suffix[1]), 1, 2)]
  # remove suffixes and deduplicate
  to_coalesce <- unique(substr(
    to_coalesce, 
    1, 
    nchar(to_coalesce) - nchar(suffix_used)
  ))
  
  coalesced <- purrr::map_dfc(to_coalesce, ~dplyr::coalesce(
    joined[[paste0(.x, suffix[1])]], 
    joined[[paste0(.x, suffix[2])]]
  ))
  names(coalesced) <- to_coalesce
  
  dplyr::bind_cols(joined, coalesced)[cols]
}

==================================================
File: ./resample.R
==================================================
#' @keywords internal
gen_id <- function(n) {
  width <- nchar(n)
  sprintf(paste0("%0", width, "d"), seq_len(n))
}

#' @keywords internal
.get_samples <- function(obj, voxlist) {
  ret <- lapply(voxlist, function(vox) {
    sam <- data_sample(obj, vox)
  })
  
  n <- length(ret)
  df <- tibble::tibble(sample = ret)
  df[[".id"]] <- gen_id(n)
  df
}

#' @export
get_samples.mvpa_dataset <- function(obj, vox_list) {
  .get_samples(obj, vox_list)
}

#' @export
get_samples.mvpa_surface_dataset <- function(obj, vox_list) {
  .get_samples(obj, vox_list)
}


#' @export
data_sample.mvpa_dataset <- function(obj, vox) {
  structure(
    list(
      #data = obj,
      data=NULL,
      vox=vox
    ),
    class = "data_sample"
  )
}


#' @export
print.data_sample <- function(x, ...) {
  if (is.matrix(x$vox)) {
    cat("data sample with : ", nrow(x$vox), "features")
  } else {
    cat("data sample with : ", length(x$vox), "features")
  }
}

#' @keywords internal
filter_roi.default <- function(roi, ...) {
  stop("Unsupported ROI type")
}

#' @keywords internal
#' @importFrom neuroim2 ROIVec space coords values
filter_roi.ROIVec <- function(roi, ...) {
  # Extract the train data values
  trdat <- values(roi$train_roi)
  
  # Find columns with missing values (NA)
  nas <- apply(trdat, 2, function(v) any(is.na(v)))
  
  # Find columns with non-zero standard deviation
  sdnonzero <- apply(trdat, 2, sd, na.rm=TRUE) > 0
  
  # Determine columns to keep
  keep <- !nas & sdnonzero
  
  # If no valid columns are found, throw an error
  if (sum(keep) == 0) {
    stop("filter_roi: roi has no valid columns")
  }
  
  # If there's no test ROI data, return filtered train ROI data only
  if (is.null(roi$test_roi)) {
    troi <- ROIVec(space(roi$train_roi), 
                   coords(roi$train_roi)[keep,,drop=FALSE], 
                   data=trdat[,keep,drop=FALSE])
    list(train_roi=troi, test_roi=NULL)
  } else {
    # Filter train ROI data
    troi <- ROIVec(space(roi$train_roi), 
                   coords(roi$train_roi)[keep,,drop=FALSE], 
                   data=trdat[,keep,drop=FALSE])
    
    # Filter test ROI data
    tedat <- values(roi$test_roi)
    teroi <- ROIVec(space(roi$test_roi), 
                    coords(roi$test_roi)[keep,,drop=FALSE], 
                    data=tedat[,keep,drop=FALSE])
    
    list(train_roi=troi, test_roi=teroi)
  }
}

#' @keywords internal
#' @importFrom neurosurf ROISurfaceVector geometry nodes
filter_roi.ROISurfaceVector <- function(roi, ...) {
  # Extract the train data values
  trdat <- roi$train_roi@data
  
  # Find columns with missing values (NA)
  nas <- apply(trdat, 2, function(v) any(is.na(v)))
  
  # Find columns with non-zero standard deviation
  sdnonzero <- apply(trdat, 2, sd, na.rm=TRUE) > 0
  
  # Determine columns to keep
  keep <- !nas & sdnonzero
  
  # If no valid columns are found, throw an error
  if (sum(keep) == 0) {
    stop("filter_roi: roi has no valid columns")
  }
  
  # If there's no test ROI data, return filtered train ROI data only
  if (is.null(roi$test_roi)) {
    troi <- ROISurfaceVector(geometry=roi$train_roi@geometry,
                            indices=roi$train_roi@indices[keep],
                            data=trdat[,keep,drop=FALSE])
    list(train_roi=troi, test_roi=NULL)
  } else {
    # Filter train ROI data
    troi <- ROISurfaceVector(geometry=roi$train_roi@geometry,
                            indices=roi$train_roi@indices[keep],
                            data=trdat[,keep,drop=FALSE])
    
    # Filter test ROI data
    tedat <- roi$test_roi@data
    teroi <- ROISurfaceVector(geometry=roi$test_roi@geometry,
                             indices=roi$test_roi@indices[keep],
                             data=tedat[,keep,drop=FALSE])
    
    list(train_roi=troi, test_roi=teroi)
  }
}


#' @keywords internal
#' @noRd
#' @importFrom neuroim2 series_roi
as_roi.data_sample <- function(x, data, ...) {
  
  train_roi <- try(series_roi(data$train_data, x$vox))
  
  test_roi <- if (has_test_set(data)) {
    series_roi(data$test_data, x$vox)
  }
  
  #cds <- if (is.vector(x$vox)) {
  #  cds <- indexToGrid(space(x$data$mask), x$vox)
  #} else {
  #  x$vox
  #}

  if (is.null(test_roi)) {
    list(train_roi=train_roi,
         test_roi=NULL)
  } else {
    list(train_roi=train_roi,
         test_roi=test_roi)
  }
  
  
}

#' @keywords internal
#' @noRd
#' @importFrom neuroim2 space series series_roi
as.data.frame.data_sample <- function(x, data, ...) {
  train_mat <- neuroim2::series(data$train_data, x$vox)
  
  test_mat <- if (has_test_set(data)) {
    neuroim2::series(data$test_data, x$vox)
  }
  
  cds <- if (is.vector(x$vox)) {
    cds <- neuroim2::index_to_grid(space(data$mask), x$vox)
  } else {
    x$vox
  }
  
  if (!is.null(test_mat)) {
    .type <- rep(c("train", "test"), c(nrow(train_mat), nrow(test_mat)))
    ret <- as.data.frame(rbind(train_mat, test_mat))
    ret$.type <- .type
    attr(ret, "vox") <- cds
    ret
  } else {
    .type <- rep(c("train"), nrow(train_mat))
    ret <- as.data.frame(train_mat)
    ret$.type <- .type
    attr(ret, "vox") <- cds
    ret
  }
  
}
  
 


==================================================
File: ./vector_rsa_model.R
==================================================
#' Construct a design for a vectorized RSA model
#'
#' This function constructs a design for an RSA model using a single distance matrix, labels, and blocks.
#'
#' @param D A representational dissimilarity matrix with row.names indicating labels.
#' @param labels character vector of labels corresponding to rows in another dataset X.
#' @param block_var vector indicating the block (strata) each label belongs to.
#' @return A list containing the elements of the RSA design, class attributes "vector_rsa_design" and "list".
#' @details The function verifies that all labels in the 'labels' are present in the row.names of 'D'.
#' It also sets up for precomputing cross-block pairs if needed.
#' @export
vector_rsa_design <- function(D, labels, block_var) {
  # Verify that all labels are present in row.names of D
  assertthat::assert_that(all(labels %in% rownames(D)), 
                          msg = "All labels must be present in the row.names of D.")
  
  # S <- dist_to_sim(D)
  # Set up data and preprocessing steps if necessary
  # Here, the design is simple and only needs to handle labels and block structure.
  # Further preprocessing for cross-block pairs might be handled in vector_rsa_model_mat
  
  # Process block_var if provided
  block_var <- if (!is.null(block_var)) {
    parse_variable(block_var, data)
  }
  
  assertthat::assert_that(length(labels) == length(block_var), msg = "Length of labels and block_var must match.")
  
  design <- list(
    D = D,
    #S=S,
    labels = labels,
    block = block_var
  )
  
  # Add model matrix to the design list
  mmat <- vector_rsa_model_mat(design)
  design$model_mat <- mmat
  
  class(design) <- c("vector_rsa_design", "list")
  
  
  return(design)
}

#' @noRd
vector_rsa_model_mat <- function(design) {
  # Convert distance matrix to similarity matrix
  #similarity_matrix <- 1 - as.matrix(design$D)
  
  # Ensure the matrix is expanded to match all instances in X
  Dexpanded <- design$D[match(design$labels, rownames(design$D)), match(design$labels, rownames(design$D))]
  
  # Precompute cross-block pairs with indices for fast access
  label_indices <- match(design$labels, rownames(design$D))
  
  cross_block_data <- lapply(sort(unique(design$block)), function(b) {
    inds <- which(design$block != b)
    list(other_labels = design$labels[inds], indices=inds, block=b)
  })
  
  names(cross_block_data) <- unique(design$block)
  
  # Bundle precomputed data
  precomputed <- list(
    Dexpanded=Dexpanded,
    cross_block_data = cross_block_data
  )
  
  return(precomputed)
}

#' Create a vectorized RSA model
#'
#' This function integrates a vector_rsa_design and precomputes data to create a vectorized RSA model.
#' 
#' @param dataset An \code{mvpa_dataset}  object.
#' @param design A vector_rsa_design object.
#' @param distfun an object of \code{distfun} type for computation of pairwise dissimilarities among image rows
#' @param rsa_simfun a character string specifying the similarity function to use for RSA
#' @return An object representing the RSA model, which includes both the design and precomputed data.
#' @details Integrates RSA design and precomputed data to facilitate efficient RSA computations. 
#' It directly incorporates precomputations for cross-block comparisons.
#' @export
vector_rsa_model <- function(dataset, design, distfun=cordist(), rsa_simfun=c("pearson", "spearman")) {
  rsa_simfun = match.arg(rsa_simfun)
  assert_that(inherits(dataset, "mvpa_dataset"))
  # Verify that design is correctly formatted
  assertthat::assert_that(inherits(design, "vector_rsa_design"),
                          msg = "Input must be a 'vector_rsa_design' object.")
  
  # Precompute cross-block pairs
  label_indices <- match(design$labels, rownames(design$D))
  
  cross_block_data <- lapply(unique(design$block), function(b) {
    inds <- which(design$block != b)
    list(other_labels = design$labels[inds], indices=inds) #indices = label_indices[inds])
  })
  
  names(cross_block_data) <- unique(design$block)
  
  # Store precomputed data in the model
  model <- list(
    dataset = dataset,
    design = design,
    distfun=distfun,
    rsa_simfun=rsa_simfun,
    cross_block_data = cross_block_data
  )
  
  create_model_spec("vector_rsa_model", dataset, design,distfun=distfun,
                    rsa_simfun=rsa_simfun,
                    cross_block_data = cross_block_data)
  

}


#' Train a vector RSA model
#'
#' @param obj An object of class \code{vector_rsa_model}.
#' @param X The data matrix where rows correspond to trials.
#' @param train_dat the training data
#' @param indices the spatial indices of the training data
#' @param ... addiitonal arguments
#' @export
train_model.vector_rsa_model <- function(obj, train_dat, indices, ...) {
  # Compute trial scores using the similarity matrix instead of the distance matrix
  scores <- compute_trial_scores(obj, train_dat)
  return(scores)
}

# Perform vector RSA for a subset of data
#
# @param roi A subset of data, usually representing one ROI or one trial block.
# @param mod_spec The RSA model specification.
# @param rnum roi ids
# @return A tibble with RSA results and potentially error information.
# @noRd
# do_vector_rsa <- function(roi, mod_spec, rnum) {
#   xtrain <- tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair=.name_repair)
#   ind <- indices(roi$train_roi)
#   tryCatch({
#     scores <- train_model(mod_spec, xtrain)
#     tibble::tibble(result = list(NULL), indices=list(ind), performance=list(scores), id=rnum, error = FALSE, error_message = "~")
#   }, error = function(e) {
#     tibble::tibble(result = list(NULL), indices=list(ind), performance=list(NULL), id=rnum, error = TRUE, error_message = e$message)
#   })
# }



#' Iterate over data sets applying the vector RSA model
#
# @param mod_spec The model specification.
# @param vox_list A list of voxel sets to analyze.
# @param ids Identifiers for each data set.
# @noRd
# vector_rsa_iterate <- function(mod_spec, vox_list, ids = seq_along(vox_list)) {
#   # Ensure IDs match the number of data sets
#   if (length(ids) != length(vox_list)) {
#     stop("Length of ids must match the number of data sets.")
#   }
#  
#   assert_that(length(ids) == length(vox_list), msg=paste("length(ids) = ", length(ids), "::", "length(vox_list) =", length(vox_list)))
#   
#   sframe <- get_samples(mod_spec$dataset, vox_list)
#   ## iterate over searchlights using parallel futures
#   sf <- sframe %>% dplyr::mutate(rnum=ids) 
#   
#   fut_vector_rsa(mod_spec,sf)
#  
# }


# Apply the RSA model in parallel using futures
#
# @param mod_spec The model specification.
# @param sf A tibble containing the data sets and their identifiers.
# @param method Method for computing similarities.
# @return A combined result of all RSA analyses.
# @noRd
# fut_vector_rsa <- function(mod_spec, sf, ...) {
#   gc()
#   sf %>% furrr::future_pmap(function(sample, rnum, .id) {
#     do_vector_rsa(as_roi(sample, mod_spec$dataset), mod_spec, rnum, ...)
#   }, .options = furrr::furrr_options(seed = T)) %>% dplyr::bind_rows()
#    
# }

#' @noRd
#' @keywords internal
compute_trial_scores <- function(obj, X) {
  
  # Retrieve precomputed data
  precomputed = obj$design$model_mat
  dissimilarity_matrix = precomputed$Dexpanded  # Unexpanded similarity matrix
  cross_block_data = precomputed$cross_block_data
  X <- as.matrix(X)
  # Calculate similarity vectors using precomputed data
  #browser()
  second_order_similarity(obj$distfun, X, dissimilarity_matrix, obj$design$block, obj$rsa_simfun)
  
}













==================================================
File: ./performance.R
==================================================



#' Calculate the Predicted Class from Probability Matrix
#'
#' This function calculates the predicted class from a matrix of predicted probabilities. The class with the highest probability is selected as the predicted class.
#'
#' @param prob A matrix of predicted probabilities with column names indicating the classes.
#' @return A vector of predicted classes corresponding to the highest probability for each row in the input matrix.
#' @export
predicted_class <- function(prob) {
  maxid <- max.col(prob, ties.method="random")
  pclass <- colnames(prob)[maxid]
}

#' Calculate Performance Metrics for Regression Result
#'
#' This function calculates performance metrics for a regression result object, including R-squared, Root Mean Squared Error (RMSE), and Spearman correlation.
#'
#' @param x A \code{regression_result} object.
#' @param split_list Split results by indexed sub-groups (not supported for regression analyses yet).
#' @param ... extra args (not used).
#' @return A named vector with the calculated performance metrics: R-squared, RMSE, and Spearman correlation.
#' @details
#' The function calculates the following performance metrics for the given regression result object:
#' - R-squared: proportion of variance in the observed data that is predictable from the fitted model.
#' - RMSE: root mean squared error, a measure of the differences between predicted and observed values.
#' - Spearman correlation: a measure of the monotonic relationship between predicted and observed values.
#' @seealso \code{\link{regression_result}}
#' @export
performance.regression_result <- function(x, split_list,...) {
  if (!is.null(split_list)) {
    ## TODO: add support
    stop("split_by not supported for regression analyses yet.")
  }
  
  #browser()
  R2 <- 1 - sum((x$observed - x$predicted)^2)/sum((x$observed-mean(x$observed))^2)
  rmse <- sqrt(mean((x$observed-x$predicted)^2))
  rcor <- cor(x$observed, x$predicted, method="spearman")
  c(R2=R2, RMSE=rmse, spearcor=rcor)
}


#' Apply Custom Performance Metric to Prediction Result
#'
#' This function applies a user-supplied performance metric to a prediction result object.
#'
#' @param x The prediction result object.
#' @param custom_fun The function used to compute performance metrics, i.e., \code{custom_fun(x)}.
#' @param split_list An optional named list of splitting groups. If provided, the performance metric will be computed for each group and returned as a named vector.
#' @return A named vector with the calculated custom performance metric(s).
#' @details
#' The function allows users to apply a custom performance metric to a prediction result object.
#' If a split list is provided, the performance metric will be computed for each group separately, and the results will be returned as a named vector.
#' @export
custom_performance <- function(x, custom_fun, split_list=NULL) {
  if (is.null(split_list)) {
    custom_fun(x)
  } else {
    total <- custom_fun(x)
    subtots <- unlist(lapply(names(split_list), function(tag) {
      ind <- split_list[[tag]]
      ret <- custom_fun(sub_result(x, ind))
      names(ret) <- paste0(names(ret), "_", tag)
      ret
    }))
    
    c(total, subtots)
  }
  
}

#' @export
merge_results.binary_classification_result <- function(x,...) {
  rlist <- list(x,...)
  probs <- Reduce("+", lapply(rlist, function(x) x$probs))/length(rlist)
  
  mc <- max.col(probs)
  predicted <- levels(x$observed)[mc]
  binary_classification_result(observed=x$observed, predicted=predicted, probs=probs, testind=x$testind, 
                               test_design=x$test_design, predictor=x$predictor)
}

#' @export
merge_results.regression_result <- function(x,...) {
  rlist <- list(x,...)
  pred <- Reduce("+", lapply(rlist, function(x) x$predicted))/length(rlist)
  regression_result(observed=x$observed, predicted=pred, testind=x$testind, 
                               test_design=x$test_design, predictor=x$predictor)
}



#' @export
prob_observed.binary_classification_result <- function(x) {
  x$probs[cbind(seq(1,nrow(x$probs)),as.integer(x$observed))]
}

#' @export
prob_observed.multiway_classification_result <- function(x) {
  x$probs[cbind(seq(1,nrow(x$probs)),as.integer(x$observed))]
}

#' @export
merge_results.multiway_classification_result <- function(x,...) {
  
  rlist <- list(x,...)
  #ds <- sapply(rlist, function(x) nrow(x$probs))
  
  probs <- Reduce("+", lapply(rlist, function(x) x$probs))/length(rlist)
  mc <- max.col(probs)
  predicted <- levels(x$observed)[mc]
  
  multiway_classification_result(observed=x$observed, predicted=predicted, probs=probs, 
                                 testind=x$testind,  test_design=x$test_design, predictor=x$predictor)
}

#' @export
performance.binary_classification_result <- function(x, split_list=NULL,...) {
  stopifnot(length(x$observed) == length(x$predicted))
  
  if (is.null(split_list)) {
    ret <- binary_perf(x$observed, x$predicted, x$probs)
  } else {
    total <- binary_perf(x$observed, x$predicted, x$probs)
    
    subtots <- unlist(lapply(names(split_list), function(tag) {
      ind <- split_list[[tag]]
      if (!is.null(x$testind)) {
        ind <- which(x$testind %in% ind)
      }
      ret <- binary_perf(x$observed[ind], x$predicted[ind], x$probs[ind,])
      names(ret) <- paste0(names(ret), "_", tag)
      ret
    }))
    
    ret <- c(total, subtots)
  }
}


#' @export
performance.multiway_classification_result <- function(x, split_list=NULL, class_metrics=FALSE,...) {
  stopifnot(length(x$observed) == length(x$predicted))

  if (is.null(split_list)) {
    multiclass_perf(x$observed, x$predicted, x$probs, class_metrics)
  } else {
    total <- multiclass_perf(x$observed, x$predicted, x$probs, class_metrics)
    subtots <- unlist(lapply(names(split_list), function(tag) {
      ind <- split_list[[tag]]
      
      if (!is.null(x$testind)) {
        ind <- which(x$testind %in% ind)
      }
      
      ret <- multiclass_perf(x$observed[ind], x$predicted[ind], x$probs[ind,], class_metrics)
      names(ret) <- paste0(names(ret), "_", tag)
      ret
    }))
    
    c(total, subtots)
    
  }
  
}

#' @keywords internal
#' @noRd
combinedAUC <- function(Pred, Obs) {
  Obs <- as.factor(Obs)
  mean(sapply(1:ncol(Pred), function(i) {
    lev <- levels(Obs)[i]
    pos <- Obs == lev
    pclass <- Pred[,i]
    pother <- rowMeans(Pred[,-i,drop=FALSE])
    Metrics::auc(as.numeric(pos), pclass - pother)-.5
  }))
}


#' @keywords internal
#' @noRd
combinedACC <- function(Pred, Obs) {
  levs <- levels(as.factor(Obs))
  maxind <- apply(Pred, 1, which.max)
  pclass <- levs[maxind]
  sum(pclass == Obs)/length(pclass)
  
}


#' @keywords internal
binary_perf <- function(observed, predicted, probs) {
  obs <- as.character(observed)
  ncorrect <- sum(obs == predicted)
  ntotal <- length(obs)
  #maxClass <- max(table(obs))
  
  #out <- binom.test(ncorrect,
  #                  ntotal,
  #                  p = maxClass/ntotal,
  #                  alternative = "greater")
  
  
  #c(ZAccuracy=-qnorm(out$p.value), Accuracy=ncorrect/ntotal, AUC=Metrics::auc(observed == levels(observed)[2], probs[,2])-.5)
  c(Accuracy=ncorrect/ntotal, AUC=Metrics::auc(observed == levels(observed)[2], probs[,2])-.5)
  
}

#' @keywords internal
multiclass_perf <- function(observed, predicted, probs, class_metrics=FALSE) {
  
  obs <- as.character(observed)
  ntotal <- length(obs)
 
  aucres <- sapply(1:ncol(probs), function(i) {
    lev <- try(levels(observed)[i])
    pos <- obs == lev
    pclass <- probs[,i]
    pother <- rowMeans(probs[,-i, drop=FALSE])
    Metrics::auc(as.numeric(pos), pclass - pother)-.5
  })
  
  names(aucres) <- paste0("AUC_", colnames(probs))
  
  
  if (class_metrics) {
    c(Accuracy=sum(obs == as.character(predicted))/length(obs), AUC=mean(aucres, na.rm=TRUE), aucres)
  } else {
    c(Accuracy=sum(obs == as.character(predicted))/length(obs), AUC=mean(aucres, na.rm=TRUE))
  }
}
  






==================================================
File: ./regional.R
==================================================
#' @keywords internal
get_unique_regions.NeuroVol <- function(region_mask, ...) {
  sort(unique(region_mask[region_mask > 0]))
}

#' @keywords internal
get_unique_regions.NeuroSurface <- function(region_mask, ...) {
  sort(unique(region_mask@data[region_mask@data > 0]))
}

#' Combine Regional Results
#'
#' This function combines regional results from a list into a single data frame.
#'
#' @param results A list of regional results.
#' @return A data frame with combined regional results.
#' @details
#' The function is used to combine regional results from a list into a single data frame.
#' It handles both factor and non-factor observed values and creates a combined data frame with the corresponding columns.
#' @keywords internal
#' @noRd
combine_regional_results = function(results) {
  roinum=NULL
  .rownum=NULL
  
  # Check if the observed values are factors (for categorical data)
  if (is.factor(results$result[[1]]$observed)) {
    results %>% dplyr::rowwise() %>% dplyr::do( {
      
      # Create a tibble containing observed, predicted, and additional information
      tib1 <- tibble::tibble(
        .rownum=seq_along(.$result$observed),
        roinum=rep(.$id, length(.$result$observed)),
        observed=.$result$observed,
        pobserved=sapply(seq_along(.$result$observed), function(i) .$result$probs[i, .$result$observed[i]]),
        predicted=.$result$predicted,
        correct=as.character(.$result$observed) == as.character(.$result$predicted)
      )
      
      # Create a tibble with the probabilities for each class
      tib2 <- tibble::as_tibble(.$result$probs, .name_repair=.name_repair)
      names(tib2) <- paste0("prob_", names(tib2))
      
      # Combine tib1 and tib2
      cbind(tib1, tib2)
    })
  } else {
    # For non-factor observed values (for continuous data)
    results %>% dplyr::rowwise() %>% dplyr::do(
      tibble::tibble(
        .rownum=seq_along(.$result$observed),
        roinum=rep(.$id, length(.$result$observed)),
        observed=.$result$observed,
        predicted=.$result$predicted)
    )
  }
}

#' Combine prediction tables
#'
#' Combines multiple prediction tables (e.g., from different models or regions) into a single table.
#' Supports weighted combination and collapsing regions.
#'
#' @param predtabs A list of prediction tables (data frames) to be combined.
#' @param wts A vector of weights, with the same length as \code{predtabs}. Default is equal weights.
#' @param collapse_regions A logical value; if TRUE, regions are collapsed in the final prediction table.
#'
#' @return A combined prediction table (data frame).
#' @import dplyr
#' @importFrom purrr map_df
#' @examples
#' # Create example prediction tables
#' observed = factor(sample(letters[1:2], 10, replace = TRUE))
#' predtab1 <- data.frame(.rownum = 1:10,
#'                        roinum = rep(1, 10),
#'                        observed = observed,
#'                        prob_A = runif(10),
#'                        prob_B = runif(10))
#' predtab2 <- data.frame(.rownum = 1:10,
#'                        roinum = rep(2, 10),
#'                        observed = observed,
#'                        prob_A = runif(10),
#'                        prob_B = runif(10))
#'
#' # Combine the tables
#' combined_table <- combine_prediction_tables(list(predtab1, predtab2))
#' @export
combine_prediction_tables <- function(predtabs, wts=rep(1,length(predtabs)), collapse_regions=FALSE) {
  assert_that(length(predtabs) == length(wts))
  assert_that(sum(wts) > 0)
  assert_that(all(purrr::map_lgl(predtabs, is.data.frame)))
  
  wts <- wts/sum(wts)
  
  .weight <- NULL
  .rownum <- NULL
  roinum <-  NULL
  observed <- NULL
  predicted <- NULL
  
  if (is.character(predtabs[[1]]$observed) || is.factor(predtabs[[1]]$observed)) {
    ## applies constant weight to each table and concatenates
    ptab <- map(seq_along(predtabs), function(i) predtabs[[i]] %>% mutate(.tableid=i, .weight=wts[i])) %>% 
      map_df(bind_rows) %>% as_tibble(.name_repair=.name_repair)
    
    probs <- if (collapse_regions) {
      
      ptab %>% dplyr::group_by(.rownum,observed) %>% summarise_at(vars(starts_with("prob_")), 
                                                                  list(~stats::weighted.mean(., w = .weight)))
    } else {
     
      ## groups over rownames, condition, and roinum, then compute weighted means of probabilities
      ptab %>% dplyr::group_by(.rownum,observed,roinum) %>% summarise_at(vars(starts_with("prob_")), 
                                                                         list(~stats::weighted.mean(., w = .weight)))
    }
    
    p <- probs %>% ungroup() %>% dplyr::select(dplyr::starts_with("prob_"))
    pmat <- as.matrix(p)
      
    pobserved <- pmat[cbind(seq(1,nrow(probs)),as.integer(probs$observed))]
    mc <- max.col(pmat)
    preds <- levels(probs$observed)[mc]
    
    
    prediction_table <- tibble(
      .rownum=probs$.rownum,
      roinum=if (collapse_regions) 1 else probs$roinum,
      observed=probs$observed,
      pobserved=pobserved,
      predicted=preds,
      correct = predicted == probs$observed,
    ) %>% bind_cols(p)
  } else if (is.numeric(predtabs[[1]]$observed)) {
    stop("combining continuous predictions not implemented")
  }
}


#' Merge regional MVPA results
#'
#' Merge multiple regional MVPA results into a single result.
#'
#' @param x A \code{regional_mvpa_result} object.
#' @param ... Additional \code{regional_mvpa_result} objects to be merged.
#'
#' @return A merged \code{regional_mvpa_result} object.
#' @export
merge_results.regional_mvpa_result <- function(x, ...) {
  rlist <- list(x,...)
  combine_prediction_tables(rlist)
}


#' Create a \code{regional_mvpa_result} instance
#'
#' Constructs a regional MVPA result object that stores the results of MVPA analysis in a specific region.
#'
#' @param model_spec A model specification object.
#' @param performance_table A data frame with performance measures.
#' @param prediction_table A data frame with prediction results.
#' @param vol_results A list of voxel-level results.
#' @param fits Optional model fits.
#'
#' @return A \code{regional_mvpa_result} object.
#' @examples
#' # Create example inputs
#' model_spec <- list(dataset = "Example dataset")
#' performance_table <- data.frame(accuracy = c(0.8, 0.85))
#' prediction_table <- data.frame(observed = factor(rep(letters[1:2], 5)),
#'                                 predicted = factor(rep(letters[1:2], 5)))
#' vol_results <- list(vol1 = "Example vol_result 1", vol2 = "Example vol_result 2")
#' fits <- list(fit1 = "Example fit 1", fit2 = "Example fit 2")
#'
#' # Construct a regional_mvpa_result
#' regional_result <- regional_mvpa_result(model_spec, performance_table,
#'                                         prediction_table, vol_results, fits = fits)
#' @export
regional_mvpa_result <- function(model_spec, performance_table, prediction_table, vol_results, fits=fits) {
  ret <- list(model_spec=model_spec, 
              performance_table=performance_table,
              prediction_table=prediction_table,
              vol_results=vol_results,
              fits=fits)
  
  class(ret) <- c("regional_mvpa_result", "list")
  ret
  
}

#' Prepare regional data for MVPA analysis
#'
#' This function processes the input data and prepares the regions for MVPA analysis by extracting
#' voxel indices for each region of interest (ROI) specified in the region_mask.
#'
#' @param model_spec A model specification object.
#' @param region_mask A mask representing different regions in the brain image.
#'
#' @return A list containing information about the regions for further processing:
#'   * allrois: A vector of unique ROI labels.
#'   * region_vec: A vector representation of the region_mask.
#'   * region_set: A sorted vector of unique ROI labels in the region_mask.
#'   * vox_iter: A list containing voxel indices for each ROI.
#'   * lens: A vector containing the number of voxels in each ROI.
#'   * keep: A logical vector indicating if an ROI should be kept for analysis (those with more than one voxel).
#'
#' @examples
#' # Create example inputs
#' model_spec <- list(dataset = "Example dataset")
#' region_mask <- matrix(c(rep(0, 5), rep(1, 5), rep(2, 5), rep(3, 5)), nrow = 5)
#'
#' # Prepare regional data
#' regional_data <- prep_regional(model_spec, region_mask)
#' @export
prep_regional <- function(model_spec, region_mask) {
  allrois <- get_unique_regions(region_mask)
  ##allrois <- sort(unique(region_mask[region_mask>0]))
  region_vec <- as.vector(region_mask)
  region_set <- sort(as.integer(unique(region_vec[region_vec > 0])))
  if (length(region_set) < 1) {
    stop("run_regional: invalid ROI mask, number of ROIs = 0")
  }
  
  vox_iter <- lapply(region_set, function(rnum) which(region_vec == rnum & model_spec$dataset$mask > 0))
  lens <- sapply(vox_iter, length)
  keep <- lens > 1
  
  if (all(!keep)) {
    futile.logger::flog.error("run_regional: no ROIs have more than one voxel.")
    stop()
  }
  
  if (any(lens < 2)) {
    futile.logger::flog.warn(paste("some ROIs have less than two voxels, removing them from list: ", paste(region_set[lens < 2], collapse=",")))
    vox_iter <- vox_iter[keep]
    region_set <- region_set[keep]
  }
  
  list(allrois=allrois, region_vec=region_vec, region_set=region_set,
       vox_iter=vox_iter, lens=lens, keep=keep)
  
}

#' Compile performance results and generate volumetric results
#'
#' This function compiles the performance results from the regional MVPA analysis
#' and generates volumetric results based on the region_mask.
#'
#' @param results A list containing the results from regional MVPA analysis.
#' @param region_mask A mask representing different regions in the brain image.
#'
#' @return A list containing:
#'   * vols: A list of volumetric results for each performance metric.
#'   * perf_mat: A data frame containing the compiled performance results with an added 'roinum' column.
#'
#' @importFrom neuroim2 map_values
#' @keywords internal
#' @noRd
comp_perf <- function(results, region_mask) {
  roinum <- NULL
  ## compile performance results
  perf_mat <- do.call(rbind, results$performance)
  
  ## generate volumetric results
  ## TODO fails when region_mask is an logical vol
  vols <- lapply(1:ncol(perf_mat), function(i) map_values(region_mask, 
                                                          cbind(as.integer(results$id), 
                                                                perf_mat[,i])))
  names(vols) <- colnames(perf_mat)
  
  perfmat <- tibble::as_tibble(perf_mat,.name_repair=.name_repair) %>% dplyr::mutate(roinum = unlist(results$id)) %>% dplyr::select(roinum, dplyr::everything())
  list(vols=vols, perf_mat=perfmat)
}

#' Run regional MVPA analysis on a specified MVPA model
#'
#' This function runs a regional MVPA analysis using a specified MVPA model and
#' region mask. The analysis can be customized to return model fits, predictions,
#' and performance measures.
#'
#' @param model_spec An object of type \code{mvpa_model} specifying the MVPA model to be used.
#' @param region_mask A mask representing different regions in the brain image.
#' @param processor 
#' @param ... Additional arguments to be passed to the function.
#'
#' @return A \code{list} of type \code{regional_mvpa_result} containing a named list of \code{NeuroVol} objects,
#' where each element contains a performance metric and is labeled according to the metric used (e.g. Accuracy, AUC).
#'
#' @import itertools
#' @import foreach
#' @import doParallel
#' @import parallel
#' @export
run_regional.mvpa_model <- function(model_spec, region_mask, coalesce_design_vars=FALSE, processor=NULL, 
                                    verbose=FALSE, ...) {  
  ## to get rid of package check warnings
  roinum=NULL
  ###
  
  prepped <- prep_regional(model_spec, region_mask)
  #flog.info("model is: %s", model_spec$model$label)
  
  ## run mvpa for each region
  results <- mvpa_iterate(model_spec, prepped$vox_iter, ids=prepped$region_set, processor)


  perf <- if (model_spec$compute_performance) comp_perf(results, region_mask) else list(vols=list(), perf_mat=tibble())
   
  
  ## compile full prediction table
  prediction_table <- if (model_spec$return_predictions) {
    combine_regional_results(results) 
  }
  
  if (coalesce_design_vars && model_spec$return_predictions) {
    prediction_table <- coalesce_join(prediction_table, test_design(model_spec$design), 
                                      by=".rownum")
  }
  
  fits <- if (model_spec$return_fits) {
    lapply(results$result, "[[", "predictor")
  }
  
  regional_mvpa_result(model_spec=model_spec, performance_table=perf$perf_mat, 
                       prediction_table=prediction_table, vol_results=perf$vols, fits=fits)
}


#' Run regional RSA analysis on a specified RSA model
#'
#' This function runs a regional RSA analysis using a specified RSA model and
#' region mask. The analysis can be customized to return model fits and performance measures.
#'
#' @param model_spec An object of type \code{rsa_model} specifying the RSA model to be used.
#' @param region_mask A mask representing different regions in the brain image.
#' @param return_fits Whether to return model fit for every ROI (default is \code{FALSE} to save memory).
#' @param compute_performance \code{logical} indicating whether to compute performance measures (e.g. Accuracy, AUC).
#' @param regtype The regression method ("pearson", "spearman", "lm", or "rfit").
#' @param distmethod The distance computing method ("pearson" or "spearman").
#' @param coalesce_design_vars Concatenate additional design variables with output stored in `prediction_table`.
#' @param ... Additional arguments to be passed to the function.
#'
#' @return A \code{list} of type \code{regional_mvpa_result} containing a named list of \code{NeuroVol} objects,
#' where each element contains a performance metric and is labeled according to the metric used (e.g. Accuracy, AUC).
#'
#' @export
run_regional.rsa_model <- function(model_spec, region_mask, return_fits=FALSE, 
                                   compute_performance=TRUE, coalesce_design_vars=FALSE, ...) {  
  
  prepped <- prep_regional(model_spec, region_mask)
  
  results <- mvpa_iterate(model_spec, prepped$vox_iter, ids=prepped$region_set)
  perf <- if (compute_performance) comp_perf(results, region_mask) else list(vols=list(), perf_mat=tibble())
  
  regional_mvpa_result(model_spec=model_spec, performance_table=perf$perf_mat, 
                       prediction_table=NULL, vol_results=perf$vols, fits=NULL)
  
}
  




==================================================
File: ./manova_model.R
==================================================
#' Create a MANOVA Design
#'
#' This function creates a MANOVA design object containing a formula expression and a named list of data.
#'
#' @param formula A formula expression specifying the MANOVA regression model.
#' @param data A named list containing the dissimilarity matrices and any other auxiliary variables.
#' @return A MANOVA design object with class attributes "manova_design" and "list".
#' @details
#' The function takes a formula expression and a named list of data as input, and returns a MANOVA design object.
#' The object is a list that contains the formula expression and the named list of data with class attributes "manova_design" and "list".
#' This object can be further used for MANOVA analysis or other related multivariate statistical methods.
#' @importFrom assertthat assert_that
#' @importFrom purrr is_formula
#' @importFrom ffmanova ffmanova
#' @examples
#' # Create a MANOVA design
#' formula <- y ~ x1 + x2
#' data_list <- list(y = dissimilarity_matrix_y, x1 = dissimilarity_matrix_x1, x2 = dissimilarity_matrix_x2)
#' manova_design_obj <- manova_design(formula, data_list)
#' @export
manova_design <- function(formula, data) {
  assert_that(purrr::is_formula(formula))
  
  des <- list(
    formula=formula,
    data=data
  )
  class(des) <- c("manova_design", "list")
  des
}


#' Create a MANOVA Model
#'
#' This function creates a MANOVA model object containing an `mvpa_dataset` instance and a `manova_design` instance.
#'
#' @param dataset An \code{mvpa_dataset} instance.
#' @param design A \code{manova_design} instance.
#' @return A MANOVA model object with class attributes "manova_model" and "list".
#' @details
#' The function takes an `mvpa_dataset` instance and a `manova_design` instance as input, and returns a MANOVA model object.
#' The object is a list that contains the dataset and the design with class attributes "manova_model" and "list".
#' This object can be used for further multivariate statistical analysis using the MANOVA method.
#' @importFrom assertthat assert_that
#' @importFrom purrr is_formula
#' @examples
#' # Create a MANOVA model
#' dataset <- create_mvpa_dataset(data_matrix, labels, subject_ids)
#' formula <- y ~ x1 + x2
#' data_list <- list(y = dissimilarity_matrix_y, x1 = dissimilarity_matrix_x1, x2 = dissimilarity_matrix_x2)
#' design <- manova_design(formula, data_list)
#' manova_model_obj <- manova_model(dataset, design)
#' @export
manova_model <- function(dataset,
                      design) {
  
  assert_that(inherits(dataset, "mvpa_dataset"))
  assert_that(inherits(design, "manova_design"))
  
  create_model_spec("manova_model", dataset, design)
  
  
}


#' Train a MANOVA Model
#'
#' This function trains a multivariate analysis of variance (MANOVA) model using the specified design.
#'
#' @param obj An object of class \code{manova_model}.
#' @param train_dat The training data.
#' @param indices The indices of the training data.
#' @param ... Additional arguments passed to the training method.
#' @return A named numeric vector of -log(p-values) for each predictor in the MANOVA model.
#' @importFrom stats as.formula
train_model.manova_model <- function(obj, train_dat, indices, ...) {
  dframe <- obj$design$data
  dframe$response <- as.matrix(train_dat)
  form <- stats::as.formula(paste("response", paste(as.character(obj$design$formula), collapse='')))
  
  fres=ffmanova(form, data=dframe)
  pvals=fres$pValues
  names(pvals) <- sanitize(names(pvals))   
  lpvals <- -log(pvals)
  lpvals
}


#' @export
#' @method print manova_model
print.manova_model <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  formula_style <- crayon::italic$blue
  var_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ MANOVA Model ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Model Specification"), "\n")
  cat(info_style("│  └─ Formula: "), formula_style(deparse(x$design$formula)), "\n")
  
  # Dataset information
  cat(section_style("├─ Dataset"), "\n")
  dims <- dim(x$dataset$train_data)
  dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                   " × ", number_style(dims[length(dims)]), " observations")
  cat(info_style("│  ├─ Dimensions: "), dim_str, "\n")
  cat(info_style("│  └─ Type: "), class(x$dataset$train_data)[1], "\n")
  
  # Variables section
  cat(section_style("├─ Variables"), "\n")
  predictors <- all.vars(x$design$formula[[3]])  # Get predictor names from RHS of formula
  response <- all.vars(x$design$formula[[2]])    # Get response name from LHS of formula
  cat(info_style("│  ├─ Response: "), var_style(response), "\n")
  cat(info_style("│  └─ Predictors: "), var_style(paste(predictors, collapse=", ")), "\n")
  
  # Data structure
  cat(section_style("└─ Data Structure"), "\n")
  
  # Check if there's a test set
  has_test <- !is.null(x$dataset$test_data)
  cat(info_style("   ├─ Test Set: "), 
      if(has_test) crayon::green("Present") else crayon::red("None"), "\n")
  
  # Check if there's a mask
  if (!is.null(x$dataset$mask)) {
    mask_sum <- sum(x$dataset$mask > 0)
    cat(info_style("   └─ Active Voxels/Vertices: "), 
        number_style(format(mask_sum, big.mark=",")), "\n")
  } else {
    cat(info_style("   └─ Mask: "), crayon::red("None"), "\n")
  }
  
  cat("\n")
}

#' @export
#' @method print manova_design
print.manova_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  formula_style <- crayon::italic$blue
  var_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ MANOVA Design ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Formula"), "\n")
  cat(info_style("│  └─ "), formula_style(deparse(x$formula)), "\n")
  
  # Data section
  cat(section_style("└─ Variables"), "\n")
  var_names <- names(x$data)
  cat(info_style("   ├─ Total Variables: "), crayon::green(length(var_names)), "\n")
  cat(info_style("   └─ Names: "), var_style(paste(var_names, collapse=", ")), "\n")
  
  cat("\n")
}




==================================================
File: ./design.R
==================================================


#' @export
nobs.mvpa_design <- function(x) {
  length(x$y_train)
}

#' @export
nresponses.mvpa_design <- function(x) {
  if (is.factor(x$y_train)) {
    length(levels(x$y_train))
  } else if (is.vector(x$y_train)) {
    length(x$y_train)
  } else if (is.matrix(x$y_train)) {
    ncol(x$y_train)
  } else {
    stop()
  }
}

#' @export
has_test_set.mvpa_design <- function(obj) {
  !is.null(obj$y_test) 
}


#' @export
y_train.mvpa_design <- function(obj) obj$y_train


#' @export
y_test.mvpa_design <- function(obj) if (is.null(obj$y_test)) obj$y_train else obj$y_test


#' @export
test_design.mvpa_design <- function(obj) {
  if (is.null(obj$y_test)) obj$train_design else obj$test_design
}



#' @keywords internal
#' @noRd
parse_variable <- function(var, design) {
  ret <- if (purrr::is_formula(var)) {
    vnames <- all.vars(var[[2]])
    ret <- if (length(vnames) > 1) {
      do.call("interaction", c(lapply(vnames, function(vname) as.factor(design[[vname]])), sep=":"))
    } else {
      design[[vnames]]
    }
    
    assertthat::assert_that(!is.null(ret), msg=paste("formula variable", 
                                                     paste(as.character(var), collapse=" "), " not found."))
    ret
    
  } else if (is.character(var) && length(var) == 1) {
    if (is.null(design[[var]])) {
      stop(paste("`design` does not contain variable named: ", var))
    }
    design[[var]]
  } else if (is.factor(var) || is.integer(var)) {
    if (is.data.frame(design)) {
      assertthat::assert_that(nrow(design) == length(var)) 
    }
    var
  } else {
    stop("'var' must be a formula, factor, or character vector")
  }
  
  if (is.factor(ret)) {
    droplevels(ret)
  } else {
    ret
  }
  
}


#' Create an MVPA Design Object
#'
#' Creates a design object for MVPA analysis that encapsulates training and testing designs,
#' response variables, and optional blocking and splitting factors.
#'
#' @param train_design A data frame containing the training design matrix
#' @param test_design Optional data frame containing the test design matrix (default: NULL)
#' @param y_train Formula or vector specifying the training response variable
#' @param y_test Optional formula or vector specifying the test response variable (default: NULL)
#' @param block_var Optional formula or vector specifying the blocking variable for cross-validation
#' @param split_by Optional formula or vector for splitting analyses
#' @param ... Additional arguments (currently unused)
#'
#' @return An \code{mvpa_design} object (S3 class) containing:
#'   \describe{
#'     \item{train_design}{Data frame of training design}
#'     \item{test_design}{Data frame of test design (if provided)}
#'     \item{y_train}{Training response variable}
#'     \item{y_test}{Test response variable (if provided)}
#'     \item{block_var}{Blocking variable for cross-validation (if provided)}
#'     \item{split_by}{Splitting factor (if provided)}
#'   }
#'
#' @details
#' The \code{y_train} and \code{y_test} can be specified either as formulas (e.g., ~ condition) 
#' or as vectors. If formulas are used, they are evaluated within the respective design matrices.
#' 
#' The \code{block_var} and \code{split_by} can also be specified as formulas or vectors. 
#' If formulas, they are evaluated within the training design matrix.
#'
#' @examples
#' # Basic design with only training data
#' train_df <- data.frame(condition = rep(c("A", "B"), each = 50),
#'                        block = rep(1:5, each = 20),
#'                        group = rep(c("Group1", "Group2"), 50))
#' design <- mvpa_design(train_df, y_train = ~ condition)
#'
#' # Design with test data and blocking variable
#' test_df <- data.frame(condition = rep(c("A", "B"), each = 25))
#' design_with_test <- mvpa_design(
#'   train_df, 
#'   test_df, 
#'   y_train = ~ condition, 
#'   y_test = ~ condition,
#'   block_var = ~ block
#' )
#'
#' # Design with split_by factor
#' design_split <- mvpa_design(
#'   train_df, 
#'   y_train = ~ condition,
#'   split_by = ~ group
#' )
#'
#' @seealso 
#' \code{\link{mvpa_dataset}} for creating the corresponding dataset object
#'
#' @importFrom stats as.formula
#' @export
mvpa_design <- function(train_design, test_design=NULL, y_train, y_test=NULL, block_var=NULL, split_by=NULL, ...) {
 
  y_train <- if (!purrr::is_formula(y_train) && length(y_train) > 1) {
    y_train
  } else {
    parse_variable(y_train, train_design)
  }
  
  if (is.factor(y_train) || is.character(y_train)) {
    y_train <- as.factor(y_train)
   
    if (any(table(y_train) == 0)) {
      futile.logger::flog.warn("y_train: ", table(y_train), capture=TRUE)
      futile.logger::flog.warn("y_train factor has at least one level with zero training instances: dropping unused levels.")
      y_train <- droplevels(y_train)
    }
    
    if (length(table(y_train)) <= 1) {
      #futile.logger::flog.error("y_train: ", table(y_train), capture=TRUE)
      stop(paste("error: y_train factor must have at least 2 levels with one or more training instances"))
    }
    
    ytab <- table(levels(y_train))
    
    if (any(ytab == 0)) {
      futile.logger::flog.info("y_train: ", table(y_train), capture=TRUE)
      stop(paste("error: y_train factor must have at least 1 training instance for every factor level"))
    }
  }
  
  if (!is.null(y_test)) {
    
    ## must have a test_design
    assert_that(!is.null(test_design))
    y_test <- if (!purrr::is_formula(y_test) && length(y_test) > 1) {
      y_test
    } else {
      parse_variable(y_test, test_design)
    }
  }
  
  check_split <- function(split_var) {
    minSplits <- min(table(split_var))
    if (minSplits < 3) {
      stop(paste("error: splitting condition results in fewer than 3 observations in at least one set"))
    }
  }
  
  if (!is.null(split_by)) {
    des <- if (!is.null(test_design) && nrow(test_design) > 0) test_design else train_design
    split_var <- parse_variable(split_by, des)
    split_groups <- split(1:nrow(des), split_var)
  } else {
    split_groups=NULL
  }
  
  if (!is.null(block_var)) {
    block_var <- parse_variable(block_var, train_design)
    assertthat::assert_that(!is.null(block_var))
  }
 
  test_design <- if (!is.null(test_design)) {
    tibble::as_tibble(test_design, .name_repair = .name_repair) %>% mutate(.rownum=1:n()) 
  }
  
  train_design <- tibble::as_tibble(train_design,.name_repair = .name_repair) %>% mutate(.rownum=1:n())
  
  des <- list(
    train_design=tibble::as_tibble(train_design, .name_repair = .name_repair),
    y_train=y_train,
    test_design=if (!is.null(test_design)) tibble::as_tibble(test_design, .name_repair = .name_repair) else NULL,
    y_test=y_test,
    split_by=split_by,
    split_groups=split_groups,
    block_var=block_var
  )
  
  class(des) <- c("mvpa_design", "list")
  des
}

#' @export
#' @method print rsa_design
print.rsa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  formula_style <- crayon::italic$green
  var_style <- crayon::blue
  
  # Print header
  cat("\n", header_style("█▀▀ RSA Design ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Formula"), "\n")
  cat(info_style("│  └─ "), formula_style(deparse(x$formula)), "\n")
  
  # Variables section
  cat(section_style("├─ Variables"), "\n")
  cat(info_style("│  └─ "), var_style(paste(names(x$data), collapse=", ")), "\n")
  
  # Block information
  cat(section_style("└─ Structure"), "\n")
  if (!is.null(x$block_var)) {
    blocks <- table(x$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), crayon::green(length(blocks)), "\n")
    cat(info_style("   └─ Block Sizes: "), 
        crayon::green(paste0(names(blocks), ": ", blocks, collapse=", ")), "\n")
  } else {
    cat(info_style("   └─ Blocking: "), crayon::red("None"), "\n")
  }
  cat("\n")
}

#' @export
#' @method print mvpa_design
print.mvpa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  level_style <- crayon::blue
  stat_style <- crayon::italic$white
  
  # Print header
  cat("\n", header_style("█▀▀ MVPA Design ▀▀█"), "\n\n")
  
  # Training section
  cat(section_style("├─ Training Data"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$y_train), big.mark=",")), "\n")
  
  if (is.factor(x$y_train)) {
    cat(info_style("│  ├─ Response Type: "), "Factor\n")
    cat(info_style("│  ├─ Levels: "), level_style(paste(levels(x$y_train), collapse=", ")), "\n")
    level_counts <- table(x$y_train)
    cat(info_style("│  └─ Class Distribution: "), 
        number_style(paste0(names(level_counts), ": ", level_counts, collapse=", ")), "\n")
  } else {
    cat(info_style("│  ├─ Response Type: "), "Numeric\n")
    cat(info_style("│  └─ Range: "), 
        number_style(sprintf("[%.2f, %.2f]", min(x$y_train), max(x$y_train))), "\n")
  }
  
  # Test data section
  cat(section_style("├─ Test Data"), "\n")
  if (!is.null(x$y_test)) {
    cat(info_style("│  ├─ Observations: "), number_style(format(length(x$y_test), big.mark=",")), "\n")
    if (is.factor(x$y_test)) {
      test_counts <- table(x$y_test)
      cat(info_style("│  └─ Class Distribution: "), 
          number_style(paste0(names(test_counts), ": ", test_counts, collapse=", ")), "\n")
    } else {
      cat(info_style("│  └─ Range: "), 
          number_style(sprintf("[%.2f, %.2f]", min(x$y_test), max(x$y_test))), "\n")
    }
  } else {
    cat(info_style("│  └─ "), crayon::red("None"), "\n")
  }
  
  # Structure section
  cat(section_style("└─ Structure"), "\n")
  
  # Block variable info
  if (!is.null(x$block_var)) {
    blocks <- table(x$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), number_style(length(blocks)), "\n")
    cat(info_style("   ├─ Mean Block Size: "), 
        number_style(format(mean(blocks), digits=2)),
        stat_style(" (SD: "),
        number_style(format(sd(blocks), digits=2)),
        stat_style(")"), "\n")
  } else {
    cat(info_style("   ├─ Blocking: "), crayon::red("None"), "\n")
  }
  
  # Split information
  if (!is.null(x$split_by)) {
    split_info <- table(x$split_by)
    cat(info_style("   └─ Split Groups: "), 
        number_style(paste0(names(split_info), ": ", split_info, collapse=", ")), "\n")
  } else {
    cat(info_style("   └─ Split Groups: "), crayon::red("None"), "\n")
  }
  cat("\n")
}





==================================================
File: ./mvpa_result.R
==================================================
#' Create a \code{classification_result} instance
#'
#' Constructs a classification result object based on the observed and predicted values,
#' as well as other optional parameters.
#'
#' @param observed A vector of observed or true values.
#' @param predicted A vector of predicted values.
#' @param probs A \code{matrix} of predicted probabilities, with one column per level.
#' @param testind The row indices of the test observations (optional).
#' @param test_design An optional design for the test data.
#' @param predictor An optional predictor object.
#'
#' @return A classification result object, which can be one of: \code{regression_result},
#'   \code{binary_classification_result}, or \code{multiway_classification_result}.
#'
#' @examples
#' # A vector of observed values
#' yobs <- factor(rep(letters[1:4], 5))
#'
#' # Predicted probabilities
#' probs <- data.frame(a = runif(1:20), b = runif(1:20), c = runif(1:20), d = runif(1:20))
#' probs <- sweep(probs, 1, rowSums(probs), "/")
#'
#' # Get the max probability per row and use this to determine the predicted class
#' maxcol <- max.col(probs)
#' predicted <- levels(yobs)[maxcol]
#'
#' # Construct a classification result
#' cres <- classification_result(yobs, predicted, probs)
#'
#' # Compute default performance measures (Accuracy, AUC)
#' performance(cres)
#' @export
#' @family classification_result
classification_result <- function(observed, predicted, probs, testind=NULL, test_design=NULL,predictor=NULL) {
  
  
  if (is.numeric(observed)) {
    regression_result(observed, predicted, testind, test_design, predictor)
  } else if (length(levels(as.factor(observed))) == 2) {
    binary_classification_result(as.factor(observed), predicted, probs,  testind, test_design, predictor)
  } else if (length(levels(as.factor(observed))) > 2) {
    multiway_classification_result(as.factor(observed),predicted, probs, testind, test_design, predictor)
  } else {
    stop("observed data must be a factor with 2 or more levels")
  }
}

#' Classification results for binary outcome
#'
#' Constructs a binary classification result object based on the observed and predicted values,
#' as well as other optional parameters.
#'
#' @param observed A vector of observed or true values.
#' @param predicted A vector of predicted values.
#' @param probs A \code{matrix} of predicted probabilities, with one column per level.
#' @param testind The row indices of the test observations (optional).
#' @param test_design An optional design for the test data.
#' @param predictor An optional predictor object.
#'
#' @return A binary classification result object, with the class attribute set to "binary_classification_result".
#' @family classification_result
#' @export
binary_classification_result <- function(observed, predicted, probs, testind=NULL, test_design=NULL, predictor=NULL) {
  assertthat::assert_that(length(observed) == length(predicted))
  ret <- list(
    observed=observed,
    predicted=predicted,
    probs=as.matrix(probs),
    testind=testind,
    test_design=test_design,
    predictor=predictor
  )
  
  class(ret) <- c("binary_classification_result", "classification_result", "list")
  ret
}



#' Subset Multiway Classification Result
#'
#' This function subsets a multiway classification result based on the provided indices.
#'
#' @param x An object of class \code{multiway_classification_result} containing the multiway classification results.
#' @param indices The set of indices used to subset the results.
#'
#' @return A \code{multiway_classification_result} object containing the subset of results specified by the indices.
#'
#' @export
#' @family sub_result
sub_result.multiway_classification_result <- function(x, indices) {
  ret <- list(
    observed=x$observed[indices],
    predicted=x$predicted[indices],
    probs=as.matrix(x$probs)[indices,],
    testind=x$testind[indices],
    test_design=x$test_design[indices,],
    predictor=x$predictor)
  
  class(ret) <- c("multiway_classification_result", "classification_result", "list")
  ret
}

#' Subset Binary Classification Result
#'
#' This function subsets a binary classification result based on the provided indices.
#'
#' @param x An object of class \code{binary_classification_result} containing the binary classification results.
#' @param indices The set of indices used to subset the results.
#'
#' @return A \code{binary_classification_result} object containing the subset of results specified by the indices.
#'
#' @export
#' @family sub_result
sub_result.binary_classification_result <- function(x, indices) {
  ret <- list(
    observed=x$observed[indices],
    predicted=x$predicted[indices],
    probs=as.matrix(x$probs)[indices,],
    testind=x$testind[indices],
    test_design=x$test_design[indices,],
    predictor=x$predictor)
  
  class(ret) <- c("binary_classification_result", "classification_result", "list")
  ret
}


 
#' Create a Multiway Classification Result Object
#'
#' This function creates a multiway classification result object containing the observed and predicted values, class probabilities, test design, test indices, and predictor.
#'
#' @param observed A vector of observed values.
#' @param predicted A vector of predicted values.
#' @param probs A matrix of class probabilities.
#' @param testind A vector of indices for the test data (optional).
#' @param test_design The test design (optional).
#' @param predictor The predictor used in the multiway classification model (optional).
#' @return A list with class attributes "multiway_classification_result", "classification_result", and "list" containing the observed and predicted values, class probabilities, test design, test indices, and predictor.
#' @family classification_result
multiway_classification_result <- function(observed, predicted, probs,testind=NULL, test_design=NULL, predictor=NULL) {
  assertthat::assert_that(length(observed) == length(predicted))
  ret <- list(
    observed=observed,
    predicted=predicted,
    probs=as.matrix(probs),
    testind=testind,
    test_design=test_design,
    predictor=predictor)
  
  class(ret) <- c("multiway_classification_result", "classification_result", "list")
  ret
}

 
#' Create a Regression Result Object
#'
#' This function creates a regression result object containing the observed and predicted values, test design, test indices, and predictor.
#'
#' @param observed A vector of observed values.
#' @param predicted A vector of predicted values.
#' @param testind A vector of indices for the test data (optional).
#' @param test_design The test design (optional).
#' @param predictor The predictor used in the regression model (optional).
#' @return A list with class attributes "regression_result", "classification_result", and "list" containing the observed and predicted values, test design, test indices, and predictor.
#' @family classification_result
regression_result <- function(observed, predicted, testind=NULL, test_design=NULL, predictor=NULL) {
  ret <- list(
    observed=observed,
    predicted=predicted,
    test_design=test_design,
    testind=testind,
    predictor=predictor)
  class(ret) <- c("regression_result", "classification_result", "list")
  ret
}

#' @export
#' @method print classification_result
print.classification_result <- function(x, ...) {
  UseMethod("print")
}

#' @export
#' @method print regression_result
print.regression_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Regression Result ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(length(x$observed)), "\n")
  cat(info_style("│  ├─ Test Indices: "), 
      if(!is.null(x$testind)) number_style(length(x$testind)) else crayon::red("None"), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  mse <- mean((x$observed - x$predicted)^2)
  rmse <- sqrt(mse)
  r2 <- cor(x$observed, x$predicted)^2
  mae <- mean(abs(x$observed - x$predicted))
  
  cat(info_style("   ├─ MSE: "), number_style(sprintf("%.4f", mse)), "\n")
  cat(info_style("   ├─ RMSE: "), number_style(sprintf("%.4f", rmse)), "\n")
  cat(info_style("   ├─ MAE: "), number_style(sprintf("%.4f", mae)), "\n")
  cat(info_style("   └─ R²: "), number_style(sprintf("%.4f", r2)), "\n\n")
}

#' @export
#' @method print binary_classification_result
print.binary_classification_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  level_style <- crayon::blue
  
  # Print header
  cat("\n", header_style("█▀▀ Binary Classification Result ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(length(x$observed)), "\n")
  cat(info_style("│  ├─ Classes: "), level_style(paste(levels(x$observed), collapse=", ")), "\n")
  cat(info_style("│  └─ Test Indices: "), 
      if(!is.null(x$testind)) number_style(length(x$testind)) else crayon::red("None"), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  conf_mat <- table(Observed=x$observed, Predicted=x$predicted)
  accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
  sensitivity <- conf_mat[2,2] / sum(conf_mat[2,])
  specificity <- conf_mat[1,1] / sum(conf_mat[1,])
  
  cat(info_style("   ├─ Accuracy: "), number_style(sprintf("%.4f", accuracy)), "\n")
  cat(info_style("   ├─ Sensitivity: "), number_style(sprintf("%.4f", sensitivity)), "\n")
  cat(info_style("   └─ Specificity: "), number_style(sprintf("%.4f", specificity)), "\n\n")
}

#' @export
#' @method print multiway_classification_result
print.multiway_classification_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  level_style <- crayon::blue
  
  # Print header
  cat("\n", header_style("█▀▀ Multiway Classification Result ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(length(x$observed)), "\n")
  cat(info_style("│  ├─ Number of Classes: "), number_style(length(levels(x$observed))), "\n")
  cat(info_style("│  ├─ Classes: "), level_style(paste(levels(x$observed), collapse=", ")), "\n")
  cat(info_style("│  └─ Test Indices: "), 
      if(!is.null(x$testind)) number_style(length(x$testind)) else crayon::red("None"), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  conf_mat <- table(Observed=x$observed, Predicted=x$predicted)
  accuracy <- sum(diag(conf_mat)) / sum(conf_mat)
  
  # Calculate per-class metrics
  class_metrics <- lapply(levels(x$observed), function(cls) {
    tp <- sum(x$observed == cls & x$predicted == cls)
    total <- sum(x$observed == cls)
    recall <- tp / total
    precision <- tp / sum(x$predicted == cls)
    f1 <- 2 * (precision * recall) / (precision + recall)
    c(recall=recall, precision=precision, f1=f1)
  })
  names(class_metrics) <- levels(x$observed)
  
  cat(info_style("   ├─ Overall Accuracy: "), number_style(sprintf("%.4f", accuracy)), "\n")
  cat(info_style("   └─ Per-Class Metrics:"), "\n")
  
  for(cls in levels(x$observed)) {
    metrics <- class_metrics[[cls]]
    cat(info_style("      ├─ "), level_style(cls), ":\n")
    cat(info_style("      │  ├─ Recall: "), number_style(sprintf("%.4f", metrics["recall"])), "\n")
    cat(info_style("      │  ├─ Precision: "), number_style(sprintf("%.4f", metrics["precision"])), "\n")
    cat(info_style("      │  └─ F1: "), number_style(sprintf("%.4f", metrics["f1"])), "\n")
  }
  cat("\n")
}



==================================================
File: ./model_fit.R
==================================================

#' @keywords internal
#' @noRd
requireNamespaceQuietStop <- function(package) {
  if (!requireNamespace(package, quietly = TRUE))
    stop(paste('package',package,'is required'), call. = FALSE)
}

#' @keywords internal
#' @noRd
mclass_summary <- function (data, lev = NULL, model = NULL) {
  if (!all(levels(data[, "pred"]) == levels(data[, "obs"]))) 
    stop("levels of observed and predicted data do not match")
  has_class_probs <- all(lev %in% colnames(data))
  
  if (has_class_probs) {
    requireNamespaceQuietStop("ModelMetrics")
    prob_stats <- lapply(levels(data[, "pred"]), function(x) {
      obs <- ifelse(data[, "obs"] == x, 1, 0)
      prob <- data[, x]
      AUCs <- try(ModelMetrics::auc(obs, data[, x]), silent = TRUE)
      return(AUCs)
    })
    roc <- mean(unlist(prob_stats))
  } else {
    stop("Cannot compute AUC. Class probabilities unavailable for model: ", model)
  }
  
  c(AUC=roc)
}



#' @keywords internal
#' @noRd
load_caret_libs <- function(x) {
  for (lib in x$model$library) {
    library(lib, character.only = TRUE)
  }
}


#' @keywords internal
#' @noRd
get_control <- function(y, nreps) {
  if (is.factor(y) && length(levels(y)) == 2) {
    ctrl <- caret::trainControl("boot", number=nreps, verboseIter=TRUE, classProbs=TRUE, returnData=FALSE, returnResamp="none",allowParallel=FALSE, trim=TRUE, summaryFunction=caret::twoClassSummary)
    metric <- "ROC"
  } else if (is.factor(y) && length(levels(y)) > 2) {
    ctrl <- caret::trainControl("boot", number=nreps, verboseIter=TRUE, classProbs=TRUE, returnData=FALSE, returnResamp="none",allowParallel=FALSE, trim=TRUE, summaryFunction=mclass_summary)
    metric <- "AUC"
  } else {
    ctrl <- caret::trainControl("boot", number=nreps, verboseIter=TRUE, returnData=FALSE, returnResamp="none",allowParallel=FALSE, trim=TRUE)
    metric = "RMSE"
  }
  
  list(ctrl=ctrl, metric=metric)
}


#'
#' This function finds the best hyperparameters for a given model specification
#' using a specified tuning grid and cross-validation.
#'
#' @param mspec A model specification derived from the \code{mvpa_model} class.
#' @param x The training data matrix.
#' @param y The response vector.
#' @param wts Optional class weights (if the underlying model supports it).
#' @param param A \code{data.frame} representing the tuning grid, where
#'        parameter names are indicated by column names.
#' @param nreps The number of bootstrap replications (default is 10).
#' @return A data frame containing the best hyperparameter values.
#' @keywords internal
#' @noRd
tune_model <- function(mspec, x, y, wts, param, nreps=10) {
  ctrl <- get_control(y, nreps)
  cfit <-caret::train(as.data.frame(x), y, method=mspec$model, weights=wts, metric=ctrl$metric, trControl=ctrl$ctrl, tuneGrid=param)
  cfit$bestTune
}

#' Fit an MVPA model
#'
#' This function fits a multivariate pattern analysis (MVPA) model to the given data.
#'
#' @param obj An object derived from the \code{mvpa_model} class.
#' @param x The training data matrix.
#' @param y The response vector.
#' @param wts Optional class weights (if the underlying model supports it).
#' @param param The hyperparameters of the model.
#' @param classProbs Logical; if TRUE, class probabilities should be computed (default is FALSE).
#' @param ... Additional arguments to be passed to the underlying model fitting function.
#' @return A fitted model object with additional attributes "obsLevels" and "problemType".
#' @noRd
fit_model.mvpa_model <- function(obj, x, y, wts, param, classProbs, ...) {
  fit <- obj$model$fit(x,y,wts=wts,param=param,lev=levels(y), classProbs=classProbs, ...)
  
  # Add levels both as attribute and list element for consistency
  fit$obsLevels <- levels(y)
  attr(fit, "obsLevels") <- levels(y)
  
  if (is.factor(y)) {
    attr(fit, "problemType") <- "Classification"
  } else {
    attr(fit, "problemType") <- "Regression"
  }
  
  fit
}

#' Predict class labels and probabilities for new data using a fitted model
#'
#' @param object A fitted model object of class \code{class_model_fit}.
#' @param newdata New data to predict on, either as a \code{matrix} or a \code{NeuroVec} or \code{NeuroSurfaceVector} object.
#' @param sub_indices The subset of row indices to compute predictions on (optional).
#' @param ... Additional arguments to be passed to the underlying prediction function.
#' @return A list containing class predictions and probabilities with class attributes "classification_prediction", "prediction", and "list".
#' @noRd
#' @keywords internal
predict.class_model_fit <- function(object, newdata, sub_indices=NULL,...) {
  tryCatch({
    mat <- if (inherits(newdata, "NeuroVec") || inherits(newdata, "NeuroSurfaceVector")) {
      series(newdata, object$fit$vox_ind)
    } else {
      newdata
    }
    
    if (!is.null(sub_indices)) {
      assert_that(is.vector(sub_indices))
      mat <- mat[sub_indices,,drop=FALSE]
    }
    
    if (!is.null(object$feature_mask)) {
      mat <- mat[, object$feature_mask,drop=FALSE]
    }

    futile.logger::flog.debug("Predicting with data dimensions: %s", paste(dim(mat), collapse=" x "))
    
    probs <- object$model$prob(object$fit, mat)
    if (is.null(probs) || length(probs) == 0) {
      stop("Model probability calculation returned NULL or empty result")
    }
    
    colnames(probs) <- levels(object$y)
    cpred <- max.col(probs)
    cpred <- levels(object$y)[cpred]
    ret <- list(class=cpred, probs=probs)
    class(ret) <- c("classification_prediction", "prediction", "list")
    ret
    
  }, error = function(e) {
    futile.logger::flog.error("Class model prediction failed: %s", e$message)
    futile.logger::flog.debug("Input data dimensions: %s", paste(dim(newdata), collapse=" x "))
    stop(sprintf("Prediction failed: %s", e$message))
  })
}



#' Predict continuous values for a new dataset using a regression model
#'
#' This function predicts continuous values for new data using a fitted regression model.
#'
#' @param object A fitted model object of class \code{regression_model_fit}.
#' @param newdata New data to predict on, either as a matrix or a \code{NeuroVec} or \code{NeuroSurfaceVector} object.
#' @param sub_indices A vector of indices used to subset rows of `newdata` (optional).
#' @param ... Additional arguments to be passed to the underlying prediction function.
#' @return A list containing predicted continuous values with class attributes "regression_prediction", "prediction", and "list".
#' @noRd
#' @keywords internal
predict.regression_model_fit <- function(object, newdata, sub_indices=NULL,...) {
  #browser()
  tryCatch({
    mat <- if (inherits(newdata, "NeuroVec") || inherits(newdata, "NeuroSurfaceVector")) {
      series(newdata, object$fit$vox_ind)
    } else {
      newdata
    }
    
    if (!is.null(sub_indices)) {
      assert_that(is.vector(sub_indices))
      mat <- mat[sub_indices,,drop=FALSE]
    }
    
    if (!is.null(object$feature_mask)) {
      mat <- mat[, object$feature_mask,drop=FALSE]
    }

    futile.logger::flog.debug("Regression prediction with data dimensions: %s", paste(dim(mat), collapse=" x "))
    
    preds <- object$model$predict(object$fit, mat)
    if (is.null(preds) || length(preds) == 0) {
      stop("Model prediction returned NULL or empty result")
    }
    
    ret <- list(preds=preds)
    class(ret) <- c("regression_prediction", "prediction", "list")
    ret
    
  }, error = function(e) {
    futile.logger::flog.error("Regression model prediction failed: %s", e$message)
    futile.logger::flog.debug("Input data dimensions: %s", paste(dim(newdata), collapse=" x "))
    stop(sprintf("Prediction failed: %s", e$message))
  })
}


#' @export
#' @method merge_predictions regression_prediction
merge_predictions.regression_prediction <- function(obj1, rest, weights=rep(1,length(rest)+1)/(length(rest)+1)) {
  allobj <- c(obj1, rest)
  assert_that(all(sapply(allobj, function(obj) inherits(obj, "regression_prediction"))))
  
  #preds <- lapply(1:length(allobj), function(i) {
  #  predict(allobj[[i]], newdata, ...)$pred * weights[i]
  #})
  
  preds <- lapply(1:length(allobj), function(i) {
    allobj[[i]]$pred * weights[i]
  })
  
  final_pred <- rowMeans(do.call(cbind, preds))
  ret <- list(preds=final_pred)
  class(ret) <- c("regression_prediction", "prediction", "list")
  ret
}


#' @export
#' @method merge_predictions classification_prediction
merge_predictions.classification_prediction <- function(obj1, rest, weights=rep(1,length(rest)+1)/(length(rest)+1)) {
  allobj <- vector(mode="list", length(rest)+1)
  allobj[[1]] <- obj1
  allobj[2:length(allobj)] <- rest
  
  #allobj <- c(obj1, rest)
  assert_that(all(sapply(allobj, function(obj) inherits(obj, "classification_prediction"))))
  
  #preds <- lapply(1:length(allobj), function(i) {
  #  predict(allobj[[i]], newdata, ...)$prob * weights[i]
  #})
  
  preds <- lapply(1:length(allobj), function(i) {
    allobj[[i]]$prob * weights[i]
  })
  
  prob <- preds[!sapply(preds, function(x) is.null(x))]
  pfinal <- Reduce("+", prob)
  
  cnames <- colnames(pfinal)
  maxids <- apply(pfinal, 1, which.max)
  len <- sapply(maxids, length)
  
  if (any(len == 0)) {
    maxids[len == 0] <- NA
  }
  
  maxids <- unlist(maxids)
  
  pclass <- cnames[maxids]
  ret <- list(class=pclass, probs=pfinal)
  class(ret) <- c("classification_prediction", "prediction", "list")
  ret
  
}


#' Create a Model Fit Object
#'
#' Constructs a model fit object, representing the result of a single model fit to a chunk of data. The object contains information about the model, response variable, model fit, problem type, model parameters, voxel indices, and an optional feature mask.
#'
#' @param model The caret-style model object.
#' @param y The response variable (predictand).
#' @param fit The fitted model.
#' @param model_type The problem type, either "classification" or "regression" (default). Must be one of the provided options.
#' @param param The model parameters.
#' @param vox_ind The voxel indices indicating the data coordinates.
#' @param feature_mask An optional logical mask indicating the selected subset of columns (features).
#'
#' @return An object of class \code{model_fit}, containing the model, response variable, fitted model, problem type, model parameters, voxel indices, and optional feature mask. The object is also assigned a class based on the problem type: \code{class_model_fit} for classification or \code{regression_model_fit} for regression.
#'
#' @keywords internal
#' @noRd
model_fit <- function(model, y, fit, model_type=c("classification", "regression"), param, vox_ind, feature_mask=NULL) {
  model_type=match.arg(model_type)
  
  ret <- list(
    model=model,
    y=y,
    fit=fit,
    model_type=model_type,
    param=param,
    vox_ind=vox_ind,
    feature_mask=feature_mask)
  
  if (model_type == "classification") {
    class(ret) <- c("class_model_fit", "model_fit")
  } else {
    class(ret) <- c("regression_model_fit", "model_fit")
  }
  ret
}

#' Create a Weighted Consensus Model
#'
#' Constructs a weighted consensus model formed as a weighted average of a set of models. The consensus model combines the input models according to their respective weights.
#'
#' @param fits A list of model fits to be combined.
#' @param names An optional list of names, one per model fit (default: numeric indices).
#' @param weights A vector of weights, one per model fit, that sum up to 1 (default: equal weights for all models).
#'
#' @return An object of class \code{weighted_model}, containing the list of model fits, their names, and the assigned weights. The object is also assigned a class `list`.
#'
#' @examples
#' # Create two sample model fits
#' fit1 <- list(model = "model1", y = c(0, 1), fit = "fit1")
#' fit2 <- list(model = "model2", y = c(1, 0), fit = "fit2")
#'
#' # Combine the model fits into a weighted consensus model
#' w_model <- weighted_model(fits = list(fit1, fit2), names = c("model1", "model2"), weights = c(0.6, 0.4))
#'
#' @keywords internal
#' @noRd
weighted_model <- function(fits, names=1:length(fits), weights=rep(1/length(fits), length(fits))) {
  stopifnot(length(weights) == length(fits))
  ret <- fits
  names(ret) <- names
  attr(ret, "weights") <- weights
  class(ret) <- c("weighted_model", "list")
  ret
}

#' a list of model fits
#' 
#' @param fits a list of fits
#' @param names the names of the fits
#' @export
#' @noRd
#' @keywords internal
list_model <- function(fits, names=1:length(fits)) {
  stopifnot(is.list(fits))
  ret <- fits
  names(ret) <- names
  class(ret) <- c("list_model", "list")
  ret
}

#' @export
#' @method predict weighted_model
predict.weighted_model <- function(object, newdata=NULL, ...) {
  if (is.null(newdata)) {
    stop("newdata cannot be null")
  }

  preds <- lapply(object, function(fit) predict(fit, newdata, ...))
  merge_predictions(preds[[1]], preds[2:length(preds)], attr(object, "weights"))
  
}

#' @export
#' @method predict list_model
predict.list_model <- function(object, newdata=NULL,...) {
  if (is.null(newdata)) {
    stop("newdata cannot be null")
  }
  
  res <- lapply(object, function(fit) {
    predict(fit, newdata,...)
  })
  
}



#' Train an MVPA Model
#'
#' This function trains a Multi-Variate Pattern Analysis (MVPA) model on the provided data, taking care of feature selection, parameter tuning, and model fitting.
#'
#' @param obj An object of class \code{mvpa_model}, specifying the MVPA problem.
#' @param train_dat Training data, an instance of class \code{ROIVolume} or \code{ROISurface}.
#' @param y The dependent variable (response variable), either a numeric vector or a factor.
#' @param indices The spatial indices associated with each column.
#' @param wts Optional class weights (if the underlying model supports it).
#' @param ... Additional arguments passed to other methods.
#' @return A model fit object containing the trained model, its fit, the model type (classification or regression), the best tuning parameters, the voxel indices, and the feature mask.
train_model.mvpa_model <- function(obj, train_dat, y, indices, wts=NULL, ...) {
  
  tryCatch({
    futile.logger::flog.debug("Starting train_model with data dimensions: %s", 
                             paste(dim(train_dat), collapse=" x "))
    futile.logger::flog.debug("Response variable levels: %s", 
                             paste(levels(y), collapse=", "))
    
    param <- tune_grid(obj, train_dat, y, len=1)
    futile.logger::flog.debug("Tuning grid parameters: %s", 
                             paste(names(param), collapse=", "))

    if (is.character(y)) {
      y <- as.factor(y)
    }
    
    ## columns that have zero variance
    nzero <- nonzeroVarianceColumns2(train_dat)
    futile.logger::flog.debug("Non-zero variance columns: %d", sum(nzero))
    
    ## columns with NAs
    nacols <- na_cols(train_dat)
    futile.logger::flog.debug("NA columns: %d", sum(nacols))
    
    ## duplicated columns
    dup <- !duplicated(t(train_dat))
    futile.logger::flog.debug("Non-duplicate columns: %d", sum(dup))
    
    ## invalid columns
    nzero <- nzero & dup & !nacols
    futile.logger::flog.debug("Valid columns after filtering: %d", sum(nzero))
    
    if (length(nzero) == 0 || sum(nzero,na.rm=TRUE) < 2) {
      stop(sprintf("training data must have more than one valid feature (found %d)", 
                  sum(nzero,na.rm=TRUE)))
    }
    
    ## feature selection and variable screening
    feature_mask <- if (!is.null(obj$feature_selector)) {
      nz <- which(nzero)
      fsel <- select_features(obj, train_dat[,nz], y)
      mask <- logical(ncol(train_dat))
      mask[nz[fsel]] <- TRUE
      mask
    } else {
      nzero
    }
    
    futile.logger::flog.debug("Features selected: %d", sum(feature_mask))
    
    if (sum(feature_mask) < 2) {
      stop("train_model: training data must have more than one valid feature after feature selection")
    }
    
    train_dat <- train_dat[,feature_mask,drop=FALSE]
    
    ## parameter_tuning
    best_param <- if (!is.vector(param) && !is.null(nrow(param)) && nrow(param) > 1) {
      bp <- tune_model(obj, train_dat, y, wts, param, obj$tune_reps)
      futile.logger::flog.debug("Best tuning parameters: %s", 
                               paste(capture.output(print(bp)), collapse="\n"))
      bp
    } else {
      param
    }
    
    mtype <- if (is.factor(y)) {
      "classification"
    } else if (is.numeric(y)) {
      "regression"
    } else {
      stop("'y' must be a numeric vector or factor")
    }
    
    futile.logger::flog.debug("Fitting model of type: %s", mtype)
    fit <- fit_model(obj, train_dat, y, wts=wts, param=best_param, classProbs=TRUE)
    model_fit(obj$model, y, fit, mtype, best_param, indices, feature_mask)
    
  }, error = function(e) {
    futile.logger::flog.error("train_model failed: %s", e$message)
    futile.logger::flog.debug("Data dimensions: %s", paste(dim(train_dat), collapse=" x "))
    futile.logger::flog.debug("Response levels: %s", paste(levels(y), collapse=", "))
    stop(e$message)  # Re-throw the error after logging
  })
}




==================================================
File: ./allgeneric.R
==================================================
#' Get Unique Region IDs
#'
#' Extract unique region IDs from a region mask, handling both volume and surface data.
#'
#' @param region_mask A region mask object (NeuroVol or NeuroSurface)
#' @param ... Additional arguments passed to methods
#'
#' @return A sorted vector of unique region IDs greater than 0
#' @keywords internal
get_unique_regions <- function(region_mask, ...) {
  UseMethod("get_unique_regions")
}

#' Select Features
#'
#' Given a \code{feature_selection} specification object and a dataset, returns the set of selected features as a binary vector.
#'
#' @param obj The \code{feature_selection} object specifying the feature selection method and its parameters.
#' @param X The dataset containing the training features. This can be a matrix or a \code{ROIVolume} or \code{ROISurface} object.
#' @param Y The dependent variable as a factor or numeric variable.
#' @param ... Additional arguments to be passed to the method-specific function.
#' 
#' @return A logical vector indicating the columns of \code{X} matrix that were selected.
#' 
#' @examples 
#' fsel <- feature_selector("FTest", "top_k", 2)
#' coords <- rbind(c(1,1,1), c(2,2,2), c(3,3,3))
#' ROI <- neuroim2::ROIVec(neuroim2::NeuroSpace(c(10,10,10)), coords=coords, matrix(rnorm(100*3), 100, 3))
#' Y <- factor(rep(c("a", "b"), each=50))
#' featureMask <- select_features(fsel, neuroim2::values(ROI), Y)
#' sum(featureMask) == 2
#'
#' fsel2 <- feature_selector("FTest", "top_p", .1)
#' featureMask <- select_features(fsel2, neuroim2::values(ROI), Y)
#' 
#' @export
select_features <- function(obj, X, Y, ...) {
  UseMethod("select_features")
}

#' Format Result Object
#'
#' @param obj The result object to be formatted.
#' @param result The result object to be formatted.
#' @param error_message An optional error message.
#' @param ... Additional arguments to be passed to the method-specific function.
#' @export
format_result <- function(obj, result, error_message, ...) {
  UseMethod("format_result")
}


#' Merge Multiple Results
#'
#' @param obj The base object containing merge specifications
#' @param result_set List of results to be merged
#' @param indices List of indices corresponding to each result
#' @param id Identifier for the merged result
#' @param ... Additional arguments passed to specific merge methods
#'
#' @return A merged result object containing:
#' \itemize{
#'   \item Combined results from all input objects
#'   \item Associated indices
#'   \item Merged metadata
#' }
#'
#' @export
merge_results <- function(obj, result_set, indices, id, ...) {
  UseMethod("merge_results")
}

#' Run Future
#'
#' Run a future-based computation defined by the object and frame.
#'
#' @param obj An object specifying the computation.
#' @param frame A data frame or environment containing data for the computation.
#' @param processor A function or object specifying how to process the frame.
#' @param ... Additional arguments passed to the method-specific function.
#' @noRd
run_future <- function(obj, frame, processor, ...) {
  UseMethod("run_future")
}

#' Process ROI
#'
#' Process a region of interest (ROI) with possible cross-validation.
#'
#' @param mod_spec The model specification object.
#' @param roi The region of interest data.
#' @param rnum A numeric or string identifier for the ROI.
#' @param ... Additional arguments passed to the method-specific function.
#' @keywords internal
#' @noRd
process_roi <- function(mod_spec, roi, rnum, ...) {
  UseMethod("process_roi")
}

#' Default Process ROI Method
#'
#' Default implementation for processing ROIs when no custom ROI processor is provided.
#'
#' @inheritParams process_roi
#' @keywords internal
#' @noRd
process_roi.default <- function(mod_spec, roi, rnum, ...) {
  if (!is.null(mod_spec$process_roi)) {
    mod_spec$process_roi(mod_spec, roi, rnum, ...)
  } else if (has_test_set(mod_spec$dataset)) {
    external_crossval(mod_spec, roi, rnum, ...)
  } else if (has_crossval(mod_spec)) {
    internal_crossval(mod_spec, roi, rnum, ...)
  } else {
    process_roi_default(mod_spec, roi, rnum, ...)
  }
}

#' Default ROI Processing Helper
#'
#' @param mod_spec The model specification object.
#' @param roi The ROI containing training data.
#' @param rnum The region number or identifier.
#' @param ... Additional arguments passed to specific methods.
#' @keywords internal
#' @noRd
process_roi_default <- function(mod_spec, roi, rnum, ...) {
  xtrain <- tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair=.name_repair)
  ind <- indices(roi$train_roi)
  ret <- try(train_model(mod_spec, xtrain, ind))
  if (inherits(ret, "try-error")) {
    tibble::tibble(result=list(NULL), indices=list(ind), performance=list(ret), id=rnum, error=TRUE, error_message=attr(ret, "condition")$message)
  } else {
    tibble::tibble(result=list(NULL), indices=list(ind), performance=list(ret), id=rnum, error=FALSE, error_message="~")
  }
}

#' Train Model
#'
#' Train a classification or regression model.
#'
#' @param obj The model specification object.
#' @param ... Additional arguments to be passed to the method-specific function.
#' @return A trained model object.
#' @export
train_model <- function(obj,...) {
  UseMethod("train_model")
}

#' Training Labels/Response Extraction
#'
#' Extract the training labels or response variable from an object.
#'
#' @param obj The object from which to extract the training response variable.
#' @export
y_train <- function(obj) {
  UseMethod("y_train")
}

#' Test Labels/Response Extraction
#'
#' Extract the test labels or response variable from an object.
#'
#' @param obj The object from which to extract the test response variable.
#' @export
y_test <- function(obj) {
  UseMethod("y_test")
}

#' Test Design Extraction
#'
#' Return the design table associated with the test set from an object.
#'
#' @param obj The object from which to extract the test design table.
#' @export
test_design <- function(obj) {
  UseMethod("test_design")
}

#' Fit Model
#'
#' Fit a classification or regression model.
#'
#' @param obj A model fitting object.
#' @param roi_x An ROI containing the training data.
#' @param y The response vector.
#' @param wts A set of case weights.
#' @param param Tuning parameters.
#' @param lev Factor levels (for classification).
#' @param last Logical indicating if this is the last iteration.
#' @param classProbs Logical indicating if class probabilities should be returned.
#' @param ... Additional arguments to be passed to the method-specific function.
#' 
#' @export
fit_model <- function(obj, roi_x, y, wts, param, lev=NULL, last=FALSE, classProbs=FALSE, ...) {
  UseMethod("fit_model")
}

#' Tune Grid Extraction
#'
#' Extract the parameter grid to optimize for a model.
#'
#' @param obj The model object.
#' @param x The training data.
#' @param y The response vector.
#' @param len The number of elements in the tuning grid.
tune_grid <- function(obj, x,y,len) {
  UseMethod("tune_grid")
}

#' Test Set Availability
#'
#' Check if an object has a test set available.
#'
#' @param obj The object to check for a test set.
#' @export
has_test_set <- function(obj) {
  UseMethod("has_test_set")
}

#' Requires cross-validation to be performed
#' @param obj The model object.
has_crossval <- function(obj) {
  UseMethod("has_crossval")
}

#' @export
has_crossval.default <- function(obj) {
  FALSE
}

#' Compute Performance Metrics
#'
#' Compute performance metrics (accuracy, AUC, RMSE, etc.) for classification/regression results.
#'
#' @param x The classification/regression result object to evaluate.
#' @param ... Additional arguments passed to method-specific performance functions.
#'
#' @return A list of performance metrics.
#' @export
performance <- function(x,...) {
  UseMethod("performance")
}

#' Compute Performance for an Object
#'
#' Delegates calculation of performance metrics to the appropriate method.
#'
#' @param obj The input object.
#' @param result The classification/regression result object to evaluate.
#'
#' @return A list of performance metrics.
#' @export
compute_performance <- function(obj, result) {
  UseMethod("compute_performance")
}

#' Merge Multiple Classification/Regression Results
#'
#' This function merges two or more classification/regression result objects.
#'
#' @param x The first classification/regression result object.
#' @param ... Additional classification/regression result objects.
#'
#' @return A single merged classification/regression result object.
#'
#' @export
merge_classif_results <- function(x, ...) {
  UseMethod("merge_classif_results")
}

#' Get Multiple Data Samples
#'
#' Extract multiple data samples based on a list of voxel/index sets from a dataset object.
#'
#' @param obj The input dataset object.
#' @param vox_list A list of vectors containing voxel indices to extract.
#'
#' @return A list of data samples.
#' @export
get_samples <- function(obj, vox_list) {
  UseMethod("get_samples")
}

#' Extract Sample from Dataset
#'
#' Extract a sample from a given dataset object.
#'
#' @param obj The input dataset object.
#' @param vox The voxel indices/coordinates.
#' @param ... Additional arguments to methods.
#'
#' @return A sample extracted from the dataset.
#' @export
data_sample <- function(obj, vox, ...) {
  UseMethod("data_sample")
}

#' Convert object to ROI
#'
#' Convert the provided object into an ROIVolume or ROISurface object.
#'
#' @param obj The object to be converted.
#' @param data The associated data object.
#' @param ... Additional arguments passed to methods.
#' @return An ROIVolume or ROISurface object.
#' @keywords internal
as_roi <- function(obj, data, ...) {
  UseMethod("as_roi")
}

#' Generate Searchlight Iterator
#'
#' Generate a searchlight iterator suitable for given data.
#'
#' @param obj The input dataset object.
#' @param ... Additional arguments to methods.
#'
#' @return A searchlight iterator object.
#' @export
get_searchlight <- function(obj, ...) {
  UseMethod("get_searchlight")
}

#' Wrap Output
#'
#' Wrap output values into a desired format.
#'
#' @param obj The object used to determine the wrapping method.
#' @param vals The values to be wrapped.
#' @param ... Additional arguments passed to methods.
#' @return A wrapped output object.
#' @keywords internal
wrap_output <- function(obj, vals, ...) {
  UseMethod("wrap_output")
}

#' Merge Predictions
#'
#' Combine predictions from multiple models on the same test set.
#'
#' @param obj1 The first object containing predictions.
#' @param rest Other objects containing predictions.
#' @param ... Additional arguments.
#' @return A combined object with merged predictions.
#' @export
merge_predictions <- function(obj1, rest, ...) {
  UseMethod("merge_predictions")
}

#' Extract Row-wise Subset of a Result
#'
#' Extract a subset of rows from a classification/regression result object.
#'
#' @param x The input result object.
#' @param indices Row indices to extract.
#'
#' @return A new result object with the specified rows.
#' @export
sub_result <- function(x, indices) {
  UseMethod("sub_result")
}

#' Get Number of Observations
#'
#' Retrieve the number of observations in an object.
#'
#' @param x The input object.
#' @return The number of observations.
#' @export
nobs <- function(x) {
  UseMethod("nobs")
}

#' Probability of Observed Class
#'
#' Extract the predicted probability for the observed class.
#'
#' @param x The object from which to extract the probability.
#' @return A vector of predicted probabilities.
#' @export
prob_observed <- function(x) {
  UseMethod("prob_observed")
}

#' Number of Response Categories
#'
#' Get the number of response categories or levels.
#'
#' @param x The object from which to extract the number of categories.
#' @return The number of response categories.
#' @export
nresponses <- function(x) {
  UseMethod("nresponses")
}

#' @keywords internal
#' @noRd
.name_repair = ~ vctrs::vec_as_names(..., repair = "unique", quiet = TRUE)

#' Run Searchlight Analysis
#'
#' Execute a searchlight analysis using multivariate pattern analysis.
#'
#' @param model_spec A \code{mvpa_model} instance containing the model specifications
#' @param radius The searchlight radius in millimeters
#' @param method The type of searchlight, either 'randomized' or 'standard'
#' @param niter The number of searchlight iterations (used only for 'randomized' method)
#' @param ... Extra arguments passed to specific searchlight methods
#'
#' @return A named list of \code{NeuroVol} objects containing performance metrics (e.g., AUC) at each voxel location
#'
#' @examples
#' \donttest{
#'   # Generate sample dataset with categorical response
#'   dataset <- gen_sample_dataset(
#'     D = c(8,8,8),           # 8x8x8 volume
#'     nobs = 100,             # 100 observations
#'     response_type = "categorical",
#'     data_mode = "image",
#'     blocks = 3,             # 3 blocks for cross-validation
#'     nlevels = 2             # binary classification
#'   )
#'   
#'   # Create cross-validation specification using blocks
#'   cval <- blocked_cross_validation(dataset$design$block_var)
#'   
#'   # Load the SDA classifier (Shrinkage Discriminant Analysis)
#'   model <- load_model("sda_notune")
#'   
#'   # Create MVPA model
#'   mspec <- mvpa_model(
#'     model = model,
#'     dataset = dataset$dataset,
#'     design = dataset$design,
#'     model_type = "classification",
#'     crossval = cval
#'   )
#'   
#'   # Run searchlight analysis
#'   results <- run_searchlight(
#'     mspec,
#'     radius = 8,            # 8mm radius
#'     method = "standard"    # Use standard searchlight
#'   )
#'   
#'   # Results contain performance metrics
#'   # Access them with results$performance
#' }
#'
#' @export
run_searchlight <- function(model_spec, radius, method = c("standard", "randomized"), niter = NULL, ...) {
  UseMethod("run_searchlight")
}

#' Region of Interest Based MVPA Analysis
#'
#' Run a separate MVPA analysis for multiple disjoint regions of interest.
#'
#' @param model_spec A \code{mvpa_model} instance containing the model specifications
#' @param region_mask A \code{NeuroVol} or \code{NeuroSurface} object where each region is identified by a unique integer
#' @param ... Extra arguments passed to specific regional analysis methods
#'
#' @return A named list containing:
#'   \item{performance}{Performance metrics for each region}
#'   \item{prediction_table}{Predictions for each region}
#'   \item{fits}{Model fits if return_fits=TRUE}
#'
#' @examples
#' \donttest{
#'   # Generate sample dataset (3D volume with categorical response)
#'   dataset <- gen_sample_dataset(
#'     D = c(10,10,10),       # Small 10x10x10 volume
#'     nobs = 100,            # 100 observations
#'     nlevels = 3,           # 3 classes
#'     response_type = "categorical",
#'     data_mode = "image",
#'     blocks = 3             # 3 blocks for cross-validation
#'   )
#'   
#'   # Create region mask with 5 ROIs
#'   region_mask <- NeuroVol(
#'     sample(1:5, size=length(dataset$dataset$mask), replace=TRUE),
#'     space(dataset$dataset$mask)
#'   )
#'   
#'   # Create cross-validation specification
#'   cval <- blocked_cross_validation(dataset$design$block_var)
#'   
#'   # Load SDA classifier (Shrinkage Discriminant Analysis)
#'   model <- load_model("sda_notune")
#'   
#'   # Create MVPA model
#'   mspec <- mvpa_model(
#'     model = model,
#'     dataset = dataset$dataset,
#'     design = dataset$design,
#'     model_type = "classification",
#'     crossval = cval,
#'     return_fits = TRUE    # Return fitted models
#'   )
#'   
#'   # Run regional analysis
#'   results <- run_regional(mspec, region_mask)
#'   
#'   # Access results
#'   head(results$performance)           # Performance metrics
#'   head(results$prediction_table)      # Predictions
#'   first_roi_fit <- results$fits[[1]]  # First ROI's fitted model
#' }
#'
#' @export
run_regional <- function(model_spec, region_mask, ...) {
  UseMethod("run_regional")
}

#' crossval_samples
#'
#' Apply a cross-validation scheme to split the data into training and testing sets.
#'
#' @param obj A cross-validation control object.
#' @param data A data frame containing the predictors.
#' @param y A vector containing the response variable.
#' @param ... Extra arguments passed to the specific cross-validation methods.
#'
#' @return A tibble containing training and testing sets for each fold.
#' @export
crossval_samples <- function(obj, data, y,...) { 
  UseMethod("crossval_samples") 
}

#' Generic Pairwise Distance Computation
#'
#' Compute pairwise distances between rows of X using a specified distance object.
#'
#' @param obj A distance object specifying the distance measure.
#' @param X A numeric matrix of data points (rows = samples).
#' @param ... Additional arguments passed to methods.
#'
#' @return A matrix or dist object of pairwise distances.
#' @keywords internal
#' @noRd
pairwise_dist <- function(obj, X,...) {
  UseMethod("pairwise_dist")
}

#' Filter Region of Interest (ROI)
#'
#' Filter an ROI by removing columns with missing values or zero std dev.
#'
#' @param roi A list containing the train and test ROI data.
#' @param ... Additional arguments passed to methods.
#'
#' @return A list with filtered train and test ROI data.
#' @keywords internal
filter_roi <- function(roi, ...) {
  UseMethod("filter_roi", roi$train_roi)
}






==================================================
File: ./globals.R
==================================================


==================================================
File: ./classifiers.R
==================================================
#' @keywords internal
#' @noRd
colHuber <- function(x, k=1.5, tol=1e-04) {
  mu <- matrixStats::colMedians(x)
  s <- matrixStats::colMads(x)
  n <- nrow(x)
  sapply(seq_along(mu), function(i) {
    repeat {
      yy <- pmin(pmax(mu[i] - k * s[i], x[, i]), mu[i] + k * s[i])
      mu1 <- sum(yy)/n
      if (abs(mu[i] - mu1) < tol * s[i]) 
        break
      mu[i] <- mu1
    }
    mu[i]
  })
}


#' Pre-defined MVPA Classification Models
#'
#' An environment containing custom classification models for MVPA analysis.
#'
#' @format An environment with the following models:
#' \describe{
#'   \item{corclass}{Correlation-based classifier using template matching with options (pearson, spearman, kendall)}
#'   \item{corsim}{Alias for corclass}
#'   \item{sda_notune}{Shrinkage Discriminant Analysis (SDA) without parameter tuning}
#'   \item{sda_boot}{SDA with bootstrap resampling and feature selection}
#'   \item{glmnet_opt}{Elastic net classifier (glmnet) with optimized alpha/lambda via EPSGO}
#'   \item{sparse_sda}{SDA with sparsity constraints and feature selection}
#'   \item{sda_ranking}{SDA with feature ranking and selection via higher criticism}
#'   \item{mgsda}{Multi-Group Sparse Discriminant Analysis}
#'   \item{lda_thomaz}{Modified LDA classifier for high-dimensional data}
#'   \item{hdrda}{High-Dimensional Regularized Discriminant Analysis}
#' }
#'
#' @details
#' Models are accessed via \code{load_model(name)}. Each implements caret's \code{fit}, \code{predict}, and \code{prob} methods.
#'
#' @examples
#' # Load simple SDA classifier
#' model <- load_model("sda_notune")
#'
#' # Load correlation classifier
#' model <- load_model("corclass")
#'
#' @export
MVPAModels <- new.env()

#' @importFrom MASS huber
#' @importFrom stats median
#' @noRd
corsimFit <- function(x, y, method, robust) {
  estimator <- if (robust) {
    function(vec) {
      h <- try(MASS::huber(vec), silent=TRUE)
      if (inherits(h, "try-error")) median(vec) else h$mu
    }
  } else {
    "mean"
  }
  
  lev <- levels(y)
  if (identical("mean", estimator)) {
    list(conditionMeans=group_means(x, 1, y), levs=lev, method=method, robust=robust)
  } else {
    list(conditionMeans = neuroim2::split_reduce(as.matrix(x), y, estimator), levs=lev, method=method, robust=robust)
  }
}

#' @keywords internal
#' @noRd
predict_corsimFit <- function(modelFit, newData) {
  cres <- cor(t(newData), t(modelFit$conditionMeans), method=modelFit$method)
  res <- max.col(cres)
  modelFit$levs[res]
}

#' @keywords internal
#' @noRd
prob_corsimFit <- function(modelFit, newData) {
  scores <- cor(t(newData), t(modelFit$conditionMeans), method=modelFit$method)
  mc <- scores[cbind(seq_len(nrow(scores)), max.col(scores, ties.method = "first"))]
  probs <- exp(sweep(scores, 1, mc, "-"))
  probs <- zapsmall(probs / rowSums(probs))
  colnames(probs) <- modelFit$levs
  probs
}

# PCA + LDA model
# Store lev after training so predictions can reference classes.
#' @keywords internal
#' @noRd
MVPAModels$pca_lda <- list(
  type = "Classification", 
  library = "MASS", 
  loop = NULL, 
  label="pca_lda",
  parameters=data.frame(parameters="ncomp", class="numeric", labels="ncomp"),
  grid=function(x, y, len = 5) data.frame(ncomp=1:len),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    scmat <- scale(as.matrix(x))
    pres <- svd::propack.svd(scmat, neig=param$ncomp)
    lda.fit <- lda(pres$u[, 1:param$ncomp, drop=FALSE], y)
    attr(lda.fit, "ncomp") <- param$ncomp
    attr(lda.fit, "pcfit") <- pres
    attr(lda.fit, "center") <- attr(scmat, "scaled:center")
    attr(lda.fit, "scale") <- attr(scmat, "scaled:scale")
    attr(lda.fit, "obsLevels") <- lev
    lda.fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    compind <- seq_len(attr(modelFit, "ncomp"))
    pcfit <- attr(modelFit, "pcfit")
    pcx <- scale(newdata, attr(modelFit, "center"), attr(modelFit, "scale")) %*% pcfit$v
    predict(modelFit, pcx[, compind, drop=FALSE])$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    compind <- seq_len(attr(modelFit, "ncomp"))
    pcfit <- attr(modelFit, "pcfit")
    pcx <- scale(newdata, attr(modelFit, "center"), attr(modelFit, "scale")) %*% pcfit$v
    predict(modelFit, pcx[, compind, drop=FALSE])$posterior
  }
)

# corclass and corsim
# Ensure lev is known from fit if needed. corsimFit stores lev internally.
#' @keywords internal
#' @noRd
MVPAModels$corclass <- list(
  type = "Classification", 
  library = "rMVPA", 
  label="corclass",
  loop = NULL, 
  parameters=data.frame(parameters=c("method", "robust"), class=c("character", "logical"),
                        label=c("correlation type: pearson, spearman, or kendall", "mean or huber")),
  grid=function(x, y, len = NULL) {
    if (is.null(len) || len == 1) {
      data.frame(method="pearson", robust=FALSE)
    } else {
      expand.grid(method=c("pearson","spearman","kendall"), robust=c(TRUE,FALSE))
    }
  },
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    corsimFit(x, y, as.character(param$method), param$robust)
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict_corsimFit(modelFit, as.matrix(newdata))
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    prob_corsimFit(modelFit, as.matrix(newdata))
  }
)

#' @keywords internal
#' @noRd
MVPAModels$corsim <- MVPAModels$corclass

# sda_notune
# Add levels storage
#' @keywords internal
#' @noRd
MVPAModels$sda_notune <- list(
  type = "Classification", 
  library = "sda", 
  label="sda_notune",
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    m <- sda::sda(Xtrain=as.matrix(x), L=y, verbose=FALSE, ...)
    m$obsLevels <- lev
    m
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata), verbose=FALSE)$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata), verbose=FALSE)$posterior
  }
)

# sda_boot
# Store lev and ensure predictions label correctly
#' @keywords internal
#' @noRd
MVPAModels$sda_boot <- list(
  type = "Classification", 
  library = "sda", 
  label="sda_boot",
  loop = NULL, 
  parameters=data.frame(parameters=c("reps","frac","lambda_min","lambda_max"),
                        class=c("numeric","numeric","numeric","numeric"),
                        label=c("boot reps","frac features","min lambda","max lambda")),
  grid=function(x, y, len = NULL) data.frame(reps=10, frac=1, lambda_min=.01, lambda_max=.8),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    mfits <- list()
    count <- 1
    failures <- 0
    
    split_y <- split(seq_along(y), y)
    lambda <- seq(param$lambda_min, param$lambda_max, length.out=param$reps)
    
    if (!all(sapply(split_y, function(yy) length(yy) > 0))) {
      stop("Every level in 'y' must have at least one instance for sda_boot.")
    }
    
    assertthat::assert_that(param$frac > 0, msg="sda_boot: 'frac' must be > 0")
    assertthat::assert_that(param$reps > 0, msg="sda_boot: 'reps' must be > 0")
    
    while (count <= param$reps) {
      ysam <- lapply(split_y, function(idx) if (length(idx)==1) idx else sample(idx, length(idx), replace=TRUE))
      row.idx <- sort(unlist(ysam))
      
      ret <- if (param$frac > 0 && param$frac < 1) {
        nkeep <- max(param$frac * ncol(x),1)
        ind <- sample(seq_len(ncol(x)), nkeep)
        fit <- try(sda::sda(Xtrain=x[row.idx,ind,drop=FALSE], L=y[row.idx], lambda=lambda[count], verbose=FALSE, ...),
                   silent=TRUE)
        attr(fit, "keep.ind") <- ind
        fit
      } else {
        fit <- try(sda::sda(Xtrain=x[row.idx,], L=y[row.idx], lambda=lambda[count], verbose=FALSE, ...),
                   silent=TRUE)
        attr(fit, "keep.ind") <- seq_len(ncol(x))
        fit
      }
      
      if (!inherits(ret, "try-error")) {
        mfits[[count]] <- ret
        count <- count + 1
      } else {
        message("sda model fit error, retry: ", failures + 1)
        failures <- failures + 1
        if (failures > 10) return(ret)
      }
    }
    
    out <- list(fits=mfits, lev=lev)
    class(out) <- "sda_boot"
    out
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      predict(fit, as.matrix(newdata)[,ind,drop=FALSE], verbose=FALSE)$posterior
    })
    
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    modelFit$lev[apply(pfinal, 1, which.max)]
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      predict(fit, as.matrix(newdata)[,ind,drop=FALSE], verbose=FALSE)$posterior
    })
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    colnames(pfinal) <- modelFit$lev
    pfinal
  }
)


# lda_thomaz_boot
# Store lev similarly, ensure correct predictions and probs
#' @keywords internal
#' @noRd
MVPAModels$lda_thomaz_boot <- list(
  type = "Classification", 
  library = "sparsediscrim", 
  label="lda_thomaz_boot",
  loop = NULL, 
  parameters=data.frame(parameters=c("reps","frac"), class=c("numeric","numeric"),
                        label=c("bootstrap reps","fraction of features")),
  grid=function(x, y, len = NULL) data.frame(reps=10, frac=1),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    mfits <- list()
    count <- 1
    failures <- 0
    
    split_y <- split(seq_along(y), y)
    if (!all(sapply(split_y, function(yy) length(yy) > 0))) {
      stop("All levels in 'y' must have ≥1 instance for lda_thomaz_boot.")
    }
    assertthat::assert_that(param$frac > 0, msg="lda_thomaz_boot: 'frac' must be > 0")
    
    while (count <= param$reps) {
      ysam <- lapply(split_y, function(idx) if (length(idx)==1) idx else sample(idx, length(idx), replace=TRUE))
      row.idx <- sort(unlist(ysam))
      
      ret <- if (param$frac > 0 && param$frac < 1) {
        nkeep <- max(param$frac * ncol(x),1)
        ind <- sample(seq_len(ncol(x)), nkeep)
        fit <- try(sparsediscrim::lda_thomaz(x=x[row.idx,ind,drop=FALSE], y[row.idx]), silent=TRUE)
        attr(fit, "keep.ind") <- ind
        fit
      } else {
        fit <- try(sparsediscrim::lda_thomaz(x[row.idx,,drop=FALSE], y[row.idx]), silent=TRUE)
        attr(fit, "keep.ind") <- seq_len(ncol(x))
        fit
      }
      
      if (!inherits(ret, "try-error")) {
        mfits[[count]] <- ret
        count <- count + 1
      } else {
        message("lda_thomaz model fit error, retry: ", failures + 1)
        failures <- failures + 1
        if (failures > 10) return(ret)
      }
    }
    
    out <- list(fits=mfits, lev=lev)
    class(out) <- "lda_thomaz_boot"
    out
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      scores <- -t(sparsediscrim:::predict.lda_thomaz(fit, newdata[,ind])$scores)
      mc <- scores[cbind(seq_len(nrow(scores)), max.col(scores, ties.method = "first"))]
      probs <- exp(scores - mc)
      zapsmall(probs/rowSums(probs))
    })
    
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    modelFit$lev[apply(pfinal, 1, which.max)]
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- lapply(modelFit$fits, function(fit) {
      ind <- attr(fit, "keep.ind")
      scores <- -t(sparsediscrim:::predict.lda_thomaz(fit, newdata[,ind])$scores)
      mc <- scores[cbind(seq_len(nrow(scores)), max.col(scores))]
      probs <- exp(scores - mc)
      zapsmall(probs/rowSums(probs))
    })
    
    prob <- preds[!sapply(preds, is.null)]
    pfinal <- Reduce("+", prob)/length(prob)
    # pfinal has no colnames here since lda_thomaz doesn't store them directly.
    # Normally, need levels. If needed: colnames(pfinal) <- modelFit$lev
    pfinal
  }
)

# memoized ranking for sda methods
#' @importFrom memoise memoise
#' @importFrom sda sda.ranking
#' @noRd
memo_rank <- memoise(function(X, L, fdr) {
  sda::sda.ranking(X,L,fdr=fdr,verbose=FALSE)
})

# glmnet_opt
# Store lev, fix prob calculation
#' @keywords internal
#' @noRd
MVPAModels$glmnet_opt <- list(
  type = "Classification", 
  library = c("c060","glmnet"), 
  label="glmnet_opt",
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)
    bounds <- t(data.frame(alpha=c(0,1)))
    colnames(bounds)<-c("lower","upper")
    fam <- if (length(lev) > 2) "multinomial" else "binomial"
    
    foldid <- caret::createFolds(y,k=5,list=FALSE)
    fit <- epsgo(Q.func="tune.glmnet.interval",
                 bounds=bounds,
                 parms.coding="none",
                 seed=1234,
                 show="none",
                 fminlower=-100,
                 x=x, y=y, family=fam,
                 type.min="lambda.1se",
                 foldid=foldid,
                 type.measure="mse")
    sfit <- summary(fit, verbose=FALSE)
    modelFit <- glmnet(x,y,family=fam,alpha=sfit$opt.alpha, nlambda=20)
    modelFit$opt_lambda <- sfit$opt.lambda
    modelFit$opt_alpha <- sfit$opt.alpha
    modelFit$obsLevels <- lev
    modelFit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    out <- predict(modelFit, as.matrix(newdata), s=modelFit$opt_lambda, type="class")
    if (is.matrix(out)) out[,1] else out
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    lev <- modelFit$obsLevels
    fam <- if (length(lev) > 2) "multinomial" else "binomial"
    probs <- predict(modelFit, as.matrix(newdata), s=modelFit$opt_lambda, type="response")
    if (fam=="binomial") {
      # Binary
      probs <- as.vector(probs)
      probs <- data.frame(probs_1 = 1 - probs, probs_2 = probs)
      colnames(probs) <- lev
    } else {
      # Multinomial returns a 3D array: (N, classes, 1)
      # Convert to data.frame
      probs <- as.data.frame(probs[,,1,drop=FALSE])
      colnames(probs) <- lev
    }
    probs
  }
)

# sparse_sda
# Store lev if needed, no explicit predict labeling needed except from posterior
#' @keywords internal
#' @noRd
MVPAModels$sparse_sda <- list(
  type = "Classification", 
  library = "sda", 
  label="sparse_sda",
  loop = NULL, 
  parameters=data.frame(parameters=c("frac","lambda"), class=c("numeric","numeric"), 
                        label=c("frac features","lambda")),
  grid=function(x, y, len = NULL) expand.grid(frac=seq(.1,1,length.out=len), lambda=seq(.01,.99,length.out=len)),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)                          
    nkeep <- max(param$frac * ncol(x),1)
    rank <- memo_rank(x, L=y, fdr=FALSE)
    ind <- rank[,"idx"][1:nkeep]
    
    fit <- sda::sda(Xtrain=x[,ind,drop=FALSE], L=y, lambda=param$lambda, verbose=FALSE)
    attr(fit, "keep.ind") <- ind
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]), verbose=FALSE)$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]),verbose=FALSE)$posterior
  }
)


# sda_ranking
# Store lev
#' @keywords internal
#' @noRd
MVPAModels$sda_ranking <- list(
  type = "Classification", 
  library = "sda", 
  label="sda_ranking",
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    x <- as.matrix(x)                             
    if (ncol(x) > 2) {
      rank <- sda::sda.ranking(Xtrain=x, L=y, fdr=TRUE, verbose=FALSE, ...)
      hcind <- which.max(rank[,"HC"])
      keep.ind <- if (hcind < 2) seq(1, min(ncol(x), 2)) else 1:hcind
      ind <- rank[keep.ind,"idx"]
    } else {
      ind <- seq_len(ncol(x))
    }
    
    fit <- sda::sda(Xtrain=x[,ind,drop=FALSE], L=y, verbose=FALSE)
    attr(fit, "keep.ind") <- ind
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]), verbose=FALSE)$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    predict(modelFit, as.matrix(newdata[,attr(modelFit, "keep.ind"), drop=FALSE]),verbose=FALSE)$posterior
  }
)


# mgsda
# Fix MSGDA::classifiyV to MGSDA::classifyV and store lev
# Define modelFit as a list after training
#' @keywords internal
#' @noRd
MVPAModels$mgsda <- list(
  type = "Classification", 
  library = "MGSDA", 
  loop = NULL, 
  parameters=data.frame(parameters="lambda", class="numeric", label="sparsity penalty"),
  
  grid=function(x, y, len = NULL) data.frame(lambda=seq(.001, .99, length.out=len)),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) { 
    ycodes <- as.integer(y)
    V <- MGSDA::dLDA(as.matrix(x), ycodes, lambda=param$lambda, ...)
    modelFit <- list(
      V = V,
      ycodes = ycodes,
      xtrain = as.matrix(x),
      ytrain = y,
      obsLevels = lev
    )
    modelFit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    preds <- MGSDA::classifyV(modelFit$xtrain, modelFit$ycodes, as.matrix(newdata), modelFit$V)
    modelFit$obsLevels[preds]
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    # Not implemented in original code. If probabilities are not supported, return NULL or implement if possible.
    # Here we return NULL to avoid errors.
    NULL
  }
)

# lda_thomaz
# Store lev
#' @keywords internal
#' @noRd
MVPAModels$lda_thomaz <- list(
  type = "Classification", 
  library = "sparsediscrim", 
  loop = NULL, 
  parameters=data.frame(parameters="parameter", class="character", label="parameter"),
  
  grid=function(x, y, len = NULL) data.frame(parameter="none"),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    fit <- sparsediscrim:::lda_thomaz(as.matrix(x), y, ...)
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    # Returns a list with element $class
    sparsediscrim:::predict.lda_thomaz(modelFit, as.matrix(newdata))$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    p <- sparsediscrim:::predict.lda_thomaz(modelFit, as.matrix(newdata), type="prob")
    # p is posterior probabilities with columns corresponding to classes in modelFit$obsLevels
    if (!is.null(modelFit$obsLevels)) colnames(p) <- modelFit$obsLevels
    p
  }
)

# hdrda
# Store lev
#' @keywords internal
#' @noRd
MVPAModels$hdrda <- list(
  type = "Classification", 
  library = "sparsediscrim", 
  label="hdrda",
  loop = NULL, 
  parameters=data.frame(parameters=c("lambda","gamma"), class=c("numeric","numeric"), 
                        label=c("HRDA pooling","shrinkage parameter")),
  
  grid=function(x, y, len = NULL) expand.grid(lambda=seq(.99, .001, length.out=len), gamma=seq(.001, .99, length.out=len)),
  
  fit=function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    fit <- sparsediscrim:::hdrda(as.matrix(x), y, lambda=param$lambda, gamma=param$gamma, ...)
    fit$obsLevels <- lev
    fit
  },
  
  predict=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    sparsediscrim:::predict.hdrda(modelFit, as.matrix(newdata))$class
  },
  
  prob=function(modelFit, newdata, preProc = NULL, submodels = NULL) {
    posterior <- sparsediscrim:::predict.hdrda(modelFit, as.matrix(newdata))$posterior
    posterior <- t(apply(posterior,1,function(x) x/sum(x)))
    if (!is.null(modelFit$obsLevels)) colnames(posterior) <- modelFit$obsLevels
    posterior
  }
)



==================================================
File: ./mvpa_model.R
==================================================

#' @keywords internal
#' @noRd
wrap_result <- function(result_table, design, fit=NULL) {
  
  observed <- y_test(design)
  
  ## It could happen that not all design rows are actually tested, which is why we find the unique set of test indices
  testind <- unique(sort(unlist(result_table$test_ind)))
  
  if (is.factor(observed)) {
    prob <- matrix(0, length(testind), length(levels(observed)))
    colnames(prob) <- levels(observed)
    
    for (i in seq_along(result_table$probs)) {
      p <- as.matrix(result_table$probs[[i]])
      tind <- match(result_table$test_ind[[i]], testind)
      prob[tind,] <- prob[tind,] + p
    }
    
    ## probs must sum to one, can divide by sum.
    prob <- t(apply(prob, 1, function(vals) vals / sum(vals)))
    maxid <- max.col(prob)
    pclass <- levels(observed)[maxid]
    
    ## storing observed, testind, test_design 
    classification_result(observed[testind], pclass, prob, testind=testind, design$test_design, fit)
  } else {
    
    testind <- unique(sort(unlist(result_table$test_ind)))
    preds <- numeric(length(testind))
    
    for (i in seq_along(result_table$preds)) {
      #tind <- result_table$test_ind[[i]]
      tind <- match(result_table$test_ind[[i]], testind)
      preds[tind] <- preds[tind] + result_table$preds[[i]]
    }
    
    ## TODO check me
    counts <- table(sort(unlist(result_table$test_ind)))
    preds <- preds/counts
    regression_result(observed, preds, testind=testind, test_design=design$test_design, fit)
  }
}



#' @keywords internal
merge_results.mvpa_model <- function(obj, result_set, indices, id, ...) {
  # Check if any errors occurred during the process
  if (any(result_set$error)) {
    emessage <- result_set$error_message[which(result_set$error)[1]]
    tibble::tibble(result=list(NULL), indices=list(indices), performance=list(NULL), error=TRUE, error_message=emessage)
  } else {
    # If no errors, wrap the result and compute performance if required
    cres <- if (obj$return_fit) {
      predictor <- weighted_model(result_set$fit)
      wrap_result(result_set, obj$design, predictor)
    } else {
      wrap_result(result_set, obj$design)
    }
    
    if (obj$compute_performance) {
      tibble::tibble(result=list(cres), indices=list(indices),
                     performance=list(compute_performance(obj, cres)),
                     id=id, error=FALSE, error_message="~")
    } else {
      tibble::tibble(result=list(cres), indices=list(indices),
                     performance=list(NULL),
                     id=id, error=FALSE, error_message="~")
    }
  }
 
}


#' @noRd
format_result.mvpa_model <- function(obj, result, error_message=NULL, context, ...) {
  if (!is.null(error_message)) {
    return(tibble::tibble(class=list(NULL), probs=list(NULL), y_true=list(context$ytest),
                          fit=list(NULL), error=TRUE, error_message=error_message))
  } else {
    pred <- predict(result, tibble::as_tibble(context$test, .name_repair=.name_repair), NULL)
    plist <- lapply(pred, list)
    plist$y_true <- list(context$ytest)
    plist$test_ind <- list(as.integer(context$test))
    plist$fit <- if (obj$return_fit) list(result) else list(NULL)
    plist$error <- FALSE
    plist$error_message <- "~"
    tibble::as_tibble(plist, .name_repair=.name_repair)
  }
}


#' @keywords internal
get_multiclass_perf <- function(split_list=NULL, class_metrics=TRUE) {
  function(result) {
    performance(result, split_list, class_metrics)
  }
}

#' @keywords internal
get_binary_perf <- function(split_list=NULL) {
  function(result) {
    performance(result, split_list)
  }
}

#' @keywords internal
get_regression_perf <- function(split_list=NULL) {

  function(result) {
    performance(result, split_list)
  }
}


#' @keywords internal
get_custom_perf <- function(fun, split_list) {
  function(result) {
    custom_performance(result, fun, split_list)
  }
}


#' @keywords internal
compute_performance.mvpa_model <- function(obj, result) {
  obj$performance(result)
}

#' @export
tune_grid.mvpa_model <- function(obj, x,y,len=1) {
  if (is.null(obj$tune_grid)) {
    obj$tune_grid <- obj$model$grid(x,y,len)
    obj$tune_grid
  } else {
    obj$tune_grid
  }
}


#' @export
select_features.mvpa_model <- function(obj, X, Y,...) {
  if (!is.null(obj$feature_selector)) {
    select_features(obj$feature_selector, X, Y,...)
  } else {
    ## todo check 'length' function in ROIVector
    rep(TRUE, length(X))
  }
}


#' @export
crossval_samples.mvpa_model <- function(obj,data,y,...) { 
  crossval_samples(obj$crossval,data,y) 
}

#' @export
has_test_set.mvpa_model <- function(obj) {
  !is.null(obj$design$y_test) 
}

#' @export
y_train.mvpa_model <- function(obj) y_train(obj$design)


#' @export
y_test.mvpa_model <- function(obj) y_test(obj$design)


#' @param name A character string indicating the name of the model.
#' @param dataset An `mvpa_dataset` instance.
#' @param design An `mvpa_design` instance.
#' @param return_predictions A \code{logical} indicating whether to return row-wise predictions for each voxel set (defaults to TRUE).
#' @param tune_reps The number of replications used during parameter tuning. Only relevant if `tune_grid` is supplied.
#' @param ... Additional arguments to be passed to the model.
#' @noRd
#' @export
create_model_spec <- function(name, dataset, design, return_predictions=FALSE, tune_reps=FALSE, ...) {
  ret <- list(name=name, dataset=dataset, design=design, return_predictions=return_predictions, tune_reps=tune_reps, ...)
  class(ret) <- c(name, "model_spec", "list")
  ret
}

#' Create an MVPA Model
#'
#' Create an MVPA model based on a caret-based classification or regression model.
#'
#' @param model A caret-based classification or regression model.
#' @param dataset An `mvpa_dataset` instance.
#' @param design An `mvpa_design` instance.
#' @param model_type A character string indicating the problem type: "classification" or "regression".
#' @param crossval An optional `cross_validation` instance.
#' @param feature_selector An optional `feature_selector` instance.
#' @param tune_grid An optional parameter tuning grid as a `data.frame`.
#' @param tune_reps The number of replications used during parameter tuning. Only relevant if `tune_grid` is supplied.
#' @param performance An optional custom function for computing performance metrics.
#' @param class_metrics A logical flag indicating whether to compute performance metrics for each class.
#' @param compute_performance A \code{logical} indicating whether to compute and store performance measures for each voxel set (defaults to TRUE).
#' @param return_predictions A \code{logical} indicating whether to return row-wise predictions for each voxel set (defaults to TRUE).
#' @param return_fits A \code{logical} indicating whether to return the model fit for each voxel set (defaults to FALSE).
#'
#' @export
#'
#' @details
#'
#' If `performance` is supplied, it must be a function that takes one argument and returns a named list of scalar values. 
#' The argument the function takes is a class deriving from `classification_result` appropriate for the problem at hand.
#' See example below.
#'
#' @examples
#'
#' mod <- load_model("sda")
#' traindat <- neuroim2::NeuroVec(array(rnorm(6*6*6*100), c(6,6,6,100)), neuroim2::NeuroSpace(c(6,6,6,100)))
#' mask <- neuroim2::LogicalNeuroVol(array(rnorm(6*6*6)>-.2, c(6,6,6)), neuroim2::NeuroSpace(c(6,6,6)))
#'
#' mvdset <- mvpa_dataset(traindat,mask=mask)
#' design <- data.frame(fac=rep(letters[1:4], 25), block=rep(1:10, each=10))
#' cval <- blocked_cross_validation(design$block)
#' mvdes <- mvpa_design(design, ~ fac, block_var=~block)
#'
#' custom_perf <- function(result) {
#'   c(accuracy=sum(result$observed == result$predicted)/length(result$observed))
#' }
#' mvpmod <- mvpa_model(mod, dataset=mvdset, design=mvdes, crossval=cval, performance=custom_perf)
#' ret <- run_searchlight(mvpmod)
#' stopifnot("accuracy" %in% names(ret))
mvpa_model <- function(model, 
                       dataset,
                       design,
                       model_type=c("classification", "regression"), 
                       crossval=NULL, 
                       feature_selector=NULL, 
                       tune_grid=NULL, 
                       tune_reps=15,
                       performance=NULL,
                       class_metrics=TRUE,
                       compute_performance=TRUE,
                       return_predictions=TRUE,
                       return_fits=FALSE) {
                       
  
  assert_that(!is.null(model$fit))
  
  assert_that(inherits(design, "mvpa_design"))
  assert_that(inherits(dataset, "mvpa_dataset"))
  assert_that(is.logical(class_metrics))
  
  if (is.null(dataset$test_data) && !is.null(design$y_test)) {
    stop("mvpa_model: design has `y_test` dataset must have `test_data`")
  }
  
  if (!is.null(dataset$test_data) && is.null(design$y_test)) {
    stop("mvpa_model: if dataset has `test_data` design must have `y_test`")
  }
  
  perf <- if (!is.null(performance) && is.function(performance)) {
    #assert_that(is.function(performance)) 
    get_custom_perf(performance, design$split_groups)
  } else if (is.numeric(design$y_train)) {
    get_regression_perf(design$split_groups)
  } else if (length(levels(design$y_train)) > 2) {
    get_multiclass_perf(design$split_groups, class_metrics)
  } else if (length(levels(design$y_train)) == 2) {
    get_binary_perf(design$split_groups)
  } else {
    flog.info("class of design$ytrain: %s", class(design$ytrain))
    stop("performance method not found")
  }
  

  model_type <- match.arg(model_type)
  
  if (is.null(crossval) && !is.null(design$block_var)) {
    crossval <- blocked_cross_validation(design$block_var)
  }
  
  assertthat::assert_that(!is.null(crossval))
  
  ## TODO check that crossval is compatible with design
  ## TODO check that mvpa_design is compatible with mvpa_dataset (n training obs == length(y_train))
  
  #assert_that(length(y_train(design)) == )
  ret <- create_model_spec("mvpa_model", 
              dataset=dataset,
              design=design,
              model=model,
              model_type=model_type,
              model_name=model$label,
              tune_grid=tune_grid,
              tune_reps=tune_reps,
              feature_selector=feature_selector,
              crossval=crossval,
              performance=perf,
              compute_performance=compute_performance,
              return_predictions=return_predictions,
              return_fits=return_fits)
  
  ret
  
}


#' @export
has_crossval.mvpa_model <- function(obj) {
  !is.null(obj$crossval)
}

#' @export
#' @method print mvpa_model
print.mvpa_model <- function(x,...) {
  cat("mvpa_model object. \n")
  cat("model: ", x$model_name, "\n")
  cat("model type: ", x$model_type, "\n")
  if (!is.null(x$tune_grid)) {
    cat("tune_reps: ", x$tune_reps, "\n")
    cat("tune_grid: ", "\n")
    print(x$tune_grid)
  }
  
  print(x$crossval)
  print(x$dataset)
  print(x$design)
}
  
  





==================================================
File: ./rMVPA.R
==================================================
#' rMVPA: A package for multi-voxel pattern analysis (MVPA)
#'
#' The rMVPA package provides tools for running region-of-interest and searchlight MVPA analyses.
#' 
#' 
#' @name rMVPA
NULL

##

if(getRversion() >= "2.15.1")  utils::globalVariables(c("."))


==================================================
File: ./common.R
==================================================



#' @import stringr
#' @importFrom io qread
initialize_configuration <- function(args) {
  if (!is.null(args$config)) {
    if (!file.exists(args$config)) {
      flog.error("cannot find configuration file: %s", args$config)
      stop()
    } else if (str_detect(args$config, "\\.yaml$")) {
      confyaml <- qread(args$config)
      config <- as.environment(confyaml)
    } else if (str_detect(args$config, "\\.[rR]")) {
      config <- new.env()
      source(args$config, config)
    }
  }
  
  config

}


#' @noRd
#' @keywords internal
initialize_standard_parameters <- function(config, args, analysisType) {
  set_arg("train_design", config, args, "mvpa_design.txt")
  set_arg("test_design", config, args, NULL)
  set_arg("train_data", config, args, "mvpa_design.txt")
  set_arg("test_data", config, args, NULL)
  set_arg("model", config, args, "corsim")
  set_arg("feature_selector", config, args, NULL)
  set_arg("pthreads", config, args, 1)
  set_arg("label_column", config, args, "labels")
  set_arg("skip_if_folder_exists", config, args, FALSE)
  set_arg("output", config, args, paste0(analysisType, "_", config$labelColumn))
  set_arg("block_column", config, args, NULL)
  set_arg("normalize_samples", config, args, FALSE)
  set_arg("tune_grid", config, args, NULL)
  set_arg("mask", config, args, NULL)
  set_arg("class_metrics", config, args, TRUE)
  set_arg("split_by", config, args, NULL)
  set_arg("custom_performance", config, args, NULL)
  set_arg("test_label_column", config, args, NULL)
  set_arg("data_mode", config, args, "image")
  
  config
}

#' @noRd
#' @keywords internal
#' @importFrom purrr map_dbl map
#' @importFrom neuroim2 SparseNeuroVec vols
normalize_image_samples <- function(bvec, mask) {
  vlist <- bvec %>% vols() %>% furrr::future_map(function(v) {
    scale(as.numeric(v[which(mask>0)]))[,1]
  }, .options=furrr::furrr_options(seed=TRUE))
  
  norm_datavec <- do.call(cbind, vlist)
  #norm_datavec <- do.call(cbind, eachVolume(bvec, function(x) scale(x)[,1], mask=mask))
  SparseNeuroVec(norm_datavec, space(bvec), mask=mask)  
}

#' @noRd
#' @keywords internal
#' @importFrom purrr map_dbl map
#' @importFrom neuroim2 vectors
standardize_vars <- function(bvec, mask, blockvar) {
  assertthat::assert_that(length(blockvar) == dim(bvec)[4])
  vlist <- bvec %>% vectors(subset=which(mask>0)) %>% furrr::future_map(function(v) {
    if (all(v == 0)) v else {
      unlist(map(split(v, blockvar), scale))
    }
  }, .options = furrr::furrr_options(seed=TRUE))
  
  sdatavec <- do.call(cbind, vlist)
  #norm_datavec <- do.call(cbind, eachVolume(bvec, function(x) scale(x)[,1], mask=mask))
  SparseNeuroVec(sdatavec, space(bvec), mask=mask)  
}


#' @noRd
#' @keywords internal       
normalize_surface_samples <- function(bvec, mask) {
  mat <- scale(bvec@data[indices(bvec), ,drop=FALSE])
  
  m2 <- matrix(0, length(nodes(bvec)), ncol(bvec@data))
  m2[indices(bvec),] <- mat
  
  neurosurf::NeuroSurfaceVector(geometry(bvec), indices=indices(bvec), m2)
}

#' @noRd
#' @keywords internal
initialize_surface_data <- function(config) {
  if (!is.null(config$train_subset)) {
    indices <- which(config$train_subset)
    flog.info("length of training subset %s", length(indices))
  }
  
  train_surfaces <- load_surface_data(config, "train_data", colind=indices) 
  
  if (!is.null(config$test_data)) {
    flog.info("loading test surface data: %s", config$test_data)
    indices <- which(config$test_subset)
    flog.info("length of test subset %s", length(indices))
    
    test_surfaces <- if (!is.null(config$test_subset)) {
      load_surface_data(config, "test_data", colind=indices)
    } else {
      load_surface_data(config, "test_data")
    }
    
  } else {
    test_surfaces <- NULL
  }
  
  if (config$normalize_samples) {
    flog.info("Normalizing: centering and scaling each surface of training data")
    ret <- lapply(train_surfaces, normalize_surface_samples)
    names(ret) <- names(train_surfaces)
    train_surfaces <- ret
    
    if (!is.null(test_surfaces)) {
      flog.info("Normalizing: centering and scaling each surface of test data")
      ret <- lapply(test_surfaces, normalize_surface_samples)
      names(ret) <- names(test_surfaces)
      test_surfaces <- ret
    }
  }
  
  if (!is.null(test_surfaces) && !(length(train_surfaces) == length(test_surfaces))) {
    flog.info("number of training surfaces: %s", length(train_surfaces))
    flog.info("number of test surfaces: %s", length(test_surfaces))
    flog.error("number of training surface entries must equal number of test surface entries")
    stop()
  }
  
  ret <- if (!is.null(config$mask)) {
    flog.info("loading mask: %s ", config$mask)
    masksurf <- load_surface_mask(config$mask, train_surfaces)
    
    lapply(seq_along(train_surfaces), function(i) {
      mvpa_surface_dataset(train_surfaces[[i]], test_surfaces[[i]], name=names(train_surfaces)[i], mask=masksurf[[i]])
    })
    
  } else {
    lapply(seq_along(train_surfaces), function(i) {
      mvpa_surface_dataset(train_surfaces[[i]], test_surfaces[[i]], name=names(train_surfaces)[i])
    })
  }
  
  names(ret) <- names(train_surfaces)
  ret

 
}

#' @noRd
#' @keywords internal
initialize_feature_selection <- function(config) {
  if (!is.null(config$feature_selector)) {
    feature_selector(config$feature_selector$method, config$feature_selector$cutoff_type, as.numeric(config$feature_selector$cutoff_value))
  } else {
    NULL
  }
}

#' @importFrom methods as
#' @keywords internal
initialize_image_data <- function(config, mask) {
  if (!is.null(config$train_subset)) {
    indices <- which(config$train_subset)
    flog.info("length of training subset %s", length(indices))
  }
  
  mask_volume <- as(mask, "LogicalNeuroVol")
  train_datavec <- load_image_data(config, "train_data", mask_volume=mask_volume, indices=indices)    

  if (!is.null(config$test_data)) {
    flog.info("loading test data: %s", config$test_data)
    indices=which(config$test_subset)
    flog.info("length of test subset %s", length(indices))
    
    if (!is.null(config$test_subset)) {
      test_datavec <- load_image_data(config, "test_data", mask_volume=mask_volume, indices=indices)
    } else {
      test_datavec <- load_image_data(config, "test_data", mask_volume=mask_volume)
    }
  } else {
    test_datavec <- NULL
  }
  
  if (config$normalize_samples) {
    flog.info("Normalizing: centering and scaling each volume of training data")
    train_datavec <- normalize_image_samples(train_datavec, mask_volume)
    
    if (!is.null(test_datavec)) {
      flog.info("Normalizing: centering and scaling each volume of test data")
      test_datavec <- normalize_image_samples(test_datavec, mask_volume)
    }
  }
  
  mvpa_dataset(train_datavec, test_datavec, mask=mask)

}

#' @noRd
#' @keywords internal
initialize_design <- function(config) {
  if (is.character(config$train_subset)) {
    config$train_subset <- eval(parse(text=config$train_subset))
  }
  
  if (is.character(config$test_subset)) {
    config$test_subset <- eval(parse(text=config$test_subset))
  }
  
  if (is.data.frame(config$train_design)) {
    config$full_train_design <- config$train_design
  } else {
    ## full design
    config$full_train_design <- if (length(config$train_design) > 1) {
      flog.info(paste("concatenating %s design files", length(config$train_design)))
      do.call(rbind, lapply(config$train_design, read.table, header=TRUE, comment.char=";"))
    } else {
      read.table(config$train_design, header=TRUE, comment.char=";")
    }
  }
  
  ## subset of training samples
  config$train_subset <- load_subset(config$full_train_design, config$train_subset)
  
  ## training design
  config$train_design <- config$full_train_design[config$train_subset,]
  
 
  flog.info(paste("training subset contains", nrow(config$train_design), "of", nrow(config$full_train_design), "rows."))
  
  if (!is.null(config$test_design) && is.null(config$test_data)) {
    flog.error("test_design %s is supplied with no test_data", config$test_data)
    stop()
  }
  
  if (is.null(config$test_design) && !is.null(config$test_data)) {
    flog.error("test_data %s is supplied with no test_design", config$test_data)
    stop()
  }
  
  
  #if (!is.null(config$test_subset) && is.null(config$test_design) && is.null(config$test_data)) {
  #  flog.info("test subset is taken from training design table")
  #  config$test_subset <- load_subset(config$full_train_design, config$test_subset)
  #  
  #  config$test_design <- config$full_train_design[config$test_subset,]
  #  config$full_test_design <- config$test_design
  #  config$testLabels <- loadLabels(config$test_design, config)   
  #}
  
  if (!is.null(config$test_design)) {
    has_test <- TRUE
    flog.info("test design %s is specified", config$test_design)
    
    if (is.data.frame(config$test_design)) {
      config$full_test_design <- config$test_design
    } else {
      config$full_test_design <- if (length(config$test_design) > 1) {
        flog.info(paste("concatenating %s test design files", length(config$test_design)))
        do.call(rbind, lapply(config$test_design, read.table, header=TRUE, comment.char=";"))
      } else {
        read.table(config$test_design, header=TRUE, comment.char=";")
      }
    }
    
    flog.info(paste("test design contains", nrow(config$full_test_design), "rows."))
    
    config$test_subset <- load_subset(config$full_test_design, config$test_subset)
    config$test_design <- config$full_test_design[config$test_subset,]
    
    flog.info(paste("test subset contains", nrow(config$test_design), "of", nrow(config$full_test_design), "rows."))
    
    #config$testLabels <- loadTestLabels(config$test_design, config)     
    #flog.info(paste("test subset contains", nrow(config$test_design), "of", nrow(config$full_test_design), "rows.")) 
    #flog.info(paste("first 10 test labels: ", head(config$testLabels, 10), capture=TRUE))
    
  } else {
    has_test <- FALSE
    flog.info("testing is via internal cross-validation")
    #config$testLabels <- config$labels
  }
  
  if (!is.null(config$split_by)) {
    flog.info("splitting performance metrics by %s: ", deparse(config$split_by))
  }
  
  
  if (has_test) {
    ## todo what if we don't have/need "block_column"
    mvpa_design(train_design=config$train_design, 
              y_train=config$label_column, 
              test_design=config$test_design, 
              y_test=config$test_label_column, 
              block_var=config$block_column, 
              split_by=config$split_by)
  } else {
    ## todo what if we don't have/need "block_column"
    mvpa_design(train_design=config$train_design, 
                y_train=config$label_column, 
                block_var=config$block_column, 
                split_by=config$split_by)
  }
   
  
  
}

#' @noRd
#' @keywords internal
initialize_tune_grid <- function(args, config) {
  if (!is.null(args$tune_grid) && !args$tune_grid == "NULL") {
    params <- try(expand.grid(eval(parse(text=args$tune_grid))))
    
    if (inherits(params, "try-error")) {
      stop("could not parse tune_grid expresson: ", args$tune_grid)
    }
    
    flog.info("tuning grid is", params, capture=TRUE)
    params
    
  } else if (!is.null(config$tune_grid) && !is.data.frame(config$tune_grid)) {
    params <- try(lapply(config$tune_grid, function(x) eval(parse(text=x))))
    if (inherits(params, "try-error")) {
      stop("could not parse tune_grid expresson: ", config$tune_grid)
    }
    
    flog.info("tuning grid is", params, capture=TRUE)
    expand.grid(params)
    
  } else if (is.data.frame(config$tune_grid)) {
    config$tune_grid
  } else {
    NULL
  }
  
  
}


#' @noRd
#' @keywords internal
set_default <- function(name, config, default) {
  if (is.null(config[[name]])) {
    config[[name]]<- default
  }
}

#' @noRd
#' @keywords internal
set_arg <- function(name, config, args, default) {
  if (is.null(config[[name]]) && is.null(args[[name]])) {
    config[[name]] <- default
  } else if (!is.null(args[[name]])) {
    config[[name]] <- args[[name]]
  } else if (is.null(config[[name]])) {
    config[[name]] <- default
  }    
}

#' @noRd
#' @keywords internal
make_output_dir <- function(dirname) {
  if (!file.exists(dirname)) {
    system(paste("mkdir", dirname))
    dirname
  } else {
    dirname <- paste(dirname, "+", sep="")
    Recall(dirname)
  }
}

#' @noRd
#' @keywords internal
initialize_crossval <- function(config, des=NULL) {
  cval <- if (is.null(config$cross_validation) && !is.null(des$block_var)) {
    flog.info("cross-validation type: cross validation using predefined blocking variable")
    blocked_cross_validation(des$block_var)
  } else if (!is.null(config$cross_validation)) {
    assertthat::assert_that(!is.null(des))
    cval <- config$cross_validation
    
    if (cval$name == "twofold" || cval$name == "two_fold" || cval$name == "two_fold_blocked_cross_validation") {
      flog.info("cross-validation type: twofold cross-validation.")
      if (is.null(cval$nreps)) {
        cval$nreps <- 10
      }
      flog.info("cross-validation reps: %s ", cval$nreps)
      twofold_blocked_cross_validation(block_var=des$block_var, nreps=cval$nreps)
    } else if (cval$name == "blocked" || cval$name == "blocked_cross_validation") {
      blocked_cross_validation(des$block_var)
    } else if (cval$name == "custom" || cval$name == "custom_cross_validation") {
      custom_cross_validation(cval$sample_set)
    } else {
      flog.error("unrecognized cross_validation type: %s", cval$name)
      stop()
    }
    
  } else {
    flog.info("cross-validation type: 5 fold cross-validation using random splits")
    kfold_cross_validation(nobs(des))
  }
  
  cval
}


#' Load a Pre-defined MVPA Model
#'
#' Retrieves a model specification from either the pre-defined set of MVPA models or from caret's model library.
#'
#' @param name Character string specifying the model to load. Can be either:
#'   \itemize{
#'     \item A pre-defined MVPA model name:
#'     \describe{
#'       \item{corclass}{Correlation-based classifier with template matching}
#'       \item{sda_notune}{Simple Shrinkage Discriminant Analysis without tuning}
#'       \item{sda_boot}{SDA with bootstrap resampling}
#'       \item{glmnet_opt}{Elastic net with EPSGO parameter optimization}
#'       \item{sparse_sda}{SDA with sparsity constraints}
#'       \item{sda_ranking}{SDA with automatic feature ranking}
#'       \item{mgsda}{Multi-Group Sparse Discriminant Analysis}
#'       \item{lda_thomaz}{Modified LDA for high-dimensional data}
#'       \item{hdrda}{High-Dimensional Regularized Discriminant Analysis}
#'     }
#'     \item Any valid model name from caret's model library (e.g., "rf" for random forest, "svmRadial" for SVM)
#'   }
#'
#' @return A list containing the model specification with the following components:
#'   \describe{
#'     \item{type}{Model type: "Classification" or "Regression"}
#'     \item{library}{Required R package(s) for the model}
#'     \item{label}{Human-readable model name}
#'     \item{parameters}{Data frame describing tunable parameters}
#'     \item{grid}{Function to generate parameter tuning grid}
#'     \item{fit}{Function to fit the model}
#'     \item{predict}{Function to generate predictions}
#'     \item{prob}{Function to generate class probabilities (classification only)}
#'   }
#'
#' @examples
#' # Load custom MVPA model
#' model <- load_model("sda_notune")
#' 
#' # Load correlation classifier with parameter tuning options
#' corr_model <- load_model("corclass")
#' print(corr_model$parameters)  # View tunable parameters
#' 
#' # Load caret's random forest model
#' rf_model <- load_model("rf")
#' print(rf_model$parameters)  # View RF parameters
#' 
#' # Load caret's SVM model
#' svm_model <- load_model("svmRadial")
#'
#' @seealso 
#' \code{\link{MVPAModels}} for the complete list of available custom MVPA models
#' 
#' \code{\link[caret]{getModelInfo}} for the complete list of available caret models
#' 
#' \code{\link{mvpa_model}} for using these models in MVPA analyses
#'
#' @export
load_model <- function(name) {
  if (exists(name, envir=MVPAModels)) {
    return(get(name, envir=MVPAModels))
  }
  
  # Try loading from caret if not found in MVPAModels
  caret_model <- try(getModelInfo(name, regex=FALSE)[[1]], silent=TRUE)
  if (!inherits(caret_model, "try-error")) {
    return(caret_model)
  }
  
  stop("Model '", name, "' not found in MVPAModels or caret library")
}
load_model <- function(name) {
  registry <- MVPAModels
  
  ret <- if (!is.null(registry[[name]])) {
    registry[[name]]   
  } else if (length(caret::getModelInfo(name)) > 0) {
    caret::getModelInfo(name)[[name]] 
  } else {
    stop(paste("unrecognized model: ", name))
  }
  
  ret$label <- name
  
  ret
  
}

#' @noRd
load_mask <- function(config) {
  if (config$data_mode == "image") {
    if (!file.exists(config$mask)) {
      flog.error("mask %s does not exist", config$mask)
      stop()
    }
    read_vol(config$mask)
  } else if (config$data_mode == "surface") {
    NULL
  }
}

#' @noRd
load_design <- function(config, name) {
  if (!file.exists(config[[name]])) {
    futile.logger::flog.error(paste("cannot find table named: ", name))
    stop()
  } else {
    read.table(config[[name]], header=TRUE, comment.char=";")
  }
}

#' @noRd
load_mvpa_model <- function(config, dataset, design, crossval, feature_selector) {
  mod <- load_model(config$model)
  mvp_mod <- mvpa_model(mod,dataset, design=design, 
                        model_type=config$model_type,
                        crossval=crossval,
                        feature_selector=feature_selector, 
                        tune_grid=config$tune_grid,
                        performance=config$performance,
                        class_metrics=config$class_metrics)
  
}

#' @keywords internal
#' @importFrom futile.logger flog.error
load_subset <- function(full_design, subset) {
  if (is.character(subset)) {
    if (substr(subset, 1,1) != "~") {
      subset <- paste0("~", subset)
    }   
    subset <- eval(parse(text=subset))
  } 
  
  keep <- if(is.null(subset)) rep(TRUE, nrow(full_design)) else {  
    subexpr <- subset[[2]]   
    keep <- eval(subexpr, full_design)
    if (sum(keep) == nrow(full_design)) {
      warning(paste("subset has same number of rows as full table"))
    }
    if (sum(keep) <= 1) {
      flog.error("train_subset %s results in design with only 1 row.", as.character(subexpr))
      stop()
    }
    keep
  }
  
  keep
  
}


#' @importFrom neuroim2 read_vec read_vol sub_vector
#' @noRd
load_image_data_series <- function(fnames, config, indices, mask_volume) {
  if (!all(file.exists(fnames))) {
    offenders <- fnames[!file.exists(fnames)]
    message(paste("data files", offenders, "not found."))
    stop()
  }
  
  
  ### TODO make more efficient. This loads in all data then subsets.
  #vecmat <- do.call(rbind, lapply(seq_along(fnames), function(i) {
  #  fname <- fnames[i]
  #  flog.info("loading data file %s", fname)
  #  ## TODO does as.matrix do the right thing? must return nonzero subset...
  #  mat <- neuroim2::as.matrix(read_vec(fname, mask=mask_volume))
  #  flog.info("data file %s has %s voxels and %s samples", fname, nrow(mat), ncol(mat))
  #  mat
  #}))
  
  
  ## TODO use indices in read_vec
  vec <- read_vec(fnames, mask=mask_volume)
  
  if (!is.null(indices)) {
    sub_vector(vec, indices)
  }
  
  #SparseNeuroVec(vecmat[indices,], space(mask_volume), mask=mask_volume)
}

#' @noRd
load_image_data <- function(config, name, mask_volume, indices=NULL) {
  fname <- config[[name]]
  if (length(fname) > 1) {
    load_image_data_series(fname, config, indices, mask_volume=mask_volume)
  } else if (!file.exists(fname)) {
    flog.error("datafile %s not found.", fname)
    stop()
  } else {
    flog.info("loading data file %s", fname)
    if (!is.null(indices)) {
      read_vec(fname, indices=indices, mask=mask_volume)
    } else {
      read_vec(fname, mask=mask_volume)
    }
    
  }
}


#' @noRd
load_surface_mask <- function(masklist, train_surf) {
  sections <- names(train_surf)
  assert_that(all(names(sections) == names(masklist)))
  
  masksurfaces <- lapply(sections, function(section) {
    msurf <- neurosurf::read_surf_data(train_surf[[section]]@geometry, masklist[[section]])
    flog.info("mask for %s has %s regions", section, length(unique(msurf@data)))
    msurf
  })
  
  names(masksurfaces) <- sections
  masksurfaces
}

#' @noRd
load_surface_data <- function(config, name, nodeind=NULL, colind=NULL) {
  tdat <- config[[name]]
  sections <- names(tdat)
  
  flog.info("surface sections: ", sections, capture=TRUE)
  
  surfaces <- lapply(sections, function(section) {
    ## TODO check me
    neurosurf::read_surf_data(tdat[[section]]$geometry, tdat[[section]]$data, nodeind=nodeind, colind=colind)
  })
  
  names(surfaces) <- sections
  surfaces
    
}





  

==================================================
File: ./searchlight.R
==================================================
#' Wrap output results
#'
#' This function wraps the output results of the performance matrix into a list
#' of SparseNeuroVec objects for each column in the performance matrix.
#'
#' @keywords internal
#' @param perf_mat A performance matrix containing classifier results.
#' @param dataset A dataset object containing the dataset information.
#' @param ids An optional vector of voxel IDs.
#' @return A named list of SparseNeuroVec objects representing the wrapped output results.
wrap_out <- function(perf_mat, dataset, ids=NULL) {
  out <- lapply(1:ncol(perf_mat), function(i) create_searchlight_performance(dataset, perf_mat[,i], ids))
  names(out) <- colnames(perf_mat)
  
  # Add class and metadata
  structure(
    list(
      results = out,
      n_voxels = length(dataset$mask),
      active_voxels = sum(dataset$mask > 0),
      metrics = colnames(perf_mat)
    ),
    class = c("searchlight_result", "list")
  )
}

#' @export
#' @method print searchlight_result
print.searchlight_result <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  metric_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ Searchlight Analysis Results ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Coverage"), "\n")
  cat(info_style("│  ├─ Total Voxels: "), number_style(format(x$n_voxels, big.mark=",")), "\n")
  cat(info_style("│  └─ Active Voxels: "), number_style(format(x$active_voxels, big.mark=",")), "\n")
  
  # Performance metrics
  cat(section_style("└─ Performance Metrics"), "\n")
  for (metric in x$metrics) {
    results <- x$results[[metric]]
    if (inherits(results, "searchlight_performance")) {
      cat(info_style("   ├─ "), metric_style(metric), "\n")
      cat(info_style("   │  ├─ Mean: "), number_style(sprintf("%.4f", results$summary_stats$mean)), "\n")
      cat(info_style("   │  ├─ SD: "), number_style(sprintf("%.4f", results$summary_stats$sd)), "\n")
      cat(info_style("   │  ├─ Min: "), number_style(sprintf("%.4f", results$summary_stats$min)), "\n")
      cat(info_style("   │  └─ Max: "), number_style(sprintf("%.4f", results$summary_stats$max)), "\n")
    }
  }
  
  if (!is.null(x$pobserved)) {
    cat(section_style("\n└─ Observed Probabilities"), "\n")
    # Add probability summary if needed
  }
  
  cat("\n")
}

#' Combine standard classifier results
#'
#' This function combines the standard classifier results from a good results data frame
#' by binding the performance rows together and optionally computes the observed probabilities.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification
#' @param good_results A data frame containing the successful classifier results
#' @param bad_results A data frame containing the unsuccessful classifier results
#' @return A list containing the combined performance matrix and other information
combine_standard <- function(model_spec, good_results, bad_results) {
  result <- NULL
  
  ind <- unlist(good_results$id)
  perf_mat <- good_results %>% dplyr::select(performance) %>% (function(x) do.call(rbind, x[[1]]))
  
  has_results <- any(unlist(purrr::map(good_results$result, function(x) !is.null(x))))
  ret <- wrap_out(perf_mat, model_spec$dataset, ind)
  
  if (has_results) {
    pobserved <- good_results %>% 
      dplyr::select(result) %>% 
      pull(result) %>% 
      purrr::map(~ prob_observed(.)) %>% 
      bind_cols()
    
    # Create appropriate type of output based on dataset type
    if (inherits(model_spec$dataset, "mvpa_surface_dataset")) {
      # For surface data
      pobserved <- neurosurf::NeuroSurfaceVector(
        geometry = geometry(model_spec$dataset$train_data),
        indices = seq_len(nrow(pobserved)),  # Or appropriate indices
        mat = as.matrix(pobserved)
      )
    } else {
      # For volume data
      pobserved <- SparseNeuroVec(
        as.matrix(pobserved), 
        space(model_spec$dataset$mask), 
        mask=as.logical(model_spec$dataset$mask)
      )
    }
    
    ret$pobserved <- pobserved
  }

  ret
}



#' Combine RSA standard classifier results
#'
#' This function combines the RSA standard classifier results from a good results data frame
#' by binding the performance rows together.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification.
#' @param good_results A data frame containing the successful classifier results.
#' @param bad_results A data frame containing the unsuccessful classifier results.
#' @return A list containing the combined performance matrix along with other information from the dataset.
combine_rsa_standard <- function(model_spec, good_results, bad_results) {
  ind <- unlist(good_results$id)
  perf_mat <- good_results %>% dplyr::select(performance) %>% (function(x) do.call(rbind, x[[1]]))
  ret <- wrap_out(perf_mat, model_spec$dataset, ind)
  ret
}

#' Combine Vector RSA standard classifier results
#'
#' This function combines the Vector RSA standard classifier results from a good results data frame
#' by binding the performance rows together.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification.
#' @param good_results A data frame containing the successful classifier results.
#' @param bad_results A data frame containing the unsuccessful classifier results.
#' @return A list containing the combined performance matrix along with other information from the dataset.
combine_vector_rsa_standard <- function(model_spec, good_results, bad_results) {
  ind <- unlist(good_results$id)
  perf_mat <- good_results %>% dplyr::select(performance) %>% (function(x) do.call(rbind, x[[1]]))
  score_mat <- data.frame(sim=rowMeans(perf_mat))
  ret <- wrap_out(score_mat, model_spec$dataset, ind)
  ret
}




#' Combine randomized classifier results
#'
#' This function combines the randomized classifier results from a good results data frame
#' and normalizes the performance matrix by the number of instances for each voxel index.
#'
#' @keywords internal
#' @param model_spec A list containing the model specification.
#' @param good_results A data frame containing the successful classifier results.
#' @param bad_results A data frame containing the unsuccessful classifier results.
#' @return A list containing the combined and normalized performance matrix along with other information from the dataset.
combine_randomized <- function(model_spec, good_results, bad_results=NULL) {
  #browser()
  all_ind <- sort(unlist(good_results$indices))
  ind_count <- table(all_ind)
  ind_set <- unique(all_ind)
  ncols <- length(good_results$performance[[1]])
  
  perf_mat <- Matrix::sparseMatrix(i=rep(ind_set, ncols), j=rep(1:ncols, each=length(ind_set)), 
                                  x=rep(0, length(ind_set)*ncols), 
                                  dims=c(length(model_spec$dataset$mask), ncols))
  
  for (i in 1:nrow(good_results)) {
    ind <- good_results$indices[[i]]
    if (!is.null(ind)) {
      m <- kronecker(matrix(good_results$performance[[i]], 1, ncols), rep(1,length(ind)))
      perf_mat[ind,] <- perf_mat[ind,] + m
    }
  }

  perf_mat[ind_set,] <- sweep(perf_mat[ind_set,,drop=FALSE], 1, as.integer(ind_count), FUN="/")
  colnames(perf_mat) <- names(good_results$performance[[1]])
  ret <- wrap_out(perf_mat, model_spec$dataset)
  ret
}

#' Pool classifier results
#'
#' This function pools classifier results collected over a set of overlapping indices.
#'
#' @keywords internal
#' @param ... A variable list of data frames containing classifier results to be pooled.
#' @return A list of merged classifier results.
#' @noRd
pool_results <- function(...) {
  reslist <- list(...)
  check <- sapply(reslist, function(res) inherits(res, "data.frame")) 
  assertthat::assert_that(all(check), msg="pool_results: all arguments must be of type 'data.frame'")
  good_results <- do.call(rbind, reslist)
 
  ## the sorted vector of all voxel indices
  all_ind <- sort(unlist(good_results$indices))
  ## how many instances of each voxel?
  ind_count <- table(all_ind)
  ind_set <- unique(all_ind)
  
  ## map every result to the set of indices in that set
  indmap <- do.call(rbind, lapply(1:nrow(good_results), function(i) {
    ind <- good_results$indices[[i]]
    cbind(i, ind)
  }))
  
  
  respsets <- split(indmap[,1], indmap[,2])
  
  merged_results <- purrr::map(respsets, do_merge_results, good_results=good_results)
}



#' Merge searchlight results
#'
#' This function merges searchlight results, combining the first result with the rest of the results.
#'
#' @keywords internal
#' @param r1 A list of indices representing the searchlight results to be merged.
#' @param good_results A data frame containing the valid searchlight results.
#' @return A combined searchlight result object.
do_merge_results <- function(r1, good_results) {
  if (length(r1) > 1) {
    first <- r1[1]
    rest <- r1[2:length(r1)]
    z1 <- good_results$result[[first]]
    z2 <- good_results$result[rest]
    ff <- purrr::partial(merge_results, x=z1)
    do.call(ff, z2)
  } else {
    good_results$result[[r1[1]]]
  }
}

#' Combine randomized searchlight results by pooling
#'
#' This function combines randomized searchlight results by pooling the good results.
#'
#' @keywords internal
#' @param model_spec An object specifying the model used in the searchlight analysis.
#' @param good_results A data frame containing the valid searchlight results.
#' @param bad_results A data frame containing the invalid searchlight results.
#' @return An object containing the combined searchlight results.
pool_randomized <- function(model_spec, good_results, bad_results) {
  if (nrow(good_results) == 0) {
    stop("searchlight: no searchlight samples produced valid results")
  }
  
  
  merged_results <- pool_results(good_results)
  pobserved <- merged_results %>% purrr::map( ~ prob_observed(.)) %>% bind_cols()
  ind_set <- sort(unique(unlist(good_results$indices)))

  all_ids <- which(model_spec$dataset$mask > 0)
  ## if we did not get a result for all voxel ids returned results...
  mask <- if (length(ind_set) != length(all_ids)) {
    mask <- model_spec$dataset$mask
    keep <- all_ids %in% ind_set
    mask[all_ids[!keep]] <- 0
    mask
  } else {
    model_spec$dataset$mask
  }
  
  
  pobserved <- SparseNeuroVec(as.matrix(pobserved), neuroim2::space(mask), mask=as.logical(mask))
  
  #perf_list <- furrr::future_map(merged_results, function(res) compute_performance(model_spec, res))
  perf_list <- purrr::map(merged_results, function(res) compute_performance(model_spec, res))
  
  ncols <- length(perf_list[[1]])
  pmat <- do.call(rbind, perf_list)
  
  perf_mat <- Matrix::sparseMatrix(i=rep(ind_set, ncols), j=rep(1:ncols, each=length(ind_set)), 
                                   x=as.vector(pmat), dims=c(length(model_spec$dataset$mask), ncols))
  
  
  colnames(perf_mat) <- names(perf_list[[1]])
  ret <- wrap_out(perf_mat, model_spec$dataset, ids=NULL) 
  ret$pobserved <- pobserved
  ret
}

#' Perform randomized searchlight analysis
#'
#' This function performs randomized searchlight analysis using a specified model, radius, and number of iterations.
#' It can be customized with different MVPA functions, combiners, and permutation options.
#'
#' @keywords internal
#' @param model_spec An object specifying the model to be used in the searchlight analysis.
#' @param radius The radius of the searchlight sphere.
#' @param niter The number of iterations for randomized searchlight.
#' @param mvpa_fun The MVPA function to be used in the searchlight analysis (default is \code{mvpa_iterate}).
#' @param combiner The function to be used to combine results (default is \code{pool_randomized}).
#' @param ... Additional arguments to be passed to the MVPA function.
#'
#' @importFrom futile.logger flog.error flog.info
#' @importFrom dplyr filter bind_rows
#' @importFrom furrr future_map
do_randomized <- function(model_spec, radius, niter, 
                         mvpa_fun=mvpa_iterate, 
                         combiner=pool_randomized, 
                         ...) {
  error=NULL 
  
  ret <- purrr::map(seq(1,niter), function(i) {
    futile.logger::flog.info("searchlight iteration: %s", i)
    slight <- get_searchlight(model_spec$dataset, "randomized", radius)
    cind <- purrr::map_int(slight, ~ .@parent_index)
    
    # Add debugging
    futile.logger::flog.debug("Searchlight samples: %d", length(slight))
    futile.logger::flog.debug("Parent indices: %d", length(cind))
    
    result <- mvpa_fun(model_spec, slight, cind, ...)
    
    # Add debugging
    futile.logger::flog.debug("MVPA results rows: %d", nrow(result))
    futile.logger::flog.debug("MVPA results columns: %s", paste(colnames(result), collapse=", "))
    
    result
  })
  
  nmodels <- sum(unlist(sapply(ret, nrow)))
  futile.logger::flog.info("number of models fit: %s", nmodels)
 
  results <- dplyr::bind_rows(ret)
  
  # Add debugging
  futile.logger::flog.debug("Combined results rows: %d", nrow(results))
  futile.logger::flog.debug("Combined results columns: %s", paste(colnames(results), collapse=", "))
  
  good_results <- results %>% dplyr::filter(error == FALSE)
  bad_results <- results %>% dplyr::filter(error == TRUE)
  
  # Add debugging
  futile.logger::flog.debug("Good results rows: %d", nrow(good_results))
  futile.logger::flog.debug("Bad results rows: %d", nrow(bad_results))
  
  if (nrow(bad_results) > 0) {
    futile.logger::flog.info(bad_results$error_message)
  }
  
  if (nrow(good_results) == 0) {
    futile.logger::flog.error("no valid results for randomized searchlight, exiting.")
  }
  
  ## could simply merge all searchlights to produce global classification measure  
  combiner(model_spec, good_results)
}



#' Perform standard searchlight analysis
#'
#' This function performs standard searchlight analysis using a specified model and radius.
#' It can be customized with different MVPA functions, combiners, and permutation options.
#'
#' @keywords internal
#' @param model_spec An object specifying the model to be used in the searchlight analysis.
#' @param radius The radius of the searchlight sphere.
#' @param mvpa_fun The MVPA function to be used in the searchlight analysis (default is \code{mvpa_iterate}).
#' @param combiner The function to be used to combine results (default is \code{combine_standard}).
#' @param ... Additional arguments to be passed to the MVPA function.
do_standard <- function(model_spec, radius, mvpa_fun=mvpa_iterate, combiner=combine_standard, ...) {
  error=NULL
  flog.info("creating standard searchlight")
  slight <- get_searchlight(model_spec$dataset, "standard", radius)
  
   
  cind <- which(model_spec$dataset$mask > 0)
  flog.info("running standard searchlight iterator")
  ret <- mvpa_fun(model_spec, slight, cind, ...)
  good_results <- ret %>% dplyr::filter(!error)
  bad_results <- ret %>% dplyr::filter(error == TRUE)
  
  if (nrow(bad_results) > 0) {
    flog.info(bad_results$error_message)
  }
  
  if (nrow(good_results) == 0) {
    ## TODO print out some debug information
    flog.error("no valid results for standard searchlight, exiting.")
  }
  
  combiner(model_spec, good_results, bad_results)
}


#' Run searchlight analysis on a specified MVPA model
#'
#' This function runs a searchlight analysis using a specified MVPA model, radius, and method.
#' It can be customized with a combiner function and permutation options.
#'
#' @param model_spec An object of type \code{mvpa_model} specifying the MVPA model to be used.
#' @param radius The radius of the searchlight sphere (default is 8, allowable range: 1-100).
#' @param method The method used for the searchlight analysis ("randomized" or "standard").
#' @param niter The number of iterations for randomized searchlight (default is 4).
#' @param combiner A function that combines results into an appropriate output, or one of the following strings: "pool" or "average".
#' @param ... Additional arguments to be passed to the function.
#'
#' @import itertools foreach doParallel parallel
#' @importFrom purrr pmap
#' @importFrom futile.logger flog.info flog.error flog.debug
#' @references 
#' Bjornsdotter, M., Rylander, K., & Wessberg, J. (2011). A Monte Carlo method for locally multivariate brain mapping. Neuroimage, 56(2), 508-516.
#' 
#' Kriegeskorte, N., Goebel, R., & Bandettini, P. (2006). Information-based functional brain mapping. Proceedings of the National academy of Sciences of the United States of America, 103(10), 3863-3868.
#' @export
#' @rdname run_searchlight
#' @examples 
#'  
#' dataset <- gen_sample_dataset(c(4,4,4), 100, blocks=3)
#' cval <- blocked_cross_validation(dataset$design$block_var)
#' model <- load_model("sda_notune")
#' mspec <- mvpa_model(model, dataset$dataset, design=dataset$design, model_type="classification", crossval=cval)
#' res <- run_searchlight(mspec, radius=8, method="standard")
#' 
#' # A custom "combiner" can be used to post-process the output of the searchlight classifier for special cases.
#' # In the example below, the supplied "combining function" extracts the predicted probability of the correct class 
#' # for every voxel and every trial and then stores them in a data.frame.
#' 
#' \dontrun{ 
#' custom_combiner <- function(mspec, good, bad) { 
#'    good %>% pmap(function(result, id, ...) { 
#'      data.frame(trial=1:length(result$observed), id=id, prob=prob_observed(result)) 
#'    }) %>% bind_rows()
#' }
#' 
#' res2 <- run_searchlight(mspec, radius=8, method="standard", combiner=custom_combiner)
#' }
run_searchlight.model_spec <- function(model_spec, radius=8, 
                                       method=c("randomized", "standard"),  
                                       niter=4, 
                                       combiner="average", ...) {
  
  if (radius < 1 || radius > 100) {
    stop(paste("radius", radius, "outside allowable range (1-100)"))
  }
  
  method <- match.arg(method)
  
  if (method == "randomized") {
    assert_that(niter >= 1)
  }
  
  #flog.info("model is: %s", model_spec$model$label)
  
  res <- if (method == "standard") {
    flog.info("running standard searchlight with %s radius ", radius)
    if (is.function(combiner)) {
      do_standard(model_spec, radius, combiner=combiner,...)    
    } else {
      if (combiner == "pool") {
        do_standard(model_spec, radius, combiner=combine_standard,...)  
      } else if (combiner == "average") {
        do_standard(model_spec, radius, combiner=combine_standard,...)  
      }
    }
  } else if (method == "randomized") {
    
    flog.info("running randomized searchlight with %s radius and %s iterations", radius, niter)
    if (combiner == "pool") {
      do_randomized(model_spec, radius, niter, combiner=pool_randomized,  ...)
    } else if (combiner == "average") {
      do_randomized(model_spec, radius, niter, combiner=combine_randomized,...)
    } else if (is.function(combiner)) {
      ## combiner could be anything, assume defaults. Extra args will be passed to mvpa_iterate.
      do_randomized(model_spec, radius, niter, combiner=combiner, ...)
    } else {
      stop(paste("'combiner' must be either 'average', 'pool', or a user-supplied custom 'function'"))
    }
  } 
  
}



#' Run searchlight analysis on a specified vector RSA model
#'
#' This function runs a searchlight analysis using a specified vector RSA model, radius, and method.
#' It can be customized with permutation options, distance computation methods, and regression methods.
#'
#' @param model_spec An object of type \code{vector_rsa_model} specifying the vector RSA model to be used.
#' @param radius The radius of the searchlight sphere (default is 8, allowable range: 1-100).
#' @param method The method used for the searchlight analysis ("randomized" or "standard").
#' @param niter The number of iterations for randomized searchlight (default is 4).
#' @param ... Additional arguments to be passed to the function.
#'
#' @importFrom futile.logger flog.info flog.error flog.debug
#' @export
#' @rdname run_searchlight
run_searchlight.vector_rsa <- function(model_spec, radius=8, method=c("randomized", "standard"), niter=4, ...) {
  
  
  
  if (radius < 1 || radius > 100) {
    stop(paste("Radius", radius, "is outside the allowable range (1-100)"))
  }
  
  method <- match.arg(method)
  
  if (method == "randomized") {
    assert_that(niter >= 1, msg="Number of iterations for randomized method must be at least 1")
  }
  
  if (method == "standard") {
    flog.info("Running standard vector RSA searchlight with radius %s", radius)
    results <- do_standard(model_spec, radius, mvpa_fun=vector_rsa_iterate, combiner=combine_vector_rsa_standard, ...)
  } else {
    flog.info("Running randomized vector RSA searchlight with radius %s and %s iterations", radius, niter)
    results <- do_randomized(model_spec, radius, niter=niter, mvpa_fun=vector_rsa_iterate, combiner=combine_randomized,...)
  }
  
  return(results)
}

#' Create a searchlight performance object
#'
#' @keywords internal
#' @param dataset The dataset object
#' @param perf_vec Performance vector for a single metric
#' @param ids Optional vector of voxel IDs
#' @return A searchlight_performance object
create_searchlight_performance <- function(dataset, perf_vec, ids=NULL) {
  # First use the S3 wrap_output method to create the NeuroVol
  ret <- wrap_output(dataset, perf_vec, ids)
  
  # Get non-zero and non-NA values for statistics
  vals <- perf_vec[perf_vec != 0 & !is.na(perf_vec)]
  
  # Then wrap it in our searchlight_performance structure
  structure(
    list(
      data = ret,
      metric_name = names(perf_vec)[1],
      n_nonzero = sum(perf_vec != 0, na.rm = TRUE),
      summary_stats = list(
        mean = if(length(vals) > 0) mean(vals, na.rm = TRUE) else NA,
        sd = if(length(vals) > 0) sd(vals, na.rm = TRUE) else NA,
        min = if(length(vals) > 0) min(vals, na.rm = TRUE) else NA,
        max = if(length(vals) > 0) max(vals, na.rm = TRUE) else NA
      ),
      indices = ids  # Store the indices for reference
    ),
    class = c("searchlight_performance", "list")
  )
}

#' @export
#' @method print searchlight_performance
print.searchlight_performance <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  metric_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ Searchlight Performance: "), 
      metric_style(x$metric_name), header_style(" ▀▀█"), "\n\n")
  
  # Data information
  cat(section_style("├─ Data Summary"), "\n")
  cat(info_style("│  ├─ Non-zero Values: "), 
      if(is.null(x$n_nonzero)) crayon::red("NULL") else number_style(format(x$n_nonzero, big.mark=",")), 
      "\n")
  
  # Statistics
  cat(section_style("└─ Statistics"), "\n")
  
  # Helper function to format stats with better NULL handling
  format_stat <- function(val) {
    if (is.null(val) || (length(val) == 0)) {
      crayon::red("No data")
    } else if (is.na(val)) {
      crayon::red("No valid data")
    } else {
      number_style(sprintf("%.4f", val))
    }
  }
  
  # Safely extract stats with NULL checking
  stats <- x$summary_stats
  if (is.null(stats)) {
    stats <- list(mean=NULL, sd=NULL, min=NULL, max=NULL)
  }
  
  cat(info_style("   ├─ Mean: "), format_stat(stats$mean), "\n")
  cat(info_style("   ├─ SD: "), format_stat(stats$sd), "\n")
  cat(info_style("   ├─ Min: "), format_stat(stats$min), "\n")
  cat(info_style("   └─ Max: "), format_stat(stats$max), "\n\n")
}






==================================================
File: ./dataset.R
==================================================
#' Generate Sample Dataset for MVPA Analysis
#' 
#' Creates a synthetic dataset for testing and demonstration of MVPA analyses.
#'
#' @param D The data dimension(s): vector of length 2 or 3 for image data, or single number for surface data
#' @param nobs The number of observations
#' @param response_type Either 'categorical' or 'continuous'
#' @param data_mode Either 'image' or 'surface'
#' @param spacing The voxel spacing (default: c(1,1,1))
#' @param blocks The number of 'blocks' in the data (for cross-validation)
#' @param nlevels The number of category levels (only used if response_type='categorical')
#' @param external_test Whether to generate an external test set
#' @param split_by Optional factor for splitting analyses
#' @param na_cols The number of columns to randomly set to NA (default: 0)
#' @param ntest_obs The number of test observations (default: nobs)
#'
#' @return A list containing:
#'   \describe{
#'     \item{dataset}{An \code{mvpa_dataset} object containing:
#'       \itemize{
#'         \item \code{train_data}: Training data as \code{NeuroVec} or \code{ROISurface}
#'         \item \code{test_data}: Test data (if external_test=TRUE)
#'         \item \code{mask}: Binary mask indicating valid voxels/vertices
#'       }
#'     }
#'     \item{design}{An \code{mvpa_design} object containing:
#'       \itemize{
#'         \item \code{y_train}: Response variable for training
#'         \item \code{y_test}: Response variable for test set (if external_test=TRUE)
#'         \item \code{block_var}: Block variable for cross-validation
#'         \item \code{split_by}: Optional splitting factor
#'       }
#'     }
#'   }
#'
#' @examples
#' # Generate categorical image dataset
#' dataset <- gen_sample_dataset(
#'   D = c(10,10,10),
#'   nobs = 100,
#'   response_type = "categorical",
#'   data_mode = "image",
#'   blocks = 3,
#'   nlevels = 2
#' )
#'
#' # Generate continuous surface dataset
#' surf_data <- gen_sample_dataset(
#'   D = 1000,  # number of vertices
#'   nobs = 50,
#'   response_type = "continuous",
#'   data_mode = "surface"
#' )
#'
#' # Generate dataset with external test set
#' test_dataset <- gen_sample_dataset(
#'   D = c(8,8,8),
#'   nobs = 80,
#'   response_type = "categorical",
#'   nlevels = 3,
#'   external_test = TRUE
#' )
#'
#' @export
gen_sample_dataset <- function(D, nobs, response_type=c("categorical", "continuous"), 
                             data_mode=c("image", "surface"), spacing=c(1,1,1), 
                             blocks=5, nlevels=5, external_test=FALSE, 
                             ntest_obs=nobs, split_by=NULL, na_cols=0) {
 
  response_type <- match.arg(response_type)
  data_mode <- match.arg(data_mode)
  
  # Ensure na_cols is numeric and has a default
  na_cols <- as.numeric(na_cols)
  if (is.null(na_cols) || is.na(na_cols)) {
    na_cols <- 0
  }
  
  if (data_mode == "image") {
    mat <- array(rnorm(prod(D)*nobs), c(D,nobs))
    if (na_cols > 0) {
      naidx <- sample(dim(mat)[4], na_cols)
      for (naid in naidx) {
        ind <- arrayInd(naid, dim(mat)[1:3])
        mat[ind[1], ind[2], ind[3],] <- NA
      }
    } 
    bspace <- neuroim2::NeuroSpace(c(D,nobs), spacing)
    bvec <- neuroim2::NeuroVec(mat, bspace)
    
    mask <- as.logical(neuroim2::NeuroVol(array(rep(0, prod(D)), D), neuroim2::NeuroSpace(D, spacing)))
    roi <- neuroim2::spherical_roi(mask, ceiling((dim(bspace)[1:3])/2), radius=ceiling(min(dim(bspace)/2)))
    mask[coords(roi)] <- 1
    
    if (external_test) {
      mat <- array(rnorm(prod(D)*ntest_obs), c(D,ntest_obs))
      bspace <- neuroim2::NeuroSpace(c(D,ntest_obs), spacing)
      testvec <- neuroim2::NeuroVec(mat, bspace)
      dset <- mvpa_dataset(train_data=bvec, test_data=testvec, mask=mask)
    } else {
      dset <- mvpa_dataset(train_data=bvec,mask=mask)
    }
  } else {
    fname <- system.file("extdata/std.lh.smoothwm.asc", package="neuroim2")
    geom <- neurosurf::read_surf_geometry(fname)
    nvert <- nrow(neurosurf::vertices(geom))
    mat <- matrix(rnorm(nvert*nobs), nvert, nobs)
    bvec <- neurosurf::NeuroSurfaceVector(geom, 1:nvert, mat)
    
    if (external_test) {
      test_data <- neurosurf::NeuroSurfaceVector(geom, 1:nvert, matrix(rnorm(nvert*ntest_obs), nvert, ntest_obs))
      dset <- mvpa_surface_dataset(train_data=bvec, test_data=test_data)
    } else {
      dset <- mvpa_surface_dataset(train_data=bvec)
    }
  }
  
  Y <- if (response_type == "categorical") {
    sample(factor(rep(letters[1:nlevels], length.out=nobs)))
  } else {
    rnorm(nobs)
  }
  
  Ytest <- if (response_type == "categorical") {
    sample(factor(rep(letters[1:nlevels], length.out=ntest_obs)))
  } else {
    rnorm(ntest_obs)
  }
  
  block_var <- as.integer(as.character(cut(1:nobs, blocks, labels=1:blocks)))
  
  des <- if (external_test) {
    message("external test")
    mvpa_design(data.frame(Y=Y, block_var=block_var), test_design=data.frame(Ytest = Ytest), 
                       block_var= "block_var", y_train= ~ Y, y_test = ~ Ytest, split_by=split_by)
  } else {
    mvpa_design(data.frame(Y=Y, block_var=block_var), block_var="block_var", y_train= ~ Y, split_by=split_by)
  }
  
  list(dataset=dset, design=des)
}


#' Create an MVPA Dataset Object
#'
#' Creates a dataset object for MVPA analysis that encapsulates a training dataset, 
#' an optional test dataset, and a voxel mask.
#'
#' @param train_data The training data set: a \code{NeuroVec} instance
#' @param test_data Optional test data set: a \code{NeuroVec} instance (default: NULL)
#' @param mask The set of voxels to include: a \code{NeuroVol} instance
#'
#' @return An \code{mvpa_dataset} object (S3 class) containing:
#'   \describe{
#'     \item{train_data}{The training data as a \code{NeuroVec} instance}
#'     \item{test_data}{The test data as a \code{NeuroVec} instance (if provided, otherwise NULL)}
#'     \item{mask}{The binary mask defining valid voxels as a \code{NeuroVol} instance}
#'   }
#'
#' @examples
#' # Create dataset from NeuroVec objects
#' train_vec <- NeuroVec(array(rnorm(1000*100), c(10,10,10,100)))
#' mask_vol <- NeuroVol(array(1, c(10,10,10)))
#' dataset <- mvpa_dataset(train_vec, mask=mask_vol)
#'
#' # Create dataset with test data
#' test_vec <- NeuroVec(array(rnorm(1000*20), c(10,10,10,20)))
#' dataset_with_test <- mvpa_dataset(train_vec, test_vec, mask=mask_vol)
#'
#' @seealso 
#' \code{\link{mvpa_surface_dataset}} for creating surface-based MVPA datasets
#' 
#' \code{\link{mvpa_design}} for creating the corresponding design object
#'
#' @importFrom assertthat assert_that
#' @export
mvpa_dataset <- function(train_data, test_data=NULL, mask) {
  assert_that(inherits(train_data, "NeuroVec"))
  if (!is.null(test_data)) {
    assert_that(inherits(test_data, "NeuroVec"))
  }
  assert_that(inherits(mask, "NeuroVol"))
  
  ret <- structure(
    list(
      train_data=train_data,
      test_data=test_data,
      mask=mask
    ),
    class=c("mvpa_image_dataset", "mvpa_dataset", "list")
  )
  ret
}


#' Create a Surface-Based MVPA Dataset Object
#'
#' Creates a dataset object for surface-based MVPA analysis that encapsulates a training dataset,
#' an optional test dataset, and a vertex mask.
#'
#' @param train_data The training data set: must inherit from \code{NeuroSurfaceVector}
#' @param test_data Optional test data set: must inherit from \code{NeuroSurfaceVector} (default: NULL)
#' @param mask Optional binary mask for vertices. If NULL, creates mask from training data indices
#' @param name Optional label to identify the dataset (e.g., "lh" or "rh" to indicate hemisphere)
#'
#' @return An \code{mvpa_surface_dataset} object (S3 class) containing:
#'   \describe{
#'     \item{train_data}{The training data as a \code{NeuroSurfaceVector} instance}
#'     \item{test_data}{The test data as a \code{NeuroSurfaceVector} instance (if provided)}
#'     \item{mask}{A numeric vector indicating valid vertices (1) and excluded vertices (0)}
#'     \item{name}{Character string identifier for the dataset}
#'   }
#'
#' @details
#' If no mask is provided, one will be created automatically using the indices from the training data.
#' The mask will be a numeric vector with length equal to the number of nodes in the surface geometry.
#'
#' @examples
#' \dontrun{
#' # Create surface dataset with automatic mask
#' train_surf <- NeuroSurfaceVector(geometry, data)
#' dataset <- mvpa_surface_dataset(train_surf, name="lh")
#'
#' # Create dataset with test data and custom mask
#' test_surf <- NeuroSurfaceVector(geometry, test_data)
#' mask <- numeric(length(nodes(geometry)))
#' mask[roi_indices] <- 1
#' dataset <- mvpa_surface_dataset(train_surf, test_surf, mask, name="rh")
#' }
#'
#' @seealso 
#' \code{\link{mvpa_dataset}} for creating volume-based MVPA datasets
#' 
#' \code{\link{mvpa_design}} for creating the corresponding design object
#'
#' @importFrom assertthat assert_that
#' @export
mvpa_surface_dataset <- function(train_data, test_data=NULL, mask=NULL, name="") {
  
  assert_that(inherits(train_data, "NeuroSurfaceVector"))
  
  if (!is.null(test_data)) {
    assert_that(inherits(test_data, "NeuroSurfaceVector"))
  }
  
  if (is.null(mask)) {
    mask <- numeric(length(nodes(train_data@geometry)))
    mask[indices(train_data)] <- 1
  }
  
  structure(
    list(
      train_data=train_data,
      test_data=test_data,
      mask=mask,
      name=name
    ),
    class=c("mvpa_surface_dataset", "mvpa_dataset", "list")
  )

  
}

#' @export
#' @method print mvpa_dataset
print.mvpa_dataset <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  dim_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ MVPA Dataset ▀▀█"), "\n\n")
  
  # Training data section
  cat(section_style("├─ Training Data"), "\n")
  dims <- dim(x$train_data)
  dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                   " × ", dim_style(dims[length(dims)]), " observations")
  cat(info_style("│  ├─ Dimensions: "), number_style(dim_str), "\n")
  cat(info_style("│  └─ Type: "), class(x$train_data)[1], "\n")
  
  # Test data section
  cat(section_style("├─ Test Data"), "\n")
  if (is.null(x$test_data)) {
    cat(info_style("│  └─ "), crayon::red("None"), "\n")
  } else {
    dims <- dim(x$test_data)
    dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                     " × ", dim_style(dims[length(dims)]), " observations")
    cat(info_style("│  ├─ Dimensions: "), number_style(dim_str), "\n")
    cat(info_style("│  └─ Type: "), class(x$test_data)[1], "\n")
  }
  
  # Mask information
  cat(section_style("└─ Mask Information"), "\n")
  mids <- table(x$mask[x$mask != 0])
  if (length(mids) > 0) {
    midstr <- paste(names(mids), ":", number_style(mids), collapse = ", ")
    cat(info_style("   ├─ Areas: "), midstr, "\n")
  }
  cat(info_style("   └─ Active voxels/vertices: "), 
      number_style(format(sum(x$mask > 0), big.mark=",")), "\n\n")
}

#' @export
#' @method print mvpa_surface_dataset
print.mvpa_surface_dataset <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  dim_style <- crayon::italic$blue
  name_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ Surface MVPA Dataset ▀▀█"), "\n\n")
  
  # Dataset name
  if (nzchar(x$name)) {
    cat(section_style("├─ Name: "), name_style(x$name), "\n")
  }
  
  # Training data section
  cat(section_style("├─ Training Data"), "\n")
  dims <- dim(x$train_data)
  vertices <- length(nodes(geometry(x$train_data)))
  cat(info_style("│  ├─ Vertices: "), number_style(format(vertices, big.mark=",")), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(dims[length(dims)]), "\n")
  cat(info_style("│  └─ Type: "), class(x$train_data)[1], "\n")
  
  # Test data section
  cat(section_style("├─ Test Data"), "\n")
  if (is.null(x$test_data)) {
    cat(info_style("│  └─ "), crayon::red("None"), "\n")
  } else {
    dims <- dim(x$test_data)
    cat(info_style("│  ├─ Observations: "), number_style(dims[length(dims)]), "\n")
    cat(info_style("│  └─ Type: "), class(x$test_data)[1], "\n")
  }
  
  # Mask information
  cat(section_style("└─ Mask Information"), "\n")
  mids <- table(x$mask[x$mask != 0])
  if (length(mids) > 0) {
    midstr <- paste(names(mids), ":", number_style(mids), collapse = ", ")
    cat(info_style("   ├─ Areas: "), midstr, "\n")
  }
  cat(info_style("   └─ Active vertices: "), 
      number_style(format(sum(x$mask > 0), big.mark=",")), "\n\n")
}


#' @export
#' @method get_searchlight mvpa_image_dataset
get_searchlight.mvpa_image_dataset <- function(obj, type=c("standard", "randomized"), radius=8,...) {
  type <- match.arg(type)
  if (type == "standard") {
    neuroim2::searchlight(obj$mask, radius=radius,...)
  } else {
    neuroim2::random_searchlight(obj$mask, radius=radius,...)
  }
}

#' @export
#' @method get_searchlight mvpa_surface_dataset
get_searchlight.mvpa_surface_dataset <- function(obj, type=c("standard", "randomized"), radius=8,...) {
  type <- match.arg(type)
  #browser()
  # Create the iterator once
  slight <- if (type == "standard") {
    neurosurf::SurfaceSearchlight(geometry(obj$train_data), radius, nodeset=which(obj$mask>0), as_deflist=TRUE)
  } else {
    neurosurf::RandomSurfaceSearchlight(geometry(obj$train_data), radius, nodeset=which(obj$mask>0), as_deflist=TRUE)
  }
  
  slight
}



#' @keywords internal
#' @noRd
#' @importFrom neuroim2 NeuroVol
wrap_output.mvpa_dataset <- function(obj, vals, indices=NULL) {
  if (!is.null(indices)) {
    NeuroVol(vals, space(obj$mask), indices=indices)
  } else {
    NeuroVol(vals, space(obj$mask))
  }
}


#' @keywords internal
#' @noRd
#' @importFrom neurosurf nodes geometry NeuroSurface
wrap_output.mvpa_surface_dataset <- function(obj, vals, indices) {
  #browser()
  
  dvals <- numeric(length(nodes(geometry(obj$train_data))))
  
  #if (length(indices) != length(vals)) {
  #  browser()
  #}
  
  dvals[indices] <- vals[indices]
  ## bit of a hack
  ## we fill in with non-zero rather than allow indices to be missing
  #NeuroSurface(geometry=geometry(obj$train_data), indices=indices, data=dvals)
  NeuroSurface(geometry=geometry(obj$train_data), indices=seq_len(length(dvals)), data=dvals)
}

#' @export
has_test_set.mvpa_dataset <- function(obj) {
  !is.null(obj$test_data) 
}





==================================================
File: ./feature_selection.R
==================================================
## TODO integrate mlr "filters"


#' Feature Selection Methods
#'
#' @section Methods:
#' Two feature selection methods are available:
#' \describe{
#'   \item{FTest}{One-way ANOVA F-test for each feature}
#'   \item{catscore}{Correlation-adjusted t-scores using sda.ranking}
#' }
#'
#' @section Cutoff Types:
#' Two types of cutoffs are supported:
#' \describe{
#'   \item{top_k/topk}{Select top k features}
#'   \item{top_p/topp}{Select top p percent of features (0 < p <= 1)}
#' }
#'
#' @name feature_selection
NULL


#' @keywords internal
#' @importFrom stats pf
#' @noRd
matrixAnova <- function(Y, x) {
  if (!is.numeric(x)) stop("x must be numeric")
  if (nrow(x) != length(Y)) stop("x and Y must have compatible dimensions")
  if (any(is.na(x)) || any(is.na(Y))) stop("NA values not supported")
  x <- as.matrix(x)
  Y <- as.numeric(Y)
  k <- max(Y)
  ni <- tabulate(Y)
  n <- dim(x)[1]
  sx2 <- colSums(x^2)
  m <- rowsum(x, Y)
  a <- colSums(m^2/ni)
  b <- colSums(m)^2/n
  mst <- (a - b)/(k - 1)
  mse <- (sx2 - a)/(n - k)
  fa <- mst/mse
  pvalue <- pf(fa, k - 1, n - k, lower.tail = FALSE, log.p = FALSE)
  tab <- cbind(fa, pvalue)
  colnames(tab) <- c("Ftest", "pval")
  if (!is.null(colnames(x))) 
    rownames(tab) <- colnames(x)
  tab
  
}



#' Create a feature selection specification
#'
#' This function creates a feature selection specification using the provided
#' method, cutoff type, and cutoff value.
#'
#' @param method The type of feature selection method to use. Supported methods are "FTest" and "catscore".
#' @param cutoff_type The type of threshold used to select features. Supported cutoff types are "top_k" and "top_p".
#' @param cutoff_value The numeric value of the threshold cutoff.
#' @return A list with a class name equal to the \code{method} argument.
#' @details
#' The available feature selection methods are:
#'   - FTest: Computes a one-way ANOVA for every column in the feature matrix.
#'   - catscore: Computes a correlation adjusted t-test for every column in the matrix using \code{sda.ranking} from the \code{sda} package.
#' @examples
#' fsel <- feature_selector("FTest", "top_k", 1000)
#' fsel <- feature_selector("FTest", "top_p", .1)
#' class(fsel) == "FTest"
#' @export
feature_selector <- function(method, cutoff_type, cutoff_value) {
  ret <- list(
              cutoff_type=cutoff_type,
              cutoff_value=cutoff_value)
  class(ret) <- c(method, "feature_selector", "list")
  ret
}



#' Perform feature selection using the CATSCORE method
#'
#' This function selects features from the input data matrix X using the
#' CATSCORE method and the provided feature selection specification.
#'
#' @param obj The feature selection specification created by \code{feature_selector()}.
#' @param X The input data matrix.
#' @param Y The response variable.
#' @param ranking.score The feature score to use. Supported scores are "entropy", "avg", or "max". Default is "entropy".
#' @return A logical vector indicating which features to retain.
#' @details
#' The CATSCORE method computes a correlation adjusted t-test for every column in the matrix using \code{sda.ranking} from the \code{sda} package.
#' @seealso \code{\link{feature_selector}} for creating a feature selection specification.
#' @export
#' @examples
#' fsel <- feature_selector("catscore", "top_k", 1000)
#' X <- as.data.frame(matrix(rnorm(100 * 10), 100, 10))
#' Y <- rep(letters[1:5], 20)
#' selected_features <- select_features(fsel, X, Y, ranking.score = "entropy")
#' @importFrom sda sda.ranking
select_features.catscore <- function(obj, X, Y,  ranking.score=c("entropy", "avg", "max"),...) {
  assertthat::assert_that(obj$cutoff_type %in% c("topk", "top_k", "topp", "top_p"))
  ranking.score <- match.arg(ranking.score)
  message("selecting features via catscore")
  
  if (is.numeric(Y)) {
    medY <- median(Y)
    Y <- factor(ifelse(Y > medY, "high", "low"))
  }
  
  
  sda.1 <- sda.ranking(as.matrix(X), Y, ranking.score=ranking.score, fdr=FALSE, verbose=FALSE)
  
  keep.idx <- if (obj$cutoff_type == "top_k") {
    k <- min(ncol(X), obj$cutoff_value)
    sda.1[, "idx"][1:k]
  } else if (obj$cutoff_type == "top_p") {
    if (obj$cutoff_value <= 0 || obj$cutoff_value > 1) {
      stop("select_features.catscore: with top_p, cutoff_value must be > 0 and <= 1")
    }
    k <- max(obj$cutoff_value * ncol(X),1)
    sda.1[, "idx"][1:k]
   
  } else {
    stop(paste("select_features.catscore: unsupported cutoff_type: ", obj$cutoff_type))
  }
  
  
  keep <- logical(ncol(X))
  keep[keep.idx] <- TRUE
  message("retaining ", sum(keep), " features in matrix with ", ncol(X), " columns")
  keep
   
}



#' Perform feature selection using the F-test method
#'
#' This function selects features from the input data matrix X using the
#' F-test method and the provided feature selection specification.
#'
#' @param obj The feature selection specification created by \code{feature_selector()}.
#' @param X The input data matrix.
#' @param Y The response variable.
#' @param ... extra args (not used)
#' @return A logical vector indicating which features to retain.
#' @details
#' The F-test method computes a one-way ANOVA for every column in the feature matrix.
#' @seealso \code{\link{feature_selector}} for creating a feature selection specification.
#' @export
#' @examples
#' fsel <- feature_selector("FTest", "top_k", 1000)
#' X <- as.data.frame(matrix(rnorm(100 * 10), 100, 10))
#' Y <- rep(letters[1:5], 20)
#' selected_features <- select_features(fsel, X, Y)
#' @importFrom assertthat assert_that
select_features.FTest <- function(obj, X, Y,...) {
  message("selecting features via FTest")
  message("cutoff type ", obj$cutoff_type)
  message("cutoff value ", obj$cutoff_value)
  
 
  assertthat::assert_that(obj$cutoff_type %in% c("topk", "top_k", "topp", "top_p"))
  
  if (is.numeric(Y)) {
    medY <- median(Y)
    Y <- factor(ifelse(Y > medY, "high", "low"))
  }
  
  pvals <- matrixAnova(Y,X)[,2]
  
  keep.idx <- if (obj$cutoff_type == "top_k" || obj$cutoff_type == "topk") {
    k <- min(ncol(X), obj$cutoff_value)
    order(pvals)[1:k]
  } else if (obj$cutoff_type == "top_p" || obj$cutoff_type == "topp") {
    if (obj$cutoff_value <= 0 || obj$cutoff_value > 1) {
      stop("select_features.FTest: with top_p, cutoff_value must be > 0 and <= 1")
    }
    k <- obj$cutoff_value * ncol(X)
    order(pvals)[1:k]
  } else {
  
    stop(paste("select_features.FTest: unsupported cutoff_type: ", obj$cutoff_type))
  }
  
  
  keep <- logical(ncol(X))
  keep[keep.idx] <- TRUE
  
  message("retaining ", sum(keep), " features in matrix with ", ncol(X), " columns")
  
  keep
  
}



# Common validation function
validate_cutoff <- function(type, value, ncol) {
  type <- tolower(type)
  if (!type %in% c("top_k", "topk", "top_p", "topp")) {
    stop("Cutoff type must be one of: top_k, topk, top_p, topp")
  }
  
  if (grepl("p$", type)) {
    if (value <= 0 || value > 1) {
      stop("For percentage cutoff, value must be > 0 and <= 1")
    }
    max(ceiling(value * ncol), 1)
  } else {
    min(value, ncol)
  }
}




==================================================
File: ./rsa_model.R
==================================================
#' @noRd
#' @keywords internal
sanitize <- function(name) {
  name <- gsub(":", ".", name)
  name <- gsub(" ", "", name)
  name <- gsub("[\\(\\)]", ".", name, perl=TRUE)
  name <- gsub(",", "_", name)
  name <- gsub("\\.$", "", name)
  name
}



#' Construct a design for an RSA (Representational Similarity Analysis) model
#'
#' This function constructs a design for an RSA model using the provided formula, data, and optional parameters.
#'
#' @param formula A formula expression specifying the dissimilarity-based regression function.
#' @param data A named list containing the dissimilarity matrices and any other auxiliary variables.
#' @param block_var An optional \code{formula}, \code{character} name or \code{integer} vector designating the block structure.
#' @param split_by An optional \code{formula} indicating grouping structure for evaluating test performance.
#' @param keep_intra_run A \code{logical} indicating whether to include within-run comparisons (default: FALSE).
#' @return A list with class attributes "rsa_design" and "list", containing:
#'   \describe{
#'     \item{formula}{The input formula}
#'     \item{data}{The input data}
#'     \item{split_by}{The split_by formula}
#'     \item{split_groups}{Grouping structure for split_by}
#'     \item{block_var}{Block structure}
#'     \item{include}{Logical vector for including/excluding comparisons}
#'     \item{model_mat}{Model matrix generated by rsa_model_mat}
#'   }
#' @details
#' The function creates an RSA design based on the input parameters. It checks the validity of the input data and
#' handles splitting conditions for evaluation of test performance. It also processes optional block structures and
#' within-run comparisons.
#' @importFrom assertthat assert_that
#' @export
#' @examples
#' dismat <- dist(matrix(rnorm(100*100), 100, 100))
#' rdes <- rsa_design(~ dismat, list(dismat=dismat))
rsa_design <- function(formula, data, block_var=NULL, split_by=NULL, keep_intra_run=FALSE) {
  assert_that(purrr::is_formula(formula))
  
  # Check that all variables are either matrices, "dist", or vectors
  nr <- sapply(data, function(x) {
    if (is.matrix(x)) {
      nrow(x)
    } else if (inherits(x, "dist")) {
      attr(x, "Size")
    } else if (is.vector(x)) {
      length(x)
    } else {
      stop(paste("illegal variable type", class(x)))
    }
  })
  
  assert_that(all(nr == nr[1]), msg="all elements in 'data' must have the same number of rows")
  
  check_split <- function(split_var) {
    minSplits <- min(table(split_var))
    if (minSplits < 3) {
      stop(paste("error: splitting condition results in fewer than 3 observations in at least one set"))
    }
  }
  
  # Create split groups if split_by is provided
  split_groups <- if (!is.null(split_by)) {
    split_var <- parse_variable(split_by, data)
    split(seq_along(split_var), split_var)
  }
  
  # Process block_var if provided
  block_var <- if (!is.null(block_var)) {
    parse_variable(block_var, data)
  }
  
  # Include/exclude within-run comparisons based on keep_intra_run
  include <- if (!is.null(block_var) && !keep_intra_run) {
    as.vector(dist(block_var)) != 0
  }
  
  # Create the RSA design as a list
  des <- list(
    formula=formula,
    data=data,
    split_by=split_by,
    split_groups=split_groups,
    block_var=block_var,
    include=include
  )
  
  # Add model matrix to the design list
  mmat <- rsa_model_mat(des)
  des$model_mat <- mmat
  
  # Set the class attributes
  class(des) <- c("rsa_design", "list")
  
  # Return the RSA design
  des
}


#' Construct a model matrix for an RSA (Representational Similarity Analysis) design
#'
#' This function constructs a model matrix for the given RSA design by processing distance matrices and other variables.
#'
#' @param rsa_des An RSA design object created by \code{rsa_design()}, containing formula, data, and optional parameters.
#' @return A named list of vectors, where:
#'   \itemize{
#'     \item Names correspond to sanitized variable names from the formula
#'     \item Each vector is the processed version of the corresponding input data
#'     \item For distance matrices, only the lower triangle is included
#'     \item If rsa_des$include is specified, vectors are subset accordingly
#'   }
#' @details
#' The function takes an RSA design object as input and processes the distance matrices and other variables to
#' construct a model matrix. It handles different types of input matrices, including symmetric and asymmetric
#' distance matrices, and can include or exclude within-run comparisons based on the RSA design.
#' @examples
#' dismat <- dist(matrix(rnorm(100*100), 100, 100))
#' rdes <- rsa_design(~ dismat, list(dismat=dismat))
#' rsa_model_mat(rdes)
#' @keywords internal
#' @noRd
rsa_model_mat <- function(rsa_des) {
  rvars <- labels(terms(rsa_des$formula))
  denv <- list2env(rsa_des$data)
  vset <- lapply(rvars, function(x) eval(parse(text=x), denv))
  
  # Process input variables to create vectors from distance matrices
  vmatlist <- lapply(vset, function(v) {
    if (inherits(v, "dist")) {
      # An distance matrix of class "dist"
      as.vector(v)
    } else if (isSymmetric(v)) {
      # A full distance matrix
      v[lower.tri(v)]
    } else {
      as.vector(dist(v))
    }
  })
  
  # Include or exclude within-run comparisons based on rsa_des$include
  if (!is.null(rsa_des$include)) {
    vmatlist <- lapply(vmatlist, function(v) v[rsa_des$include])
  }
  
  # Assign sanitized names to the output list
  names(vmatlist) <- sanitize(rvars)
  
  # Return the model matrix as a named list of vectors
  vmatlist
}


#' Construct an RSA (Representational Similarity Analysis) model
#'
#' This function creates an RSA model object by taking an MVPA (Multi-Variate Pattern Analysis) dataset and an RSA design.
#'
#' @param dataset An instance of an \code{mvpa_dataset}.
#' @param design An instance of an \code{rsa_design} created by \code{rsa_design()}.
#' @param distmethod A character string specifying the method used to compute distances between observations. One of: \code{"pearson"} or \code{"spearman"} (defaults to "spearman").
#' @param regtype A character string specifying the analysis method. One of: \code{"pearson"}, \code{"spearman"}, \code{"lm"}, or \code{"rfit"} (defaults to "pearson").
#' @return A list with two elements: \code{dataset} and \code{design}, with the class attribute set to \code{"rsa_model"} and \code{"list"}.
#' @examples
#' # Create a random MVPA dataset
#' data <- matrix(rnorm(100 * 100), 100, 100)
#' labels <- factor(rep(1:2, each = 50))
#' mvpa_data <- mvpa_dataset(data, labels)
#'
#' # Create an RSA design
#' dismat <- dist(data)
#' rdes <- rsa_design(~ dismat, list(dismat = dismat))
#'
#' # Create an RSA model with default parameters
#' rsa_mod <- rsa_model(mvpa_data, rdes)
#'
#' # Create an RSA model with custom parameters
#' rsa_mod_custom <- rsa_model(mvpa_data, rdes, distmethod = "pearson", regtype = "lm")
#' @export
rsa_model <- function(dataset, design, distmethod = "spearman", regtype = "pearson") {
  assert_that(inherits(dataset, "mvpa_dataset"))
  assert_that(inherits(design, "rsa_design"))
  
  distmethod <- match.arg(distmethod, c("pearson", "spearman"))
  regtype <- match.arg(regtype, c("pearson", "spearman", "lm", "rfit"))
  
  create_model_spec("rsa_model", dataset, design, distmethod = distmethod,
                    regtype = regtype)
}



#' @keywords internal
#' @importFrom Rfit rfit
#' @noRd
run_rfit <- function(dvec, obj) {
  form <- paste("dvec", "~", paste(names(obj$design$model_mat), collapse = " + "))
  obj$design$model_mat$dvec <- dvec
  res <- Rfit::rfit(form, data=obj$design$model_mat)
  coef(res)[-1]
}


#' @keywords internal
#' @importFrom stats coef cor dist rnorm terms lm sd
#' @noRd
run_lm <- function(dvec, obj) {
  form <- paste("dvec", "~", paste(names(obj$design$model_mat), collapse = " + "))
  vnames <- names(obj$design$model_mat)
  obj$design$model_mat$dvec <- dvec
  res <- lm(form, data=obj$design$model_mat)
  res <- coef(summary(res))[-1,3]
  names(res) <- vnames
  res
}

#' @keywords internal
#' @noRd
run_cor <- function(dvec, obj) {
  res <- sapply(obj$design$model_mat, function(x) cor(dvec, x, method=obj$distmethod))
  names(res) <- names(obj$design$model_mat)
  res
}

#' Train an RSA Model
#'
#' This function trains an RSA (representational similarity analysis) model using the specified method and distance calculation.
#'
#' @param obj An object of class \code{rsa_model}.
#' @param train_dat The training data.
#' @param indices The indices of the training data.
#' @param ... Additional arguments passed to the training method.
#' @return The trained model.
#' @export
train_model.rsa_model <- function(obj, train_dat, indices, ...) {
  dtrain <- 1 - cor(t(train_dat), method=obj$distmethod)
  dvec <- dtrain[lower.tri(dtrain)]
  
  if (!is.null(obj$design$include)) {
    dvec <- dvec[obj$design$include]
  }
  
  switch(obj$regtype,
         rfit=run_rfit(dvec, obj),
         lm=run_lm(dvec,obj),
         pearson=run_cor(dvec,obj),
         spearman=run_cor(dvec,obj))
  
}

#' @export
# process_roi.rsa_model <- function( mod_spec, roi, rnum,...) {
#   xtrain <- tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair=.name_repair)
#   ind <- indices(roi$train_roi)
#   ret <- try(train_model(mod_spec, xtrain, ind))
#   if (inherits(ret, "try-error")) {
#     tibble::tibble(result=list(NULL), indices=list(ind), performance=list(ret), id=rnum, error=TRUE, error_message=attr(ret, "condition")$message)
#   } else {
#     tibble::tibble(result=list(NULL), indices=list(ind), performance=list(ret), id=rnum, error=FALSE, error_message="~")
#   }
# }

#' @export
#' @method print rsa_model
print.rsa_model <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  method_style <- crayon::magenta
  formula_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ RSA Model ▀▀█"), "\n\n")
  
  # Model configuration
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Distance Method: "), method_style(x$distmethod), "\n")
  cat(info_style("│  └─ Regression Type: "), method_style(x$regtype), "\n")
  
  # Dataset information
  cat(section_style("├─ Dataset"), "\n")
  dims <- dim(x$dataset$train_data)
  dim_str <- paste0(paste(dims[-length(dims)], collapse=" × "), 
                   " × ", number_style(dims[length(dims)]), " observations")
  cat(info_style("│  ├─ Dimensions: "), dim_str, "\n")
  cat(info_style("│  └─ Type: "), class(x$dataset$train_data)[1], "\n")
  
  # Design information
  cat(section_style("├─ Design"), "\n")
  cat(info_style("│  ├─ Formula: "), formula_style(deparse(x$design$formula)), "\n")
  
  # Variables in model matrix
  var_names <- names(x$design$model_mat)
  cat(info_style("│  └─ Predictors: "), method_style(paste(var_names, collapse=", ")), "\n")
  
  # Structure information
  cat(section_style("└─ Structure"), "\n")
  
  # Block information
  if (!is.null(x$design$block_var)) {
    blocks <- table(x$design$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), number_style(length(blocks)), "\n")
    cat(info_style("   ├─ Mean Block Size: "), 
        number_style(format(mean(blocks), digits=2)),
        crayon::italic$white(" (SD: "),
        number_style(format(sd(blocks), digits=2)),
        crayon::italic$white(")"), "\n")
  } else {
    cat(info_style("   ├─ Blocking: "), crayon::red("None"), "\n")
  }
  
  # Split information
  if (!is.null(x$design$split_by)) {
    split_info <- length(x$design$split_groups)
    cat(info_style("   └─ Split Groups: "), number_style(split_info), "\n")
  } else {
    cat(info_style("   └─ Split Groups: "), crayon::red("None"), "\n")
  }
  
  cat("\n")
}

#' @export
#' @method print rsa_design
print.rsa_design <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  formula_style <- crayon::italic$blue
  var_style <- crayon::magenta
  
  # Print header
  cat("\n", header_style("█▀▀ RSA Design ▀▀█"), "\n\n")
  
  # Formula section
  cat(section_style("├─ Formula"), "\n")
  cat(info_style("│  └─ "), formula_style(deparse(x$formula)), "\n")
  
  # Data section
  cat(section_style("├─ Variables"), "\n")
  var_types <- sapply(x$data, function(v) {
    if (inherits(v, "dist")) "distance matrix"
    else if (is.matrix(v)) "matrix"
    else if (is.vector(v)) "vector"
    else "other"
  })
  
  cat(info_style("│  ├─ Total Variables: "), number_style(length(x$data)), "\n")
  for (i in seq_along(x$data)) {
    prefix <- if (i == length(x$data)) "└" else "├"
    cat(info_style(sprintf("│  %s─ ", prefix)), 
        var_style(names(x$data)[i]), ": ", 
        number_style(var_types[i]), "\n")
  }
  
  # Structure section
  cat(section_style("└─ Structure"), "\n")
  
  # Block information
  if (!is.null(x$block_var)) {
    blocks <- table(x$block_var)
    cat(info_style("   ├─ Blocking: "), "Present\n")
    cat(info_style("   ├─ Number of Blocks: "), number_style(length(blocks)), "\n")
    cat(info_style("   ├─ Block Sizes: "), 
        number_style(paste0(names(blocks), ": ", blocks, collapse=", ")), "\n")
  } else {
    cat(info_style("   ├─ Blocking: "), crayon::red("None"), "\n")
  }
  
  # Include/exclude information
  if (!is.null(x$include)) {
    n_comparisons <- length(x$include)
    n_included <- sum(x$include)
    cat(info_style("   └─ Comparisons: "), 
        number_style(n_included), 
        crayon::italic$white(" of "), 
        number_style(n_comparisons), 
        crayon::italic$white(sprintf(" (%.1f%%)", 100*n_included/n_comparisons)), "\n")
  } else {
    cat(info_style("   └─ Comparisons: "), "All included\n")
  }
  
  cat("\n")
}







==================================================
File: ./crossval.R
==================================================


#' @noRd
#' @keywords internal
check_len <- function(y, block_var) {
  futile.logger::flog.debug("Checking length of y and block_var")
  futile.logger::flog.debug("y: %s", paste(dim(y), collapse=" x "))
  futile.logger::flog.debug("block_var: %s", length(block_var))
  if (is.vector(y)) {
    if (!length(block_var) == length(y)) {
      stop("length of `block_var` must be equal to length(y)", call. = FALSE)
    }
  } else if (is.matrix(y)) {
    if (!nrow(y) == length(block_var)) {
      stop("number of rows in `y` must be equal to length(block_var)", call. = FALSE)
    }
  }
}


#' @noRd
#' @keywords internal
subset_y <- function(y, idx) {
  if (is.vector(y) || is.factor(y)) {
    y[idx]
  } else if (is.matrix(y)) {
    y[idx,,drop=FALSE]
  }
}

#' K-fold Cross-Validation Data Preparation
#'
#' This function prepares the data for k-fold cross-validation by dividing the
#' dataset into k folds. It creates subsets of training and testing data for
#' each fold without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param k An integer specifying the number of folds for cross-validation.
#' @param id A character string specifying the identifier for the output data frame.
#' @return A tibble containing the training and testing data, response vectors, and fold IDs for each fold.
#' @examples
#' data <- iris[,-5]
#' y <- iris$Species
#' result <- crossv_k(data, y, k = 5)
#' @importFrom modelr resample
#' @export
crossv_k <- function(data, y, k = 5, id = ".id") {
  if (!is.numeric(k) || length(k) != 1) {
    stop("`k` must be a single integer.", call. = FALSE)
  }
  
  
  n <- nrow(data)
  folds <- sample(rep(1:k, length.out = n))
  
  idx <- seq_len(n)
  fold_idx <- split(idx, folds)
  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, setdiff(idx, test)),
      test = modelr::resample(data, test)
    )
  }
  
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <- gen_id(k)
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' Repeated Two-Fold Cross-Validation Data Preparation
#'
#' This function prepares the data for repeated two-fold cross-validation by
#' dividing the dataset into two folds based on the provided block variable.
#' It creates subsets of training and testing data for each repetition without
#' performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param block_ind A vector containing the ordered integer IDs of the blocks (optional).
#' @param id A character string specifying the identifier for the output data frame.
#' @param nreps An integer specifying the number of repetitions for two-fold cross-validation.
#' @return A tibble containing the training and testing data, response vectors, and fold IDs for each repetition.
#' @examples
#' X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
#' y <- rep(letters[1:4], 25)
#' block_var <- rep(1:4, each = 25)
#' cv <- crossv_twofold(X, y, block_var, nreps = 10)
#' @noRd
crossv_twofold <- function(data, y, block_var, block_ind=NULL, id = ".id", nreps=15) {
  ## every time this is called, it regenerates new indices
  if (nreps < 2) {
    stop("'nreps' must be at least 2")
  }
  
  check_len(y, block_var)
  
  if (is.null(block_ind)) {
    block_ind <- seq(1, length(sort(unique(block_var))))
  }

  nhalf <- floor(length(block_ind)/2)
  assert_that(nhalf > 0)
  
  fold_sets <- utils::combn(block_ind, nhalf)
  nreps <- min(nreps, ncol(fold_sets))
  
 
  cols <- as.integer(seq(1, ncol(fold_sets), length.out=nreps))
  #sample(1:ncol(fold_sets), nreps)
 
  fold_idx <- lapply(1:nreps, function(i) {
    bind <- fold_sets[, cols[i]]
    which(block_var %in% bind)
  })
  
  idx <- seq_len(nrow(data))
  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, test)
    )
  }
  
  ## this could return a function that when given data, returns a tibble
  ## fun <- function(data) {... for every rows, mutate(train = ..., test = ...)} 
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <-gen_id(length(fold_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")
  
  
}


#' Block Cross-Validation Data Preparation
#'
#' This function prepares the data for block cross-validation by dividing the dataset
#' based on the provided block variable. It creates subsets of training and testing
#' data for each block without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param id A character string specifying the identifier for the output data frame.
#' @return A tibble containing the training and testing data, response vectors, and block IDs for each fold.
#' @examples
#' X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
#' y <- rep(letters[1:4], 25)
#' block_var <- rep(1:4, each = 25)
#' cv <- crossv_block(X, y, block_var)
#' @export
crossv_block <- function(data, y, block_var, id = ".id") {
 
  check_len(y, block_var)
  
  idx <- seq_len(nrow(data))
  fold_idx <- split(idx, block_var)

  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, test)
    )
  }
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <- gen_id(length(fold_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' Block Bootstrap Cross-Validation Data Preparation
#'
#' This function prepares the data for block bootstrap cross-validation by dividing the dataset
#' based on the provided block variable. It creates subsets of training and testing
#' data for each block using bootstrap sampling within the training blocks, without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param nreps An integer specifying the number of bootstrap repetitions.
#' @param id A character string specifying the identifier for the output data frame.
#' @param weights An optional numeric vector of weights to be used for bootstrap sampling.
#'
#' @details
#' The function first checks if the length of the `block_var` vector matches the length of the response vector `y`.
#' It then creates a list of block indices and ensures there is more than one block to bootstrap. If weights are provided,
#' the function splits the weights according to the block variable.
#'
#' The function performs bootstrap sampling within the training blocks but keeps the test set fixed.
#' For each block, it generates a list of training indices using bootstrap sampling and creates the corresponding
#' training and testing data sets.
#'
#' @return A tibble containing the training and testing data, response vectors, and block IDs for each fold.
#'
#' @keywords internal
crossv_bootstrap_block <- function(data, y, block_var, nreps=5, id = ".id", weights=NULL) {
  check_len(y, block_var)
  
  idx <- seq_len(nrow(data))
  block_idx <- split(idx, block_var)
  
  
  assert_that(length(block_idx) > 1, msg="crossv_bootstrap_block: must have more than one block to bootstrap.")
  
 
  ## alter so that you bootstrap within the training blocks but test set is fixed
  fold_idx <- if (!is.null(weights)) {
    block_wts <- split(weights, block_var)
    fold_idx <- lapply(seq_along(block_idx), function(heldout) {
      replicate(nreps, sample(unlist(block_idx[-heldout]), replace=TRUE, prob=unlist(block_wts[-heldout])), simplify=FALSE)
    })
  } else {
    fold_idx <- lapply(seq_along(block_idx), function(heldout) {
      replicate(nreps, sample(unlist(block_idx[-heldout]), replace=TRUE), simplify=FALSE)
    })
  }
  

  
  #fold_idx <- lapply(unlist(fold_idx, recursive=FALSE), sort)
  #fold_idx <- lapply(fold_idx, function(fidx) setdiff(idx, fidx))
  
  
  fold <- function(tidx, block) {
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, block_idx[[block]]),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, block_idx[[block]])
    )
  }
  
  cols <- unlist(lapply(seq_along(fold_idx), function(i) {
    lapply(fold_idx[[i]], function(tidx) {
      fold(tidx, i)
    })
  }), recursive=FALSE)
  
  cols <- purrr::transpose(cols)
  cols[[id]] <- gen_id(nreps * length(block_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' Sequential Block Cross-Validation Data Preparation
#'
#' This function prepares the data for sequential block cross-validation by dividing the dataset
#' based on the provided block variable. It creates subsets of training and testing
#' data for each block using sequential sampling within the blocks, without performing any analysis or fitting models.
#'
#' @param data A data frame containing the training data.
#' @param y A response vector.
#' @param nfolds An integer specifying the number of folds for cross-validation.
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param nreps An integer specifying the number of repetitions for each fold.
#' @param block_ind An optional integer vector specifying the ordered ids of the blocks.
#' @param id A character string specifying the identifier for the output data frame.
#'
#' @details
#' The function first checks if the length of the `block_var` vector matches the length of the response vector `y`.
#' It then creates a list of block indices and generates a fold sequence using the provided `nfolds` and `nreps` parameters.
#'
#' For each repetition and fold, the function identifies the indices corresponding to the test data and creates the
#' corresponding training and testing data sets.
#'
#' @return A tibble containing the training and testing data, response vectors, and fold IDs for each fold and repetition.
#'
#' @examples
#' X <- data.frame(x1=rnorm(100), x2=rnorm(100))
#' y <- rep(letters[1:4], 25)
#' block_var <- rep(1:4, each=25)
#' cv <- crossv_seq_block(X,y,2, block_var)
#' @noRd
crossv_seq_block <- function(data, y, nfolds, block_var, nreps=4, block_ind = NULL, id = ".id") {
  check_len(y, block_var)
  
  idx <- seq_len(nrow(data))
  block_idx <- split(idx, block_var)
  
  if (is.null(block_ind)) {
    block_ind <- seq(1, length(sort(unique(block_var))))
  }
  
  foldseq <- replicate(nreps, {
    unlist(lapply(block_idx, function(id) {
      as.integer(as.character(cut(id, nfolds, labels=sample(1:nfolds))))
    }))
    
  }, simplify=FALSE)
  
  fold_idx <- unlist(lapply(1:nreps, function(i) {
    lapply(1:nfolds, function(j) which(foldseq[[i]] == j))
  }), recursive=FALSE)
  
  
  fold <- function(test) {
    tidx <- setdiff(idx, test)
    list(
      ytrain = subset_y(y, tidx),
      ytest = subset_y(y, test),
      train = modelr::resample(data, tidx),
      test = modelr::resample(data, test)
    )
  }
  
  cols <- purrr::transpose(purrr::map(fold_idx, fold))
  cols[[id]] <- gen_id(length(fold_idx))
  
  tibble::as_tibble(cols, .name_repair = "unique")

}


#' bootstrap_blocked_cross_validation
#' 
#' Bootstrap Blocked Cross-Validation Specification
#'
#' This function constructs a cross-validation specification using a predefined blocking variable
#' and creates bootstrap resamples within the blocks.
#'
#' @param block_var An integer vector defining the cross-validation blocks.
#' @param nreps An integer specifying the number of repetitions for each fold.
#' @param weights A numeric vector of the same length as `block_var`, representing the weights for each sample.
#'        Higher weights indicate that observations will be sampled more often. If not provided, all samples are treated as equally likely.
#'
#' @details
#' The function first checks if the provided weights are non-negative and normalizes them to sum to 1.
#' It then constructs a list containing the block variable, number of folds, block indices, number of repetitions, and weights.
#' The output list is assigned the class `"bootstrap_blocked_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the cross-validation specification, with class attributes "bootstrap_blocked_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' block_var <- rep(1:5, each=50)
#' weights <- runif(length(block_var))
#' weights[1] = 0
#' cval <- bootstrap_blocked_cross_validation(block_var, weights=weights)
#' X <- matrix(rnorm(length(block_var) * 10), length(block_var), 10)
#' y <- rep(letters[1:5], length.out=length(block_var))
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
bootstrap_blocked_cross_validation <- function(block_var, nreps=10, weights=NULL) {
  if (!is.null(weights)) {
    assert_that(length(weights) == length(block_var))
    assert_that(all(weights >= 0))  
    weights <- weights/sum(weights)
  }

  ret <- list(block_var=block_var, nfolds=length(unique(block_var)), 
              block_ind=sort(unique(block_var)), nreps=nreps, weights=weights)
  class(ret) <- c("bootstrap_blocked_cross_validation", "cross_validation", "list")
  ret
}


#' Blocked Cross-Validation Specification
#'
#' This function constructs a cross-validation specification using a predefined blocking variable.
#'
#' @param block_var An integer vector defining the cross-validation blocks.
#'
#' @details
#' The function constructs a list containing the block variable, number of folds, and block indices.
#' The output list is assigned the class `"blocked_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the cross-validation specification, with class attributes "blocked_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' block_var <- rep(1:5, each=50)
#' cval <- blocked_cross_validation(block_var)
#' X <- matrix(rnorm(length(block_var) * 10), length(block_var), 10)
#' y <- rep(letters[1:5], length.out=length(block_var))
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
blocked_cross_validation <- function(block_var) {
  ret <- list(block_var=block_var, nfolds=length(unique(block_var)), block_ind=sort(unique(block_var)))
  class(ret) <- c("blocked_cross_validation", "cross_validation", "list")
  ret
}



#' Sequential Blocked Cross-Validation Specification
#'
#' This function constructs a cross-validation specification using a predefined blocking variable, dividing each block into a specified number of folds.
#'
#' @param block_var An integer vector indicating the cross-validation blocks. Each block is indicated by a unique integer.
#' @param nfolds The number of folds to divide each sequence of trials within a block.
#' @param nreps The number of repetitions for the cross-validation procedure.
#'
#' @details
#' The function constructs a list containing the block variable, number of folds, number of repetitions, and block indices.
#' The output list is assigned the class `"sequential_blocked_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the cross-validation specification, with class attributes "sequential_blocked_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' block_var <- rep(1:5, each=50)
#' nfolds <- 2
#' nreps <- 4
#' cval <- sequential_blocked_cross_validation(block_var, nfolds, nreps)
#' X <- matrix(rnorm(length(block_var) * 10), length(block_var), 10)
#' y <- rep(letters[1:5], length.out=length(block_var))
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
sequential_blocked_cross_validation <- function(block_var, nfolds=2, nreps=4) {
  block_var <- as.integer(as.character(block_var))
  ret <- list(block_var=block_var, nfolds=nfolds, nreps=nreps, block_ind=sort(unique(block_var)))
  class(ret) <- c("sequential_blocked_cross_validation", "cross_validation", "list")
  ret
}



#' Custom Cross-Validation Specification
#'
#'
#' This function constructs a cross-validation specification that uses a user-supplied set of training and test indices.
#'
#' @param sample_set A list of training and test sample indices. Each element of the list must be a named list with two elements: "train" and "test".
#'
#' @details
#' The custom_cross_validation class allows users to define their own cross-validation structure by providing a set of training and test indices. This can be useful in situations where the standard cross-validation methods (e.g., k-fold, leave-one-out) do not adequately represent the desired validation structure.
#'
#' The function constructs a list containing the sample set and the number of folds, derived from the length of the sample set. The output list is assigned the class `"custom_cross_validation"`, `"cross_validation"`, and `"list"`.
#'
#' @return A list containing the custom cross-validation specification, with class attributes "custom_cross_validation", "cross_validation", and "list".
#'
#' @examples
#' sample_set <- list(
#'   list(train = 1:80, test = 81:100),
#'   list(train = 1:60, test = 61:100),
#'   list(train = 1:40, test = 41:100)
#' )
#' cval <- custom_cross_validation(sample_set)
#' X <- matrix(rnorm(100 * 10), 100, 10)
#' y <- rep(letters[1:4], length.out=100)
#'
#' sam <- crossval_samples(cval, as.data.frame(X), y)
#' @rdname cross_validation
#' @export
custom_cross_validation <- function(sample_set) {
  assert_that(is.list(sample_set))
  for (el in sample_set) {
    assert_that(all(names(el) == c("train", "test")))
  }
  
  ret <- list(sample_set=sample_set, nfolds=length(sample_set))
  class(ret) <- c("custom_cross_validation", "cross_validation", "list")
  ret
  
}
  
  
#' twofold_blocked_cross_validation
#'
#' Construct a cross-validation specification that randomly partitions the input set into two sets of blocks.
#'
#' This function creates a cross-validation scheme for cases where data is organized into blocks, and these blocks
#' are divided into two groups for evaluation. This approach can be useful when there is an inherent structure or
#' dependency within the blocks, and separating them can help to avoid biased estimates of model performance.
#' It returns an object of class "twofold_blocked_cross_validation", "cross_validation", and "list".
#'
#' @param block_var An integer vector representing the cross-validation blocks. Each block is indicated by a unique integer.
#' @param nreps An integer specifying the number of repetitions for the twofold split.
#' @return An object of class "twofold_blocked_cross_validation", "cross_validation", and "list" containing the block_var,
#'   nfolds (fixed at 2 for this function), nreps, and block_ind.
#' @export
#' @examples
#' blockvar <- rep(1:5, each=10)
#' nreps <- 5
#' cval <- twofold_blocked_cross_validation(blockvar, nreps=nreps)
#' samples <- crossval_samples(cval, as.data.frame(matrix(rnorm(50*50),50,50)), y=rep(letters[1:5],10))
#' stopifnot(nrow(samples) == nreps)
twofold_blocked_cross_validation <- function(block_var, nreps=10) {
  block_var <- as.integer(as.character(block_var))
  ret <- list(block_var=block_var, nfolds=2, nreps=nreps, block_ind=sort(unique(block_var)))
  class(ret) <- c("twofold_blocked_cross_validation", "cross_validation", "list")
  ret
}

#' kfold_cross_validation
#'
#' Construct a cross-validation specification that randomly partitions the input set into \code{nfolds} folds.
#'
#' This function creates a k-fold cross-validation scheme for cases where data needs to be split into a specified
#' number of folds for evaluation. It returns an object of class "kfold_cross_validation", "cross_validation", and "list".
#'
#' @param len An integer representing the number of observations.
#' @param nfolds An integer specifying the number of cross-validation folds.
#' @return An object of class "kfold_cross_validation", "cross_validation", and "list" containing the block_var and nfolds.
#' @examples
#' cval <- kfold_cross_validation(len=100, nfolds=10)
#' samples <- crossval_samples(cval, data=as.data.frame(matrix(rnorm(100*10), 100, 10)), y=rep(letters[1:5],20))
#' stopifnot(nrow(samples) == 10)
#' @export
kfold_cross_validation <- function(len, nfolds=10) {
  block_var <- sample(rep(seq(1, nfolds), length.out=len))
  ret <- list(block_var=block_var, nfolds=nfolds)
  class(ret) <- c("kfold_cross_validation", "cross_validation", "list")
  ret
}


# nest <- function(cval) {
#   clist <- lapply(cval$block_ind, function(i) {
#     blocked_cross_validation(cval$block_var, exclude=i)
#   })
#   
#   class(clist) = c("nested_blocked_cross_validation", "list")
#   clist
# }

#' @export
crossval_samples.sequential_blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_seq_block(data, y, nfolds=obj$nfolds, block_var=obj$block_var, nreps=obj$nreps, block_ind=obj$block_ind)
}

#' @export
crossval_samples.kfold_cross_validation <- function(obj, data,y,...) { 
  crossv_k(data, y, obj$nfolds)
}

#' @export
crossval_samples.blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_block(data, y, obj$block_var)
}

#' @export
crossval_samples.bootstrap_blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_bootstrap_block(data, y, block_var=obj$block_var, nreps=obj$nreps, weights=obj$weights)
}

#' @export
#' @importFrom modelr resample
crossval_samples.custom_cross_validation <- function(obj, data, y, id = ".id",...) {
  fold <- function(train, test) {
    list(
      ytrain = y[train],
      ytest = y[test],
      train = modelr::resample(data, train),
      test = modelr::resample(data, test)
    )
  }
  
  cols <- purrr::transpose(purrr::map(obj$sample_set, function(el) fold(el$train, el$test)))
  cols[[id]] <- gen_id(length(obj$sample_set))
  
  tibble::as_tibble(cols, .name_repair = "unique")
}

#' @export
crossval_samples.twofold_blocked_cross_validation <- function(obj, data, y,...) { 
  crossv_twofold(data, y, obj$block_var, obj$block_ind, nreps=obj$nreps)
}


#' @export
#' @method print blocked_cross_validation
print.blocked_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Blocked Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Dataset Information"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  └─ Number of Folds: "), number_style(x$nfolds), "\n")
  
  # Block statistics
  block_sizes <- table(x$block_var)
  cat(section_style("└─ Block Information"), "\n")
  cat(info_style("   ├─ Total Blocks: "), number_style(length(block_sizes)), "\n")
  cat(info_style("   ├─ Mean Block Size: "), 
      number_style(format(mean(block_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(block_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("   └─ Block Sizes: "), 
      number_style(paste0(names(block_sizes), ": ", block_sizes, collapse=", ")), "\n\n")
}

#' @export
#' @method print twofold_blocked_cross_validation
print.twofold_blocked_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Two-Fold Blocked Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  ├─ Number of Folds: "), number_style("2"), "\n")
  cat(info_style("│  └─ Repetitions: "), number_style(x$nreps), "\n")
  
  # Block statistics
  block_sizes <- table(x$block_var)
  cat(section_style("└─ Block Information"), "\n")
  cat(info_style("   ├─ Total Blocks: "), number_style(length(block_sizes)), "\n")
  cat(info_style("   ├─ Mean Block Size: "), 
      number_style(format(mean(block_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(block_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("   └─ Block Sizes: "), 
      number_style(paste0(names(block_sizes), ": ", block_sizes, collapse=", ")), "\n\n")
}

#' @export
#' @method print bootstrap_blocked_cross_validation
print.bootstrap_blocked_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ Bootstrap Blocked Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  └─ Bootstrap Repetitions: "), number_style(x$nreps), "\n")
  
  # Block statistics
  block_sizes <- table(x$block_var)
  cat(section_style("├─ Block Information"), "\n")
  cat(info_style("│  ├─ Total Blocks: "), number_style(length(block_sizes)), "\n")
  cat(info_style("│  ├─ Mean Block Size: "), 
      number_style(format(mean(block_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(block_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("│  └─ Block Sizes: "), 
      number_style(paste0(names(block_sizes), ": ", block_sizes, collapse=", ")), "\n")
  
  # Weight information if present
  cat(section_style("└─ Sampling Weights"), "\n")
  if (!is.null(x$weights)) {
    cat(info_style("   ├─ Status: "), crayon::green("Present"), "\n")
    cat(info_style("   ├─ Range: "), 
        number_style(sprintf("[%.3f, %.3f]", min(x$weights), max(x$weights))), "\n")
    cat(info_style("   └─ Non-zero Weights: "), 
        number_style(sum(x$weights > 0)), 
        stat_style(" ("), 
        number_style(sprintf("%.1f%%", 100*mean(x$weights > 0))),
        stat_style(")"), "\n\n")
  } else {
    cat(info_style("   └─ Status: "), crayon::red("None"), " (uniform sampling)\n\n")
  }
}

#' @export
#' @method print kfold_cross_validation
print.kfold_cross_validation <- function(x, ...) {
  # Ensure crayon is available
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty printing. Please install it.")
  }
  
  # Define color scheme
  header_style <- crayon::bold$cyan
  section_style <- crayon::yellow
  info_style <- crayon::white
  number_style <- crayon::green
  stat_style <- crayon::italic$blue
  
  # Print header
  cat("\n", header_style("█▀▀ K-Fold Cross-Validation ▀▀█"), "\n\n")
  
  # Basic information
  cat(section_style("├─ Configuration"), "\n")
  cat(info_style("│  ├─ Observations: "), number_style(format(length(x$block_var), big.mark=",")), "\n")
  cat(info_style("│  ├─ Number of Folds: "), number_style(x$nfolds), "\n")
  
  # Fold statistics
  fold_sizes <- table(x$block_var)
  cat(section_style("└─ Fold Information"), "\n")
  cat(info_style("   ├─ Mean Fold Size: "), 
      number_style(format(mean(fold_sizes), digits=2)), 
      stat_style(" (SD: "), 
      number_style(format(sd(fold_sizes), digits=2)),
      stat_style(")"), "\n")
  cat(info_style("   └─ Fold Sizes: "), 
      number_style(paste0("Fold ", names(fold_sizes), ": ", fold_sizes, collapse=", ")), 
      "\n\n")
}


==================================================
File: ./mvpa_iterate.R
==================================================
#' @noRd
#' @keywords internal
setup_mvpa_logger <- function() {
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Package 'crayon' is required for pretty logging. Please install it.")
  }
  
  # Use the standard layout but with colored messages
  futile.logger::flog.layout(futile.logger::layout.simple)
}

#' @keywords internal
try_warning  <- function(expr) {
  warn <- err <- NULL
  value <- withCallingHandlers(
    tryCatch(expr, error=function(e) {
      err <<- e
      NULL
    }), warning=function(w) {
      warn <<- paste0(warn, str_trim(as.character(w)))
      invokeRestart("muffleWarning")
    })
  list(value=value, warning=warn, error=err)
}



#' @noRd
#' @keywords internal
generate_crossval_samples <- function(mspec, roi) {
  crossval_samples(mspec$crossval, tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair="minimal"), y_train(mspec))
}

#' @noRd
#' @keywords internal
handle_model_training_error <- function(result, id, ytest) {
  futile.logger::flog.warn("⚠ Model %s fitting error: %s", 
                          crayon::blue(id), 
                          crayon::red(attr(result, "condition")$message))
  emessage <- if (is.null(attr(result, "condition")$message)) "" else attr(result, "condition")$message
  tibble::tibble(class=list(NULL), probs=list(NULL), y_true=list(ytest), 
                 fit=list(NULL), error=TRUE, error_message=emessage)
}

create_result_tibble <- function(cres, ind, mspec, id, result, compute_performance) {
  if (compute_performance) {
    tibble::tibble(result=list(cres), indices=list(ind), 
                   performance=list(compute_performance(mspec, cres)), id=id, 
                   error=FALSE, error_message="~", 
                   warning=!is.null(result$warning), 
                   warning_message=if (is.null(result$warning)) "~" else result$warning)
  } else {
    tibble::tibble(result=list(cres), indices=list(ind), performance=list(NULL), id=id, 
                   error=FALSE, error_message="~", 
                   warning=!is.null(result$warning), 
                   warning_message=if (is.null(result$warning)) "~" else result$warning)
  }
}



#' External Cross-Validation
#'
#' This function performs external cross-validation on the provided ROI and model specification.
#' It returns a tibble with performance metrics, fitted model (optional), and any warnings or errors.
#'
#' @param roi A list containing train_roi and test_roi elements.
#' @param mspec A model specification object.
#' @param id A unique identifier for the model.
#'
#' @return A tibble with performance metrics, fitted model (optional), and any warnings or errors.
#' @noRd
#' @keywords internal
external_crossval <- function(mspec, roi, id) {
  # Prepare the training data
  xtrain <- tibble::as_tibble(neuroim2::values(roi$train_roi), .name_repair="minimal")


  ytrain <- y_train(mspec)
 
  # Get the testing labels
  ytest <- y_test(mspec)

  # Get the ROI indices
  ind <- neuroim2::indices(roi$train_roi)

  # Train the model and handle any errors
  result <- try(train_model(mspec, xtrain, ytrain, indices=ind,
                            param=mspec$tune_grid,
                            tune_reps=mspec$tune_reps))

  if (inherits(result, "try-error")) {
    # Log a warning if there's an error during model training
    flog.warn("error fitting model %s : %s", id, attr(result, "condition")$message)
    # Store error messages and return a tibble with the error information
    emessage <- if (is.null(attr(result, "condition")$message)) "" else attr(result, "condition")$message
    tibble::tibble(class=list(NULL), probs=list(NULL), y_true=list(ytest),
                   fit=list(NULL), error=TRUE, error_message=emessage)
  } else {
    # Make predictions using the trained model
    pred <- predict(result, tibble::as_tibble(neuroim2::values(roi$test_roi), .name_repair="minimal"), NULL)
    # Convert predictions to a list
    plist <- lapply(pred, list)
    plist$y_true <- list(ytest)
    plist$test_ind <- list(as.integer(seq_along(ytest)))

    # Create a tibble with the predictions
    ret <- tibble::as_tibble(plist, .name_repair = .name_repair)

    # Wrap the results and return the fitted model if required
    cres <- if (mspec$return_fit) {
      wrap_result(ret, mspec$design, result$fit)
    } else {
      wrap_result(ret, mspec$design)
    }

    # Compute performance and return a tibble with the results and any warnings
    if (mspec$compute_performance) {
      tibble::tibble(result=list(cres), indices=list(ind),
                     performance=list(compute_performance(mspec, cres)), id=id,
                     error=FALSE, error_message="~",
                     warning=!is.null(result$warning),
                     warning_message=if (is.null(result$warning)) "~" else result$warning)
    } else {
      tibble::tibble(result=list(cres), indices=list(ind), performance=list(NULL), id=id,
                     error=FALSE, error_message="~",
                     warning=!is.null(result$warning),
                     warning_message=if (is.null(result$warning)) "~" else result$warning)
    }

  }
}


#' Perform Internal Cross-Validation for MVPA Models
#'
#' This function performs internal cross-validation on a region of interest (ROI) using a specified 
#' MVPA model. It handles the training, prediction, and result aggregation for each cross-validation fold.
#'
#' @param mspec An MVPA model specification object containing:
#'   \describe{
#'     \item{crossval}{Cross-validation specification}
#'     \item{compute_performance}{Logical indicating whether to compute performance metrics}
#'     \item{return_fit}{Logical indicating whether to return fitted models}
#'   }
#' @param roi A list containing at least:
#'   \describe{
#'     \item{train_roi}{Training data as a NeuroVec or NeuroSurfaceVector object}
#'   }
#' @param id Identifier for the current analysis
#'
#' @return A tibble containing:
#'   \describe{
#'     \item{result}{List of prediction results for each fold}
#'     \item{indices}{ROI indices used in the analysis}
#'     \item{performance}{Performance metrics if compute_performance is TRUE}
#'     \item{id}{Analysis identifier}
#'     \item{error}{Logical indicating if an error occurred}
#'     \item{error_message}{Error message if applicable}
#'     \item{warning}{Logical indicating if a warning occurred}
#'     \item{warning_message}{Warning message if applicable}
#'   }
#'
#' @details
#' The function performs the following steps:
#' 1. Generates cross-validation samples using the specified scheme
#' 2. For each fold:
#'    - Checks for minimum feature requirements
#'    - Trains the model on the training set
#'    - Makes predictions on the test set
#'    - Formats and stores results
#' 3. Merges results across all folds
#'
#' @note
#' This is an internal function used by mvpa_iterate and should not be called directly.
#' It assumes that input validation has already been performed.
#'
#' @keywords internal
#' @noRd
internal_crossval <- function(mspec, roi, id) {
  # Generate cross-validation samples
  # Note: This step could potentially be moved outside the function
  samples <- crossval_samples(mspec$crossval, tibble::as_tibble(neuroim2::values(roi$train_roi), 
  .name_repair=.name_repair), y_train(mspec))

  # Get ROI indices
  ind <- neuroim2::indices(roi$train_roi)

  # Iterate through the samples and fit the model
  ret <- samples %>% pmap(function(ytrain, ytest, train, test, .id) {
    # Check if the number of features is less than 2
    if (ncol(train) < 2) {
      # Return an error message
      return(
        format_result(mspec, error_message="error: less than 2 features", context=list(roi=roi, ytrain=ytrain, ytest=ytest, train=train, test=test, .id=.id))
      )
    }

    # Train the model
    result <- try(train_model(mspec, tibble::as_tibble(train, .name_repair=.name_repair), ytrain,
                              indices=ind))

    # Check if there was an error during model fitting
    if (inherits(result, "try-error")) {
      flog.warn("error fitting model %s : %s", id, attr(result, "condition")$message)
      # Store error messages
      emessage <- if (is.null(attr(result, "condition")$message)) "" else attr(result, "condition")$message
      format_result(mspec, result=NULL, error_message=emessage, context=list(roi=roi, ytrain=ytrain, ytest=ytest, train=train, test=test, .id=.id))
    } else {
      
      # Predict on test data
      format_result(mspec, result, error_message=NULL, context=list(roi=roi, ytrain=ytrain, ytest=ytest, train=train, test=test, .id=.id))
    }
  }) %>% purrr::discard(is.null) %>% dplyr::bind_rows()
  

  merge_results(mspec, ret, indices=ind, id=id)
}

    


#' @keywords internal
#' @noRd
extract_roi <- function(sample, data) {
  r <- as_roi(sample,data)
  v <- neuroim2::values(r$train_roi)
  r <- try(filter_roi(r))
  if (inherits(r, "try-error") || ncol(v) < 2) {
    NULL
  } else {
    r
  }
}
  
#' Iterate MVPA Analysis Over Multiple ROIs
#'
#' Performs multivariate pattern analysis (MVPA) across multiple regions of interest (ROIs) 
#' using batch processing and parallel computation.
#'
#' @param mod_spec An MVPA model specification object containing:
#'   \describe{
#'     \item{dataset}{The dataset to analyze}
#'     \item{compute_performance}{Logical indicating whether to compute performance metrics}
#'     \item{return_predictions}{Logical indicating whether to return predictions}
#'   }
#' @param vox_list A list of voxel indices or coordinates defining each ROI to analyze
#' @param ids Vector of identifiers for each ROI analysis (default: 1:length(vox_list))
#' @param batch_size Integer specifying number of ROIs to process per batch 
#'        (default: 10% of total ROIs)
#' @param verbose Logical indicating whether to print progress messages (default: TRUE)
#' @param processor Optional custom processing function. If NULL, uses default processor.
#'        Must accept parameters (obj, roi, rnum) and return a tibble.
#'
#' @return A tibble containing results for each ROI with columns:
#'   \describe{
#'     \item{result}{List column of analysis results (NULL if return_predictions=FALSE)}
#'     \item{indices}{List column of ROI indices used}
#'     \item{performance}{List column of performance metrics (if computed)}
#'     \item{id}{ROI identifier}
#'     \item{error}{Logical indicating if an error occurred}
#'     \item{error_message}{Error message if applicable}
#'     \item{warning}{Logical indicating if warning occurred}
#'     \item{warning_message}{Warning message if applicable}
#'   }
#'
#' @details
#' The function processes ROIs in batches to manage memory usage. For each batch:
#' 1. Extracts ROI data from the dataset
#' 2. Filters out ROIs with fewer than 2 voxels
#' 3. Processes each ROI using either the default or custom processor
#' 4. Combines results across all batches
#'
#' @importFrom furrr future_pmap
#' @importFrom purrr map
#' @export
mvpa_iterate <- function(mod_spec, vox_list, ids=1:length(vox_list), 
                         batch_size=as.integer(.1*length(ids)),
                         verbose=TRUE,
                         processor=NULL) {
  
  setup_mvpa_logger()
  
  if (length(vox_list) == 0) {
    futile.logger::flog.warn("⚠ Empty voxel list provided. No analysis to perform.")
    return(tibble::tibble())
  }
  
  # Add debugging
  futile.logger::flog.debug("Starting mvpa_iterate with %d voxels", length(vox_list))
  
  tryCatch({
    assert_that(length(ids) == length(vox_list), 
                msg=paste("length(ids) = ", length(ids), "::", "length(vox_list) =", length(vox_list)))
    
    batch_size <- max(1, batch_size)
    nbatches <- ceiling(length(ids)/batch_size)
    batch_group <- sort(rep(1:nbatches, length.out=length(ids)))
    batch_ids <- split(1:length(ids), batch_group)
    rnums <- split(ids, batch_group)
    
    dset <- mod_spec$dataset
    tot <- length(ids)
    
    # Process batches and collect results
    results <- vector("list", length(batch_ids))
    skipped_rois <- 0
    processed_rois <- 0
    
    for(i in seq_along(batch_ids)) {
      tryCatch({
        if(verbose) {
          futile.logger::flog.info("⚡ Processing batch %s/%s", 
                                  crayon::blue(i), 
                                  crayon::blue(nbatches))
        }
        
        vlist <- vox_list[batch_ids[[i]]]
        size <- sapply(vlist, function(v) length(v))
        
        # Add debugging
        futile.logger::flog.debug("Processing batch %d with %d voxels", i, length(vlist))
        
        sf <- get_samples(mod_spec$dataset, vox_list[batch_ids[[i]]]) %>% 
          mutate(.id=batch_ids[[i]], rnum=rnums[[i]], size=size) %>% 
          filter(size>=2)
        
        # Add debugging
        futile.logger::flog.debug("Sample frame has %d rows after filtering", nrow(sf))
        
        ## gpt01 suggestion::
        #sf <- sf %>%
        #  mutate(roi = map(sample, ~ extract_roi(.x, dset))) %>%
        #  select(-sample)
        
        if (nrow(sf) > 0) {
          sf <- sf %>% 
            rowwise() %>% 
            mutate(roi=list(extract_roi(sample,dset))) %>% 
            select(-sample)
          
          results[[i]] <- run_future(mod_spec, sf, processor, verbose)
          processed_rois <- processed_rois + nrow(sf)
          
          # Add debugging
          futile.logger::flog.debug("Batch %d produced %d results", i, nrow(results[[i]]))
        } else {
          skipped_rois <- skipped_rois + length(batch_ids[[i]])
          futile.logger::flog.warn("%s Batch %s: All ROIs filtered out (size < 2 voxels)", 
                                  crayon::yellow("⚠"),
                                  crayon::blue(i))
          results[[i]] <- tibble::tibble(
            result = list(NULL),
            indices = list(NULL),
            performance = list(NULL),
            id = rnums[[i]],
            error = TRUE,
            error_message = "ROI filtered out (size < 2 voxels)",
            warning = TRUE,
            warning_message = "ROI filtered out (size < 2 voxels)"
          )
        }
        
      }, error = function(e) {
        futile.logger::flog.error("Batch %d failed: %s", i, e$message)
        NULL
      })
    }
    
    # Final summary log
    futile.logger::flog.info("\n✨ MVPA Iteration Complete\n├─ Total ROIs: %s\n├─ Processed: %s\n└─ Skipped: %s",
                            crayon::blue(tot),
                            crayon::blue(processed_rois),
                            crayon::yellow(skipped_rois))
    
    # Combine all results
    final_results <- dplyr::bind_rows(results)
    
    final_results
  }, error = function(e) {
    futile.logger::flog.error("mvpa_iterate failed: %s", e$message)
    return(tibble::tibble())
  })
}

#' @noRd
run_future.default <- function(obj, frame, processor=NULL, verbose=FALSE, ...) {
  gc()
  total_items <- nrow(frame)
  processed_items <- 0
  
  do_fun <- if (is.null(processor)) {
    function(obj, roi, rnum) {
      process_roi(obj, roi, rnum)
    }
  } else {
    processor
  }
  
  frame %>% furrr::future_pmap(function(.id, rnum, roi, size) {
    # Update progress based on actual items processed
    processed_items <<- processed_items + 1
    if (verbose && (processed_items %% 100 == 0)) {
      progress_percent <- as.integer(processed_items/total_items * 100)
      futile.logger::flog.info("↻ Progress: %s%% complete", 
                              crayon::blue(progress_percent))
    }
    
    result <- do_fun(obj, roi, rnum)
    
    if (!obj$return_predictions) {
      result <- result %>% mutate(result = list(NULL))
    }
    
    result
  }, .options=furrr::furrr_options(seed=TRUE)) %>% 
    purrr::discard(is.null) %>% 
    dplyr::bind_rows()
}










==================================================
File: ./feature_rsa_model.R
==================================================
#' Create a Feature-Based RSA Design
#'
#' Creates a design for feature-based Representational Similarity Analysis (RSA).
#' You can either supply a similarity matrix S (and optionally select dimensions)
#' or directly supply a feature matrix F.
#'
#' @param S A symmetric similarity matrix representing the feature space relationships. 
#'          If NULL, you must supply F.
#' @param F A feature space matrix (observations by features). If supplied, this overrides S and k.
#' @param labels Vector of labels corresponding to the rows/columns of S or observations of F.
#' @param k Integer specifying the number of feature dimensions to retain when using S. If 0 (default),
#'          automatically determines dimensions using eigenvalue threshold > 1.
#'
#' @return A \code{feature_rsa_design} object (S3 class) containing:
#'   \describe{
#'     \item{S}{The input similarity matrix (if used)}
#'     \item{F}{Feature space projection matrix}
#'     \item{labels}{Vector of observation labels}
#'   }
#'
#' @details
#' If F is supplied directly, it is used as is (possibly scaled later by models).
#' If only S is supplied, the design computes eigen decomposition of S to find a suitable 
#' feature space (F).
#'
#' @export
feature_rsa_design <- function(S=NULL, F=NULL, labels, k=0) {
  assertthat::assert_that(!is.null(labels))
  
  if (!is.null(F)) {
    # If F is provided, we trust user supplied features
    assertthat::assert_that(is.matrix(F))
    assertthat::assert_that(nrow(F) == length(labels))
    # no dimension reduction from S, just store F
    ret <- list(S=S, F=F, labels=labels)
  } else {
    # Must have S
    assertthat::assert_that(!is.null(S))
    assertthat::assert_that(is.matrix(S))
    assertthat::assert_that(nrow(S) == length(labels))
    assertthat::assert_that(isSymmetric(S))
    
    S <- (S + t(S))/2
    
    if (k == 0) {
      eres <- eigen(S)
      k <- max(which(eres$values > 1))
      k <- max(k, 2)
      F <- eres$vectors[, 1:k, drop=FALSE]
    } else {
      assertthat::assert_that(k > 0 && k <= nrow(S))
      eres <- eigen(S)
      F <- eres$vectors[, 1:k, drop=FALSE]
    }
    ret <- list(S=S, F=F, labels=labels)
  }
  
  class(ret) <- "feature_rsa_design"
  ret
}


#' Create a Feature-Based RSA Model
#'
#' Creates a model for feature-based Representational Similarity Analysis (RSA) that relates neural patterns
#' (X) to a predefined feature space (F).
#'
#' @param dataset An \code{mvpa_dataset} object containing the neural data (X).
#' @param design A \code{feature_rsa_design} object specifying the feature space (F).
#' @param method Character string specifying the analysis method. One of:
#'   \describe{
#'     \item{scca}{Sparse Canonical Correlation Analysis}
#'     \item{pls}{Partial Least Squares}
#'     \item{pca}{Principal Component Analysis + linear regression on F}
#'   }
#' @param crossval Optional cross-validation specification.
#'
#' @return A \code{feature_rsa_model} object (S3 class).
#'
#' @details
#' Feature RSA models analyze the relationship between neural patterns X and a predefined feature space F.
#' Methods:
#'   - scca: Finds canonical correlations between X and F. For prediction, we use a canonical correlation-based reconstruction of F from X.
#'   - pls: Uses partial least squares regression to predict F from X.
#'   - pca: PCA on X, then regress F on principal components of X for prediction.
#'
#' @export
feature_rsa_model <- function(dataset,
                              design,
                              method=c("scca", "pls", "pca"),
                              crossval=NULL) {
  
  method <- match.arg(method)
  
  assertthat::assert_that(inherits(dataset, "mvpa_dataset"))
  assertthat::assert_that(inherits(design, "feature_rsa_design"))
  
  if (is.null(crossval) && !is.null(design$block_var)) {
    crossval <- blocked_cross_validation(design$block_var)
  }
  
  assertthat::assert_that(!is.null(crossval))
  
  ret <- list(method=method,
              dataset=dataset,
              design=design,
              crossval=crossval)
  
  class(ret) <- "feature_rsa_model"
  ret
}


# Helper to standardize data
.standardize <- function(X) {
  cm <- colMeans(X)
  csd <- apply(X,2,sd)
  csd[csd==0] <- 1
  X_sc <- scale(X, center=cm, scale=csd)
  list(X_sc=X_sc, mean=cm, sd=csd)
}

# Given scca result, predict F from X:
# Steps:
# 1) Standardize X with training stats
# 2) Compute CCAX = X_sc %*% WX
# 3) CCAY = CCAX * lambda (canonical correlations)
# 4) Y_sc = CCAY %*% t(WY)
# 5) Unscale Y_sc to get Y_pred
.predict_scca <- function(model, X_new) {
  # model$trained_model is result of scca(X,F)
  # model$scca_x and model$scca_f contain scaling info
  tm <- model$trained_model
  # scale X
  Xsc <- sweep(sweep(X_new,2,model$scca_x_mean,"-"),2,model$scca_x_sd,"/")
  CCAX_new <- Xsc %*% tm$WX
  # apply canonical correlations
  CCAY_new <- sweep(CCAX_new,2,tm$lambda,"*")
  # map back to Y space
  Y_sc <- CCAY_new %*% t(tm$WY)
  # unscale
  Y_pred <- sweep(sweep(Y_sc,2,model$scca_f_sd,"*"),2,model$scca_f_mean,"+")
  Y_pred
}


# For PCA method: we do prcomp(X), then regress F on PC(X).
# During training:
# 1) PCA on X (scaled)
# 2) LM: F_sc ~ PC(X)
# Store PC rotation, center, scale, and regression coefficients
.predict_pca <- function(model, X_new) {
  # standardize X
  Xsc <- sweep(sweep(X_new,2,model$pca_x_mean,"-"),2,model$pca_x_sd,"/")
  # get PCs
  PC_new <- Xsc %*% model$pcarot
  # predict F_sc = PC_new %*% coef
  F_sc_pred <- cbind(1, PC_new) %*% model$pca_coefs
  # unscale F
  F_pred <- sweep(sweep(F_sc_pred,2,model$pca_f_sd,"*"),2,model$pca_f_mean,"+")
  F_pred
}


# For PLS method:
# We fit plsr(F ~ X). pls package standardizes internally if requested.
# We'll store the fitted pls model. predict(pls_model, newx) should work directly if columns match.
# But we must ensure to apply the same scaling as training. The pls::plsr by default centers/scales X and Y if we specify.
# Let's explicitly scale ourselves:
# We'll do the scaling ourselves and use plsr with scale=FALSE since we already scaled:
# Actually, pls::plsr can do scaling internally, we just need to pass scale=TRUE and keep track of that.
# We'll rely on predict.pls to handle scaling internally. We'll store original scaling info for alignment if needed.
# As long as newdata has same column structure, predict(pls_model, newdata) will return scaled predictions. pls stores scaling info.

#' Train an RSA Model
#' @export
train_model.feature_rsa_model <- function(obj, train_dat, indices, ...) {
  X <- as.matrix(train_dat)
  F <- obj$design$F[indices,,drop=FALSE]
  
  method <- obj$method
  
  if (method == "pls") {
    # Fit PLS: F ~ X
    # Scale=TRUE so that pls handles scaling internally
    obj$trained_model <- pls::plsr(F ~ X, scale=TRUE)
    obj$training_indices <- indices
    
    predicted <- predict_model(obj, X)
    obj$performance <- evaluate_model(obj, predicted, F)
    return(obj)
    
  } else if (method == "scca") {
    # scca requires scaling
    # We'll scale X and F ourselves so we can reconstruct predictions
    sx <- .standardize(X)
    sf <- .standardize(F)
    scca_res <- whitening::scca(sx$X_sc, sf$X_sc, scale=FALSE) # already scaled
    obj$trained_model <- scca_res
    obj$training_indices <- indices
    obj$scca_x_mean <- sx$mean
    obj$scca_x_sd <- sx$sd
    obj$scca_f_mean <- sf$mean
    obj$scca_f_sd <- sf$sd
    
    predicted <- predict_model(obj, X)
    obj$performance <- evaluate_model(obj, predicted, F)
    return(obj)
    
  } else if (method == "pca") {
    # PCA on X (scaled)
    sx <- .standardize(X)
    sf <- .standardize(F)
    pca_res <- prcomp(sx$X_sc, scale.=FALSE)
    # regress F_sc on PC(X)
    PC_train <- pca_res$x
    # Fit linear model: F_sc ~ PC_train
    # coefs: we do a multivariate regression: solve(PC_train, F_sc)
    # Add intercept:
    PC_train_i <- cbind(1, PC_train)
    coefs <- solve(t(PC_train_i)%*%PC_train_i, t(PC_train_i)%*%sf$X_sc)
    
    obj$trained_model <- pca_res
    obj$training_indices <- indices
    obj$pcarot <- pca_res$rotation
    obj$pca_x_mean <- sx$mean
    obj$pca_x_sd <- sx$sd
    obj$pca_f_mean <- sf$mean
    obj$pca_f_sd <- sf$sd
    obj$pca_coefs <- coefs
    
    predicted <- predict_model(obj, X)
    obj$performance <- evaluate_model(obj, predicted, F)
    return(obj)
  }
}


#' Predict Method for Feature RSA Model
#'
#' Makes predictions of the feature space given new data X.
#' @export
predict_model.feature_rsa_model <- function(object, newdata, ...) {
  X <- as.matrix(newdata)
  
  method <- object$method
  if (method == "pls") {
    # Just call predict on pls model
    # pls model: Y = F, X = X
    # predict returns array with dims [nrow(newdata), ncol(F), ncomps]
    # we choose the final model comps
    pred_arr <- predict(object$trained_model, newdata=data.frame(X))
    # pls returns a 3D array: [n, ny, ncomp]
    # by default, last dim is last component
    ncomp <- object$trained_model$ncomp
    pred <- pred_arr[,,ncomp, drop=FALSE]
    pred <- pred[,,1] # drop extra dimension
    return(pred)
    
  } else if (method == "scca") {
    return(.predict_scca(object, X))
    
  } else if (method == "pca") {
    return(.predict_pca(object, X))
  }
}


#' Evaluate Method for Feature RSA Model
#'
#' @export
evaluate_model.feature_rsa_model <- function(object, predicted, observed, ...) {
  # Calculate correlation between predicted and observed feature projections
  cors <- diag(cor(predicted, observed))
  
  # Calculate mean squared error
  mse <- mean((predicted - observed)^2)
  
  list(
    correlations = cors,
    mse = mse
  )
}


#' Print Method for Feature RSA Model
#'
#' @export
print.feature_rsa_model <- function(x, ...) {
  cat("Feature RSA Model\n")
  cat("Method:", x$method, "\n")
  cat("Number of features:", ncol(x$design$F), "\n")
  cat("Number of observations:", nrow(x$design$F), "\n")
}


#' Summary Method for Feature RSA Model
#'
#' @export
summary.feature_rsa_model <- function(object, ...) {
  print(object)
  if (!is.null(object$trained_model)) {
    cat("\nModel Performance:\n")
    print(object$performance)
  }
}


#' Run regional RSA analysis on a specified feature RSA model
#'
#' Similar to \code{run_regional.mvpa_model}, but for feature_rsa_model.
#'
#' @param model_spec A \code{feature_rsa_model} object.
#' @param region_mask A mask representing different brain regions.
#' @param coalesce_design_vars If TRUE, merges design variables into prediction table.
#' @param processor A custom processor function for ROIs. If NULL, uses defaults.
#' @param verbose Print progress messages.
#' @param ... Additional arguments
#' 
#' @export
run_regional.feature_rsa_model <- function(model_spec, region_mask, coalesce_design_vars=FALSE, processor=NULL, 
                                           verbose=FALSE, ...) {  
  prepped <- prep_regional(model_spec, region_mask)
  
  # uses mvpa_iterate
  results <- mvpa_iterate(model_spec, prepped$vox_iter, ids=prepped$region_set, processor=processor, verbose=verbose, ...)
  
  perf <- if (model_spec$dataset$compute_performance) comp_perf(results, region_mask) else list(vols=list(), perf_mat=tibble::tibble())
  
  prediction_table <- if (model_spec$dataset$return_predictions) {
    combine_regional_results(results) 
  } else {
    NULL
  }
  
  if (coalesce_design_vars && !is.null(prediction_table)) {
    prediction_table <- coalesce_join(prediction_table, test_design(model_spec$design), 
                                      by=".rownum")
  }
  
  fits <- if (model_spec$dataset$return_fits) {
    lapply(results$result, "[[", "predictor")
  } else {
    NULL
  }
  
  regional_mvpa_result(model_spec=model_spec, performance_table=perf$perf_mat, 
                       prediction_table=prediction_table, vol_results=perf$vols, fits=fits)
}


==================================================
File: ./roisplit.R
==================================================
#' @keywords internal
#' @noRd
roi_volume_matrix <- function(mat, refspace, indices, coords) {
  structure(mat,
            refspace=refspace,
            indices=indices,
            coords=coords,
            class=c("roi_volume_matrix", "matrix"))
  
}

#' @keywords internal
#' @noRd
roi_surface_matrix <- function(mat, refspace, indices, coords) {
  structure(mat,
            refspace=refspace,
            indices=indices,
            coords=coords,
            class=c("roi_surface_matrix", "matrix"))

}


==================================================
File: ./distcalc.R
==================================================

# Default method
#' @export
pairwise_dist.default <- function(X, dist_obj) {
  stop("pairwise_dist not implemented for objects of class ", class(dist_obj)[1])
}


#' Create Distance Function Object
#'
#' This function constructs an object representing a distance function, 
#' which can be used with generic functions like `pairwise_dist` for computing distances. 
#' The object stores the method of distance calculation, labels associated with data points, 
#' and any additional parameters that may affect the distance computation.
#'
#' @param name A character string specifying the method of distance computation. 
#' This method name is used to dispatch the appropriate distance calculation function. 
#' Common methods might include "euclidean", "manhattan", "mahalanobis", etc.
#'
#' @param labels A vector of labels or identifiers associated with the rows of the data matrix. 
#' These labels are important for reference in distance computations, particularly when adjustments or 
#' restrictions based on groupings or identifiers are needed.
#'
#' @param ... Additional parameters relevant to the specific distance method. 
#' These could include tuning parameters like `lambda` for shrinkage in covariance estimation 
#' or parameters controlling the behavior of the distance computation.
#'
#' @return Returns an object of class `distfun` and the specific method class 
#' (as specified by the `method` parameter). This object encapsulates all information 
#' necessary to compute distances between data points according to the specified method 
#' and additional parameters.
#'
#' @details
#' The `create_dist` function enables the flexible creation of distance function objects. 
#' By specifying a method and associated parameters, users can customize the behavior of 
#' distance calculations. This functionality is especially useful in statistical and 
#' machine learning applications where different distance metrics can have significant 
#' impacts on the results.
#'
#' @examples
#' # Create a Euclidean distance function object
#' dist_obj_euc <- create_dist("euclidean", labels = c("A", "B", "C", "D"))
#'
#' # Create a Mahalanobis distance function object with additional parameters
#' dist_obj_maha <- create_dist("mahalanobis", labels = c("A", "B", "C", "D"))
#'
#' @export
create_dist <- function(name, labels, ...) {
  structure(list(name = name, labels = labels, ...), class = c(name, "distfun"))
}


#' Distance Function Constructors
#'
#' These functions provide convenient constructors for various types of distance functions.
#' Each constructor function initializes a distance function object for use with distance
#' computation functions, specifying the method and any necessary labels.
#'
#' @param labels A vector of labels associated with the data points.
#' @param method The method of distance computation, applicable for `cordist`.
#'
#' @return An object of class `distfun` with a specific method subclass, encapsulating
#'         all information necessary for computing distances according to the specified method.
#'
#' @details
#' The constructors allow for the specification of distance calculation methods and associated labels:
#' - `cordist` creates a correlation distance function.
#' - `mahadist` creates a Mahalanobis distance function.
#' - `eucdist` creates a Euclidean distance function.
#' - `robustmahadist` creates a robust version of the Mahalanobis distance function.
#'
#' @examples
#' dist_obj_1 <- cordist(labels = c("A", "B", "C"), method = "pearson")
#' dist_obj_2 <- mahadist(labels = c("A", "B", "C"))
#' dist_obj_3 <- eucdist(labels = c("A", "B", "C"))
#' dist_obj_4 <- robustmahadist(labels = c("A", "B", "C"))
#'
#' @seealso \code{\link{create_dist}} for the underlying constructor used by these functions.
#'
#' @rdname distance-constructors
#' @export
#' @keywords methods
cordist <- function(labels=NULL, method=c("pearson", "spearman")) {
  method=match.arg(method)
  create_dist(name="cordist", labels=labels, method=method)
}

# Example usage for Mahalanobis distance with labels
#' @rdname distance-constructors
mahadist <- function(labels=NULL) {
  create_dist("mahalanobis", labels)
}

#' @rdname distance-constructors
eucdist <- function(labels=NULL) {
  create_dist("euclidean", labels)
}

#' @rdname distance-constructors
robustmahadist <- function(labels=NULL) {
  create_dist("robustmahadist", labels)
}

#' @rdname distance-constructors
pcadist <- function(labels=NULL, ncomp=2, whiten=TRUE, threshfun=NULL, dist_method=c("euclidean", "manhattan", "cosine")) {
  dist_method <- match.arg(dist_method)
  if (is.null(threshfun)) {
    threshfun <- function(x) ncomp
  } else{
    stopifnot(is.function(threshfun))
  }
  
  create_dist("pcadist", labels, whiten=whiten, threshfun=threshfun, dist_method=dist_method)
}



#' Compute Pairwise Correlation Distances
#'
#' This method computes the pairwise correlation distances for a matrix `X`, excluding
#' comparisons within the same block as specified by the `dist_obj$block`.
#'
#' @param dist_obj A list containing the method ("correlation") and a block vector to specify
#' which rows in `X` should not be compared to avoid within-block correlation.
#' @param X Numeric matrix where rows represent observations and columns represent variables.
#'
#' @return An object of class `dist` containing the computed correlation distances.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' block <- rep(1:2, each=5)
#' dist_obj <- list(method = "pearson", block = block)
#' dist_matrix <- pairwise_dist.correlation(dist_obj, X)
#' 
#' @export
pairwise_dist.cordist <- function(obj, X) {
  1 - cor(t(X), method=obj$method)
  # block <- dist_obj$block
  # n <- nrow(X)
  # dist_matrix <- matrix(0, n, n)  # initialize with zeros
  # for (i in seq_len(n)) {
  #   valid_indices <- which(block != block[i])
  #   dist_matrix[i, valid_indices] <- 1 - cor(X[i, , drop = FALSE], t(X[valid_indices, , drop = FALSE]), method=dist_obj$method)
  # }
  # as.dist(dist_matrix)  # convert to distance object
}


#' Compute Pairwise Euclidean Distances
#'
#' Computes the pairwise Euclidean distances for a matrix `X`.
#'
#' @param dist_obj A list containing possibly additional parameters, currently unused.
#' @param X Numeric matrix where rows represent observations and columns represent variables.
#'
#' @return An object of class `dist` containing the computed Euclidean distances.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_matrix <- pairwise_dist.euclidean(list(), X)
#'
#' @export
pairwise_dist.euclidean <- function(obj, X) {
  # Estimate the inverse of the shrunken covariance matrix
  as.matrix(dist(X))
}


#' Compute Pairwise Mahalanobis Distances
#'
#' Computes the pairwise Mahalanobis distances using an inverse covariance matrix estimated
#' from the data matrix `X` with shrinkage.
#'
#' @param dist_obj A list that might include additional parameters for distance computation, 
#' currently unused.
#' @param X Numeric matrix where rows represent observations and columns represent variables.
#'
#' @return An object of class `dist` containing the computed Mahalanobis distances.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_matrix <- pairwise_dist.mahalanobis(list(), X)
#'
#' @export
#' @importFrom corpcor invcov.shrink
#' @importFrom stats mahalanobis
pairwise_dist.mahalanobis <- function(obj, X) {
  # Estimate the inverse of the shrunken covariance matrix
  inv_cov <- invcov.shrink(X)
  
  n <- nrow(X)
  
  # Compute the squared Mahalanobis distances using mahalanobis()
  dist_matrix_sq <- matrix(0, n, n)
  for (i in 1:n) {
    dist_matrix_sq[i, ] <- mahalanobis(X, center = X[i, ], cov = inv_cov, inverted = TRUE)
  }
  
  sqrt(dist_matrix_sq) # Computing the square root of the squared distances
}

#' @export
pairwise_dist.pcadist <- function(obj, X) {
  pres <- prcomp(X, center = TRUE, scale = TRUE)
  ncomp <- obj$threshfun(pres$sdev^2)
  if (ncomp < 1) {
    ncomp <- 1
    warning("Number of components set to 1, as threshold function returned a value less than 1.")
  }
  
  if (obj$whiten) {
    x <- pres$x[, 1:ncomp, drop=FALSE] %*% diag(x=1 / pres$sdev[1:ncomp], nrow=ncomp, ncol=ncomp)
  } else {
    x <- pres$x[, 1:ncomp, drop=FALSE]
  }
  if (obj$dist_method  %in% c("euclidean", "manhattan")) {
    dist_matrix <- as.matrix(dist(x, method=obj$dist_method))
  } else if (obj$dist_method == "cosine") {
    as.matrix(proxy::dist(x, method="cosine"))
  }
}


#' Compute Pairwise Robust Mahalanobis Distances
#'
#' Computes the pairwise Mahalanobis distances using a robustly estimated covariance matrix,
#' which can be more resistant to outliers.
#'
#' @param dist_obj A list that might include additional parameters for distance computation, 
#' currently unused.
#' @param X Numeric matrix where rows represent observations and columns represent variables.
#'
#' @return An object of class `dist` containing the computed robust Mahalanobis distances.
#'
#' @examples
#' X <- matrix(rnorm(100), 10, 10)
#' dist_matrix <- pairwise_dist.robustmahadist(list(), X)
#'
#' @export
pairwise_dist.robustmahadist <- function(obj, X) {
  # Use robust covariance estimation
  robust_cov <- robustcov::covGK(X)
  inv_cov <- corpcor::invcov.shrink(robust_cov)
  
  n <- nrow(X)
  dist_matrix <- matrix(0, n, n)
  
  for (i in 1:(n-1)) {
    for (j in (i + 1):n) {
      diff <- X[i, ] - X[j, ]
      dist_matrix[i, j] <- sqrt(t(diff) %*% inv_cov %*% diff)
      dist_matrix[j, i] <- dist_matrix[i, j]  # Fill lower triangle
    }
  }
  
  dist_matrix
}


#' Compute Second-Order Similarity Scores
#'
#' This function calculates the second order similarity between two similarity vectors
#' derived from a provided distance function applied to matrix X and a reference
#' similarity matrix S. The calculation takes into account a blocking variable to exclude
#' comparisons within the same block.
#'
#' @param dist_fun A distance function object or a character string specifying the 
#' method used for distance computation. This function should be capable of processing
#' the matrix X to produce a distance matrix.
#' @param X A numeric matrix where each row is an observation and columns are features.
#' Distances will be computed pairwise between rows of this matrix.
#' @param D A numeric matrix, typically a predefined dissimilarity matrix that
#' serves as a reference to compare against the computed distances from X.
#' @param block A vector (numeric or factor) indicating the block or group for each row
#' in X and S. Comparisons are only made between elements of different blocks.
#' @param method The method used for computing correlation between similarity vectors.
#' Defaults to "pearson", but "spearman" or "kendall" could also be used.
#'
#' @return A numeric vector of similarity scores, one for each observation in X, 
#' representing the correlation between distance vectors derived from X and the
#' corresponding vectors in S for non-matching blocks.
#'
#' @details
#' The function computes a distance matrix for X using the specified `dist_fun`. It then
#' compares these distances with the entries in S for each observation, excluding
#' comparisons within the same block as defined by the `block` argument. This is useful
#' for evaluating how well the distances within X align with an external similarity
#' standard, adjusting for within-block dependencies.
#'
#' @examples
#' # Assuming X and S are numeric matrices and block is a factor or numeric vector
#' dist_fun <- "euclidean"  # This should be defined or loaded from your package/environment
#' X <- matrix(rnorm(100), ncol=10)
#' D <- matrix(rnorm(100), ncol=10)
#' block <- rep(1:5, each=20)
#' scores <- second_order_similarity(dist_fun, X, D, block, method = "pearson")
#'
#' @export
second_order_similarity <- function(dist_fun, X, D, block, method = c("pearson", "spearman")) {
  method <- match.arg(method)

  # Compute distances using the provided distance function
  distance_matrix = pairwise_dist(dist_fun, X)
  
  # Initialize scores vector
  scores <- numeric(length(block))
 
  # Calculate trial-wise similarity scores, considering valid blocks
  for (i in seq_along(block)) {
    valid_indices <- which(block != block[i])
    if (length(valid_indices) > 0) {
      sim_vector_x = distance_matrix[i, valid_indices]
      sim_vector_s = D[i, valid_indices]
      scores[i] <- if (length(sim_vector_x) > 0 && length(sim_vector_s) > 0) {
        cor(sim_vector_x, sim_vector_s, method = method)
      } else {
        NA  # Handle cases where no valid comparisons
      }
    } else {
      scores[i] <- NA  # Assign NA if no valid indices
    }
  }
  
  return(scores)
}











