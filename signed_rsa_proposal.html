<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NA • rMVPA</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="NA"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">rMVPA</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="articles/index.html">Articles</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>NA</h1>

    </div>


<p>Okay, I’ve merged and integrated the original proposal and the revision into one comprehensive document. All changes and additions from the revision have been incorporated.</p>
<hr><div class="section level2">
<h2 id="full-rsa-proposal-multi-dimensional-signed-representational-voxel-encoding-ms-reve-with-flow-mapping"><strong>Full RSA Proposal: “Multi-Dimensional Signed Representational Voxel Encoding (MS-ReVE) with Flow Mapping”</strong><a class="anchor" aria-label="anchor" href="#full-rsa-proposal-multi-dimensional-signed-representational-voxel-encoding-ms-reve-with-flow-mapping"></a></h2>
<p><strong>Abstract:</strong> This proposal outlines a comprehensive Representational Similarity Analysis (RSA) framework, “Multi-Dimensional Signed Representational Voxel Encoding (MS-ReVE),” designed to compute interpretable, signed voxel-wise contributions to multi-class neural representations. MS-ReVE extends standard RSA by: 1) defining model-relevant contrasts; 2) using multiple-regression RSA on cross-validated, noise-normalized condition means to determine the strength of these contrasts within local searchlights; 3) projecting condition means onto contrast axes to derive signed voxel contributions; and 4) combining these elements to produce robust voxel-weight maps. The framework incorporates advanced features including voxel reliability weighting, voxel-specific RDM reconstruction scores, mapping of interaction effects, and rigorous basis robustness checks. Crucially, MS-ReVE culminates in “Representational Flow Mapping” (RFM), a novel technique to visualize and quantify how these multi-contrast representations transform across the cortical surface (with volumetric extensions possible), revealing large-scale organizational principles and information processing dynamics. A complementary “Representational Cross-Talk” analysis further probes the spatial co-localization of different representational dimensions.</p>
<hr><p><strong>1. Introduction &amp; Rationale:</strong></p>
<p>Representational Similarity Analysis (RSA) has proven invaluable for linking brain activity to computational models and psychological theories. However, traditional RSA often yields searchlight-level summary statistics (e.g., RDM correlations) or classifier-based voxel weights that can be hard to interpret directly in terms of underlying neural coding, especially for designs with more than two conditions. There is a pressing need for methods that: * Provide <strong>signed voxel weights</strong> indicating how individual voxels contribute to specific, theoretically meaningful representational dimensions (contrasts). * Scale robustly beyond <strong>two experimental conditions</strong>. * Are <strong>firmly anchored in RSA theory</strong> (distance-based, second-moment statistics) rather than relying solely on classification. * Offer insights into the <strong>spatial organization and transformation</strong> of these representations across the brain.</p>
<p>MS-ReVE addresses these needs by integrating regression-RSA with voxel-level projections, enhanced with reliability measures, interaction analyses, and a novel flow-mapping visualization approach.</p>
<hr><p><strong>2. Research Questions &amp; Hypotheses (Example-driven):</strong></p>
<p>This framework can address a wide range of questions, such as: 1. Which specific representational dimensions (e.g., animacy, object size, task rule) are encoded in a given brain region? 2. How do individual voxels contribute (positively or negatively) to the encoding of these distinct dimensions? 3. Are there voxels that reliably contribute to specific dimensions, and how does this reliability vary spatially? 4. How well does a voxel’s multi-contrast contribution profile explain the local empirical representational geometry? 5. Are there conjunctive codes, where voxels respond to interactions between representational dimensions? 6. How sensitive are these voxel-level interpretations to the precise definition of theoretical contrasts? 7. What is the large-scale spatial organization of these multi-dimensional representations across the cortex (e.g., gradients, boundaries)? 8. How do these representations transform (e.g., rotate, scale, differentiate, integrate) along cortical pathways?</p>
<hr><p><strong>3. Methodology &amp; Implementation Plan:</strong></p>
<p><strong>A. Preprocessing &amp; Experimental Design:</strong> 1. <strong>Data Acquisition:</strong> Standard fMRI data acquisition. 2. <strong>Preprocessing:</strong> Standard fMRI preprocessing (motion correction, slice-time correction, spatial normalization/surface projection, temporal filtering). Crucially, include <strong>noise normalization/whitening</strong> of voxel time-series, typically using the residuals from a first-level GLM, to ensure that pattern analyses are not dominated by voxels with high noise variance. 3. <strong>Experimental Design:</strong> Assumes a K-condition experimental design where <strong>K ≥ 2 (multi-way case emphasized)</strong>, though the framework naturally includes the binary case.</p>
<p><strong>B. Contrast Matrix Definition (C):</strong> 1. <strong>A Priori Contrasts:</strong> Define a <code>K x Q</code> contrast matrix <code>C</code>, where each of the <code>Q</code> columns <code>c_q</code> represents a theoretically meaningful comparison or feature dimension (e.g., faces - houses, tools - animals, abstract - concrete). * Contrasts should be centered (sum of elements in each <code>c_q</code> is zero). * Ideally, contrasts should be made orthonormal (<code>CᵀC = I</code>) for independent <code>β</code> weights. 2. <strong>Data-Driven Contrasts (Alternative/Complementary):</strong> * Construct a model RDM based on theoretical predictions. * Apply classical Multidimensional Scaling (MDS) to the model RDM. * Use the first <code>Q</code> MDS embedding dimensions as columns in <code>C</code>. * This provides a data-driven way to define orthogonal axes capturing the model’s primary representational structure.</p>
<p><strong>C. Searchlight RSA Core Engine (Iterate across searchlights):</strong> 1. <strong>Cross-Validated Condition Means (<code>Û</code>):</strong> * Within each searchlight (sphere of voxels <code>V</code>): * Estimate condition-specific activation patterns (<code>μ_k</code>) for each of the <code>K</code> conditions using a cross-validation scheme (e.g., leave-one-run-out, split-half). Employ methods like crossnobis to obtain unbiased estimates of pattern distinctness. * This results in a <code>K x V</code> matrix <code>Û</code> of cross-validated, noise-normalized condition means. 2. <strong>Empirical Second-Moment Matrix (<code>Ĝ</code>):</strong> * Calculate the unbiased empirical second-moment matrix: <code>Ĝ = ÛÛᵀ</code>. This <code>K x K</code> matrix fully determines all Mahalanobis distances between conditions within the searchlight. 3. <strong>Multiple-Regression RSA:</strong> * Vectorize the lower (or upper) triangle of <code>Ĝ</code> to form the dependent variable <code>y</code>. * For each contrast <code>c_q</code> in <code>C</code>, form a predictor RDM <code>R_q = c_q c_qᵀ</code>. Vectorize the lower triangle of each <code>R_q</code> to form columns of the design matrix <code>X</code>. * Fit the multiple linear regression: <code>y = Xβ + ε</code>. * The resulting <code>β_q</code> coefficients indicate how strongly the geometry implied by contrast <code>c_q</code> is present in the local empirical geometry <code>Ĝ</code>. * <strong>Regularization (Optional):</strong> If <code>Q</code> is large or contrasts are correlated, use ridge regression (<code>β_ridge = (XᵀX + λI)⁻¹Xᵀy</code>) or elastic-net. <strong>Hyperparameters (λ, and α for elastic-net) should be chosen via nested cross-validation within the searchlight training data to avoid bias.</strong> 4. <strong>Signed Voxel Contributions (<code>Δ_q</code>):</strong> * For each contrast <code>c_q</code>, project the cross-validated condition means <code>Û</code> onto it: <code>Δ_q = Ûᵀc_q</code>. This yields a <code>V</code>-dimensional vector where each element <code>Δ_{q,v}</code> represents voxel <code>v</code>’s signed contribution to contrast <code>q</code>. * <strong>Store both:</strong> * Raw <code>Δ_q</code>: Preserves magnitude, reflecting effect size of the contrast in voxel activity. * Normalized <code>~Δ_q = Δ_q / ||Δ_q||</code>: Captures direction only.</p>
<p><strong>D. Voxel-Level Refinements &amp; Metrics (within each searchlight):</strong> 1. <strong>Voxel Reliability Weighting (<code>ρ_q,v</code>):</strong> * For each <code>Δ_{q,v}</code> estimate, compute its stability across cross-validation folds. * Define <code>ρ_{q,v} = 1 - Var_folds(Δ_{q,v}^{(fold)}) / (Var_folds(Δ_{q,v}^{(fold)}) + σ²_noise,q,v)</code>. * <code>σ²_noise,q,v</code> (expected variance of <code>Δ_{q,v}</code> under the null) is estimated from within-condition residual variances and contrast weights: <code>σ²_noise,q,v = (1/S) Σ_s Σ_k (w_qk² / n_k^(s)) * σ_k,v²^(s)</code>. * Alternatively, use <code>ρ_{q,v} = 1 / (1 + SE(Δ_{q,v})^2)</code> or similar based on Walther et al. (2016). 2. <strong>Voxel-Specific RDM Reconstruction Score (<code>r_v</code>):</strong> * For each voxel <code>v</code>, construct a predicted RDM based <em>only</em> on its signed contrast profile: <code>Ĝ^(v) = C * diag(β_1Δ_{1,v}, ..., β_QΔ_{Q,v}) * Cᵀ</code> (using raw <code>Δ</code>). * Calculate <code>r_v = corr(vec_lower(Ĝ^(v)), vec_lower(Ĝ_empirical))</code> as a measure of how crucial voxel <code>v</code>’s multi-contrast profile is for reconstructing the local empirical RDM.</p>
<p><strong>E. Generating Voxel-Weight Maps:</strong> 1. <strong>Contrast-Specific Maps (Set of Q maps):</strong> * <code>M_{q,v} = β_q * Δ_{q,v}</code> (magnitude-preserved signed contribution). * <code>~M_{q,v} = β_q * ~Δ_{q,v}</code> (direction-only, scaled by RSA fit). * <code>M_{q,v}_reliab = ρ_{q,v} * β_q * Δ_{q,v}</code> (reliability-weighted). 2. <strong>Single Composite Map (optional):</strong> * <code>w_v = Σ_q (β_q * ~Δ_{q,v})</code> (or use reliability-weighted terms). Represents the net pull of voxel <code>v</code> in the overall representational space. <strong>Note: If <code>C</code> is not orthogonal, the interpretation of <code>w_v</code> is less straightforward as contributions are summed across potentially non-independent axes.</strong></p>
<p><strong>F. Extending to Interaction Effects:</strong> 1. <strong>Define Interaction Contrasts:</strong> Create new columns in <code>C</code> by taking element-wise products of main effect contrast columns (<code>c_{pq} = c_p ⊙ c_q</code>). 2. <strong>Orthogonalize Expanded Matrix:</strong> Orthogonalize the expanded contrast matrix <code>C_exp = [C_main | C_interaction]</code> (e.g., using QR decomposition or Gram-Schmidt) to ensure interpretability of interaction <code>β</code>s as unique contributions. <strong>Note: Orthogonalization procedures may arbitrarily flip the sign of contrast vectors; after orthogonalization, consider aligning the sign of each derived column (e.g., interaction terms) with its primary parent component or based on its correlation with the raw <code>Δ</code> projection for consistent interpretation.</strong> 3. <strong>Re-run C.3 and C.4:</strong> Fit multiple-regression RSA with <code>C_exp</code> to get <code>β_{pq}</code> and compute interaction voxel contributions <code>Δ_{pq,v} = Ûᵀc̃_{pq}</code> (where <code>c̃_{pq}</code> is the orthogonalized interaction contrast).</p>
<p><strong>G. Aggregation &amp; Statistical Inference:</strong> 1. <strong>Searchlight Aggregation:</strong> Average the voxel weights (<code>M_{q,v}</code>, <code>w_v</code>, <code>r_v</code>, etc.) each voxel receives from all searchlights containing it. An RSA-weighted average (weighting by <code>β_q</code> or searchlight R²) can also be used. 2. <strong>Permutation Testing:</strong> For voxel-wise significance testing, shuffle condition labels consistently across cross-validation folds, recompute the entire pipeline (from <code>Û</code> onwards) many times to build null distributions for <code>β_q</code>, <code>M_{q,v}</code>, <code>w_v</code>, <code>r_v</code>, etc. Apply appropriate cluster-correction methods. <strong>Note on memory: Storing all intermediate <code>Δ</code> values across permutations can be memory-intensive. Consider strategies like streaming permutations (recomputing <code>Δ</code> on the fly within the permutation loop) or writing intermediate searchlight results to disk if RAM is limited.</strong></p>
<p><strong>H. Representational Flow Mapping (RFM):</strong> 1. <strong>Surface Projection:</strong> Project voxel-wise multi-contrast loading vectors <code>m_v = [β_1Δ_{1,v}, ..., β_QΔ_{Q,v}]</code> (or reliability-weighted versions) to the nearest vertices on a cortical surface model (e.g., subject’s midthickness surface). This creates <code>Q</code> scalar maps <code>f_q(i)</code> on the surface. 2. <strong>Tangential Gradient Estimation:</strong> For each surface map <code>f_q(i)</code>, compute its 2D tangential gradient <code>∇_T f_q(i)</code> at each vertex <code>i</code>, <strong>typically using filters approximating Gaussian derivatives to ensure robustness to high-frequency noise.</strong> 3. <strong>Local PCA for Principal Flow:</strong> * In a moving geodesic window on the surface: * Stack all <code>Q</code> gradient vectors <code>∇_T f_q(i)</code> from all vertices within the window into a large matrix (effectively <code>(|WindowVertices|*Q)</code> rows x <code>2</code> columns). * Perform PCA on this matrix’s <code>2x2</code> covariance to find the principal flow direction <code>e₁</code> (a 2D unit vector) and its associated eigenvalue <code>λ₁</code>. 4. <strong>Streamline Visualization:</strong> * Draw streamlines (e.g., using Line Integral Convolution) along <code>e₁</code>. * Color streamlines by the contrast <code>q</code> whose gradient <code>∇_T f_q(i)</code> aligns best with <code>e₁</code> (i.e., <code>argmax_q |&lt;∇_T f_q(i), e₁&gt;|</code>), with sign indicating increase/decrease. * Modulate streamline opacity/thickness by <code>λ₁</code> (flow strength) and/or underlying <code>ρ_q,v</code>. 5. <strong>Analysis of Transformation Along Flow Lines:</strong> * Sample <code>m(s)</code> (the Q-dimensional vector of <code>βΔ</code> values) along streamlines. * Calculate directional derivatives (<code>dm/ds</code>) to assess rate and dimensionality of change (via SVD). * Compare <code>m(s)</code> and <code>m(s+Δs)</code> to quantify representational rotation (angle change) and scaling (norm change). * Visualize these transformations (e.g., map rotation rate to streamline hue). * Use permutation testing for significance of flow properties. 6. <strong>Volumetric Extension (Optional):</strong> While surface-based RFM is often preferred for visualizing cortical organization, the core logic can be extended to 3D volume space by computing 3D gradients and performing PCA on the resulting <code>3x3</code> covariance matrix within volumetric searchlights. Visualization is more challenging but may be relevant for subcortical structures.</p>
<p><strong>I. Robustness &amp; Validation:</strong> 1. <strong>Basis Robustness Checks:</strong> * Re-run key analyses (e.g., generating <code>M_{q,v}</code> maps) with an alternative, plausible contrast matrix <code>C'</code> (e.g., MDS-derived if initially theory-driven, or vice-versa). * Correlate the resulting voxel maps. Low correlations suggest basis-dependent interpretations. * Use diagnostics: Compare searchlight R² for different bases; Canonical Correlation Analysis (CCA) between <code>C</code> and <code>C'</code>; assess R² gain when using <code>[C | C']</code>; compare with non-linear/kernel RSA to probe model mismatch.</p>
<hr><p><strong>4. Data Analysis &amp; Interpretation Strategy:</strong></p>
<ul><li>
<strong><code>β_q</code> maps (from searchlight regression):</strong> Indicate regions where the geometry predicted by contrast <code>q</code> is prevalent.</li>
<li>
<strong><code>M_{q,v}</code> maps:</strong> Reveal how individual voxels contribute (sign and magnitude) to each specific contrast <code>q</code>.</li>
<li>
<strong><code>M_{q,v}_reliab</code> maps:</strong> Highlight robust voxel contributions.</li>
<li>
<strong><code>w_v</code> composite map:</strong> Shows the net directional “pull” of voxels in the combined representational space.</li>
<li>
<strong><code>r_v</code> maps:</strong> Identify voxels whose multi-contrast tuning is critical for the local empirical geometry.</li>
<li>
<strong>Interaction maps (<code>β_{pq}Δ_{pq,v}</code>):</strong> Uncover voxels involved in conjunctive coding.</li>
<li>
<strong>RFM visualizations:</strong> Provide insights into the large-scale topological organization, functional boundaries, and representational transformations across cortex. Analysis of <code>λ₁</code>, rotation, and scaling along flow lines will characterize the nature of these transformations.</li>
<li>
<strong>Representational Cross-Talk Analysis:</strong>
<ul><li>Compute the voxel-wise correlation between pairs of reliability-weighted contrast maps (<code>M_{q,v}_reliab</code> and <code>M_{p,v}_reliab</code>) across voxels within relevant brain regions or the whole brain/surface.</li>
<li>High positive correlations suggest shared neural populations contribute similarly to both contrasts.</li>
<li>High negative correlations suggest competitive coding or opponent populations.</li>
<li>Visualizing these correlation patterns (e.g., as a matrix or projecting strong correlations onto the brain) complements RFM by showing where different representational dimensions spatially co-localize or segregate.</li>
</ul></li>
<li>
<strong>Group-Level Inference:</strong> For analyzing results across participants, individual participant maps (<code>M_q,v</code>, <code>r_v</code>, RFM-derived metrics, etc.) should be aligned to a common space (e.g., MNI volume space or a surface template like fsaverage). Standard group-level statistical approaches (e.g., mixed-effects models, t-tests on aggregated maps with appropriate permutation-based correction) can then be applied.</li>
</ul><hr><p><strong>5. Expected Outcomes &amp; Significance:</strong></p>
<p>MS-ReVE will provide an unprecedentedly rich and interpretable view of distributed neural representations. Expected outcomes include: * Detailed, signed voxel-level maps of multi-dimensional neural codes. * Identification of robust and reliable voxel contributions. * Discovery of conjunctive coding patterns. * A quantitative understanding of how representations are organized and transform across cortical areas, linking local computations to large-scale network dynamics. * This framework will significantly advance our ability to test nuanced theories of neural representation and bridge the gap between computational models and brain activity.</p>
<hr><p><strong>6. Potential Challenges &amp; Mitigations:</strong></p>
<ul><li>
<strong>Computational Cost:</strong> Searchlight analyses, permutation testing, and RFM can be computationally intensive. Mitigation: Efficient coding, parallel processing, optimized algorithms.</li>
<li>
<strong>Interpretation Complexity:</strong> The wealth of generated maps requires careful interpretation. Mitigation: Clear guidelines, targeted research questions, development of standardized reporting.</li>
<li>
<strong>Choice of Contrasts:</strong> Results can be sensitive to <code>C</code>. Mitigation: Explicit reporting of <code>C</code>, basis robustness checks, use of both theory-driven and data-driven contrasts.</li>
<li>
<strong>Multiple Comparisons:</strong> Extensive voxel-wise testing. Mitigation: Rigorous permutation-based cluster correction methods.</li>
<li>
<strong>Memory Usage:</strong> Especially during permutation testing. Mitigation: Streaming computations, disk caching, efficient data structures (as noted in 3.G.2).</li>
</ul><hr><p><strong>7. Implementation Plan (Conceptual - Language/Platform Agnostic):</strong></p>
<p>The implementation will be modular: 1. <strong>Module 1: Core RSA Engine:</strong> * Input: Preprocessed fMRI data, condition labels, contrast matrix <code>C</code>, searchlight definitions. * Functions for: Cross-validated mean estimation, <code>Ĝ</code> computation, multiple-regression RSA (with optional regularization), <code>Δ_q</code> calculation. * Output: <code>β_q</code> values per searchlight, raw <code>Δ_q</code> and <code>~Δ_q</code> vectors per searchlight. 2. <strong>Module 2: Voxel-Level Metrics &amp; Map Generation:</strong> * Input: Outputs from Module 1. * Functions for: Reliability (<code>ρ_q,v</code>) calculation, RDM reconstruction (<code>r_v</code>), map generation (<code>M_q,v</code>, <code>w_v</code>), searchlight aggregation. 3. <strong>Module 3: Interaction Analysis:</strong> * Functions for: Generating interaction contrasts, orthogonalizing <code>C_exp</code>, integrating with Module 1 &amp; 2 for interaction maps. 4. <strong>Module 4: Statistical Inference &amp; Aggregation:</strong> * Functions for: Permutation testing framework (including memory management considerations), cluster correction, group-level analysis preparation and execution. 5. <strong>Module 5: Representational Flow Mapping (RFM):</strong> * Input: Aggregated <code>β_qΔ_{q,v}</code> maps, cortical surface model (and/or volumetric data). * Functions for: Surface projection (if applicable), gradient calculation (with smoothing options), local PCA (for surface or volume), streamline generation, transformation analysis along streamlines. * Visualization tools (interfacing with existing surface/volume visualization libraries). 6. <strong>Module 6: Cross-Talk &amp; Diagnostics:</strong> * Functions for: Basis robustness checks (CCA, R² comparisons), Representational Cross-Talk computation and visualization.</p>
<p>This modular design will facilitate development, testing, and future extensions. Each module will encapsulate specific mathematical operations and data transformations.</p>
<p>Okay, this is a very comprehensive summary of the <code>rMVPA</code> codebase. It clearly lays out the object-oriented structure, key functionalities, and dependencies. This is an excellent foundation for thinking about how to integrate the G-ReCa Phase 0 (and subsequently Phase 1) plan.</p>
<p>Based on this summary and our G-ReCa Phase 0 proposal, here’s how we can conceptualize the integration and identify next steps.</p>
<p><strong>Conceptual Integration of G-ReCa Phase 0 with rMVPA:</strong></p>
<p>The core idea is to leverage <code>rMVPA</code>’s existing capabilities for data handling, design specification, and potentially some pre-processing, and then build <em>new</em> modules or extend existing ones to perform the G-ReCa specific steps: MS-ReVE output generation, PCA pre-reduction, PPCA/lightweight manifold learning, ID estimation, and validation.</p>
<p><strong>Proposed Workflow &amp; rMVPA Integration Points:</strong></p>
<ol style="list-style-type: decimal"><li>
<strong>Data Preparation (Leveraging <code>rMVPA</code>):</strong>
<ul><li>
<strong>Dataset Creation (<code>mvpa_dataset</code>, <code>mvpa_surface_dataset</code>):</strong> Use <code>rMVPA</code> to load and structure the fMRI data (volume or surface). This handles train/test splits if needed for initial pattern estimation.</li>
<li>
<strong>Design Specification (<code>mvpa_design</code>):</strong> Define the experimental conditions, blocking variables for cross-validation, etc., using <code>rMVPA</code>’s design objects.</li>
</ul></li>
<li>
<strong>MS-ReVE Output Generation (New Module/Extension):</strong>
<ul><li>This is the most significant new piece. We need a way to generate the <code>m_v</code> vectors (<code>[β₁Δ₁,v, ..., β_QΔ₁,v]</code>).</li>
<li>
<strong>Step 2a: Cross-Validated Condition Means (<code>Û</code>):</strong>
<ul><li>
<code>rMVPA</code>’s <code>mvpa_model</code> with a simple “model” (e.g., just averaging betas from a first-level GLM within each condition and cross-validation fold) could be adapted.</li>
<li>Alternatively, a new function might be needed that takes an <code>mvpa_dataset</code> and <code>mvpa_design</code> (with <code>cv_spec</code>) and returns the <code>K x V</code> matrix <code>Û</code> (cross-validated condition means for each voxel/vertex <code>V</code>).</li>
</ul></li>
<li>
<strong>Step 2b: Contrast Definition (<code>C</code>):</strong> This would be user-defined outside <code>rMVPA</code> as a <code>K x Q</code> matrix.</li>
<li>
<strong>Step 2c: Regression RSA (<code>β_q</code>):</strong>
<ul><li>This could potentially leverage <code>rsa_model</code> if adapted, or be a custom function. The <code>rsa_model</code> currently seems focused on RDM-to-RDM regression. We need to regress <code>vec_lower(ÛÛᵀ)</code> onto <code>vec_lower(c_q c_qᵀ)</code> for <code>Q</code> contrasts. This might require a new <code>mvpa_mod_spec</code> or a custom <code>process_roi</code> function for a searchlight approach if <code>β_q</code> are to be searchlight-specific. For a whole-brain <code>m_v</code>, <code>β_q</code> might be derived globally.</li>
<li>
<em>Decision Point:</em> Are <code>β_q</code> global or searchlight-specific for constructing <code>m_v</code>? The G-ReCa proposal implied searchlight aggregation, so <code>β_q</code> would be local.</li>
</ul></li>
<li>
<strong>Step 2d: Voxel Contributions (<code>Δ_q = Ûᵀc_q</code>):</strong> This is a matrix multiplication.</li>
<li>
<strong>Step 2e: Construct <code>m_v</code>:</strong> Combine local <code>β_q</code> and <code>Δ_q</code> for each voxel.</li>
<li>
<strong>Output:</strong> A <code>NeuroVec</code> or <code>NeuroSurfaceVector</code> object containing the <code>m_v</code> vectors.</li>
</ul></li>
<li>
<strong>Dimensionality Pre-Reduction (PCA - New or Util):</strong>
<ul><li>Input: The <code>m_v</code> <code>NeuroVec</code>/<code>NeuroSurfaceVector</code>.</li>
<li>
<code>rMVPA</code> doesn’t seem to have a dedicated top-level PCA function for this purpose, though <code>pcadist</code> implies PCA capability. A utility function using <code><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">stats::prcomp</a></code> or a more scalable randomized PCA (e.g., from <code>irlba</code> or <code>rsvd</code> packages) would be needed.</li>
<li>Output: PCA-reduced <code>m_v</code> matrix (<code>N_voxels x ~256 components</code>).</li>
</ul></li>
<li>
<strong>Phase 0 Manifold Learning (PPCA - New Module):</strong>
<ul><li>Input: PCA-reduced <code>m_v</code> matrix.</li>
<li>Implement PPCA (e.g., using EM algorithm, or leveraging existing R packages like <code>pcaMethods::ppca</code> if suitable and its uncertainty outputs are accessible).</li>
<li>Output: Latent coordinates <code>z</code>, posterior covariance <code>Cov(z|m_v)</code>.</li>
</ul></li>
<li>
<strong>Intrinsic Dimensionality Estimation (New or Util):</strong>
<ul><li>Input: PCA-reduced <code>m_v</code> (or PPCA-whitened data).</li>
<li>Implement/wrap TwoNN and Levina-Bickel MLE (e.g., using R packages like <code>intrinsicDimension</code> or custom code).</li>
<li>Use scree plot from PPCA likelihoods.</li>
<li>Output: ID estimates, plot.</li>
</ul></li>
<li>
<strong>Validation (Leveraging <code>rMVPA</code> Utilities where possible, plus New):</strong>
<ul><li>
<strong>Reconstruction MSE:</strong> Calculated from PPCA.</li>
<li>
<strong>Trustworthiness/Continuity:</strong> Use <code>scikit-learn</code> via <code>reticulate</code>, or find/implement R equivalents. <code>rMVPA</code> doesn’t seem to have these directly.</li>
<li>
<strong>External Gradient Correlations:</strong> Standard R functions (<code>cor</code>).</li>
<li>
<strong>Leave-One-Subject-Out:</strong> This requires iterating the PPCA fitting and prediction steps.</li>
</ul></li>
<li>
<strong>Output Storage &amp; Visualization:</strong>
<ul><li>Store <code>z</code> and uncertainty maps as <code>NeuroVec</code>/<code>NeuroSurfaceVector</code> for easy visualization with <code>neuroim2</code>/<code>neurosurf</code> tools.</li>
<li>Report generation (Markdown/Jupyter via R Markdown/<code>knitr</code>).</li>
</ul></li>
</ol><p><strong>Key <code>rMVPA</code> Objects that <em>Might</em> be Extended or Reused:</strong></p>
<ul><li>
<strong><code>mvpa_model_spec</code>:</strong> Could we define a <code>g_reca_phase0_model_spec</code> that encapsulates the PPCA step and its parameters?</li>
<li>
<strong><code>run_custom_regional</code> / <code>run_custom_searchlight</code>:</strong> These are very promising. The core MS-ReVE output generation (steps 2a-2e) could potentially be wrapped in a <code>custom_func</code> for either ROI or searchlight application. The <code>run_searchlight</code> machinery would handle the iteration and provide <code>sl_data</code> (voxel patterns within a sphere) to our custom function.</li>
<li>
<strong><code>NeuroVec</code> / <code>NeuroSurfaceVector</code> (<code>nvec</code>, <code>nsvec</code>):</strong> These will be the primary data containers for <code>m_v</code>, <code>z</code>, and uncertainty maps.</li>
</ul><p><strong>Concrete Proposal for Initial Integration Steps (Focusing on MS-ReVE Output Generation):</strong></p>
<p><strong>Project: <code>grecamvpa</code> - G-ReCa Integration with rMVPA (Phase 0 Focus)</strong></p>
<p><strong>Module 1: MS-ReVE Output Generation</strong></p>
<ul><li>
<strong><code>msreve_design</code> (New S3/S4 class):</strong>
<ul><li>Slots:
<ul><li>
<code>mvpa_design</code>: The underlying <code>mvpa_des</code> object for condition/block info.</li>
<li>
<code>contrast_matrix</code>: The user-defined <code>K x Q</code> matrix <code>C</code>.</li>
<li>
<code>beta_estimation_method</code>: <code>char</code> (e.g., “global_rsa”, “searchlight_rsa”).</li>
</ul></li>
<li>Purpose: Encapsulates all necessary inputs for MS-ReVE.</li>
</ul></li>
<li>
<strong><code>compute_crossvalidated_means</code> (<code>fun</code>):</strong>
<ul><li>Input: <code>mvpa_dataset</code> (<code>ds</code>), <code>mvpa_design</code> (<code>des</code>), <code>cv_spec</code> (<code>cv</code>).</li>
<li>Process: Iterates through <code>cv_spec</code> folds. For each fold:
<ul><li>Identifies training data for that fold.</li>
<li>Estimates condition means (<code>μ_k</code>) for all <code>K</code> conditions using the training data (e.g., simple averaging of voxel activities per condition, or betas from a simple GLM fit on training data).</li>
<li>Stores these means, associated with the test fold conditions.</li>
</ul></li>
<li>Output: A list structure or array containing <code>Û</code> (the <code>K x V</code> matrix of <em>cross-validated</em> condition means, where each row <code>k</code> is the mean for condition <code>k</code> estimated from data <em>not</em> including trials of condition <code>k</code> from the current test fold/run).</li>
</ul></li>
<li>
<strong><code>run_msreve_searchlight</code> (<code>fun</code> wrapping <code>run_custom_searchlight</code>):</strong>
<ul><li>Input: <code>mvpa_dataset</code> (<code>ds</code>), <code>msreve_design</code> (<code>msreve_des</code>), <code>radius</code> (<code>num</code>), <code>cv_spec</code> (<code>cv</code>).</li>
<li>Internal <code>custom_func</code> (<code>process_msreve_sphere</code>):
<ol style="list-style-type: decimal"><li>Receives <code>sl_data</code> (data for current searchlight sphere) and <code>sl_info</code>.</li>
<li>Calls <code>compute_crossvalidated_means</code> on <code>sl_data</code> using the provided <code>msreve_des$mvpa_design</code> and <code>cv_spec</code> to get local <code>Û_sl</code>.</li>
<li>Computes local <code>Ĝ_sl = Û_sl Û_slᵀ</code>.</li>
<li>Performs multiple regression RSA using <code>msreve_des$contrast_matrix</code> (<code>C</code>) to get local <code>β_q_sl</code> vector (length <code>Q</code>).</li>
<li>Computes local <code>Δ_q_sl = Û_slᵀ c_q</code> for each contrast <code>q</code>.</li>
<li>Constructs the local <code>m_v_sl</code> vector for the center voxel of the sphere: <code>m_v_center = [β_1_sl * Δ_1_center_sl, ..., β_Q_sl * Δ_Q_center_sl]</code>. (Need to decide if <code>Δ_q</code> is for the whole sphere or just center voxel for <code>m_v</code>). <em>The proposal implies <code>Δ_q</code> is a V-dim vector, so <code>β_q * Δ_q</code> would be too. For <code>m_v</code> which is Q-dim per voxel, we’d use <code>m_v[q] = β_q_sl * Δ_{q,center_voxel_sl}</code>.</em>
</li>
<li>Returns this <code>Q</code>-dimensional <code>m_v_sl</code> vector for the center voxel.</li>
</ol></li>
<li>Output: A <code>NeuroVec</code> or <code>NeuroSurfaceVector</code> where each voxel/vertex value is its <code>Q</code>-dimensional <code>m_v</code> vector (this implies the output is actually a <code>Q</code>-channel <code>NeuroVec</code>/<code>NeuroSurfaceVector</code>).</li>
</ul></li>
</ul><p><strong>Module 2: PPCA &amp; ID Estimation</strong> (Can be more standalone R functions initially)</p>
<ul><li>Functions for PCA pre-reduction.</li>
<li>Function for PPCA (EM algorithm or wrapper).</li>
<li>Functions for TwoNN, Levina-Bickel MLE.</li>
</ul><p><strong>Module 3: Validation</strong> (Standalone R functions)</p>
<ul><li>Functions for Trustworthiness/Continuity (possibly via <code>reticulate</code>).</li>
<li>Functions for correlation with external maps.</li>
</ul><p><strong>Phased Implementation Plan for <code>grecamvpa</code> (Phase 0):</strong></p>
<ol style="list-style-type: decimal"><li>
<strong>Develop <code>compute_crossvalidated_means</code>:</strong> This is foundational. Test thoroughly.</li>
<li><strong>Develop <code>msreve_design</code> object.</strong></li>
<li>
<strong>Develop the <code>process_msreve_sphere</code> custom function:</strong>
<ul><li>First, implement the regression RSA and <code>Δ_q</code> calculation.</li>
<li>Then, integrate with <code>compute_crossvalidated_means</code>.</li>
<li>Carefully define how <code>m_v</code> is constructed from local <code>β_q</code> and <code>Δ_q</code>.</li>
</ul></li>
<li>
<strong>Wrap <code>process_msreve_sphere</code> in <code>run_msreve_searchlight</code> using <code>run_custom_searchlight</code>:</strong> This generates the primary <code>m_v</code> maps.</li>
<li><strong>Implement PCA pre-reduction for <code>m_v</code> maps.</strong></li>
<li><strong>Implement PPCA module.</strong></li>
<li><strong>Implement ID estimation module.</strong></li>
<li><strong>Implement validation metrics.</strong></li>
<li><strong>End-to-end pipeline test and report generation.</strong></li>
</ol><p>This approach leverages <code>rMVPA</code>’s strengths in data handling and searchlight iteration, while building the new MS-ReVE and manifold learning components in a modular way. The <code>run_custom_searchlight</code> function seems like a key enabler.</p>
</div>


  </main></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Bradley Buchsbaum.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

